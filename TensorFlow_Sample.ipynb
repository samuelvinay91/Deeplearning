{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern TensorFlow 2.x Fundamentals\n",
    "\n",
    "A comprehensive guide to TensorFlow 2.x -- from tensors and automatic differentiation to building, training, and deploying deep learning models.\n",
    "\n",
    "**This notebook replaces the original TF1.x sample** and covers the modern, Pythonic approach to deep learning with TensorFlow.\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Welcome to Modern TensorFlow](#1)\n",
    "2. [Setup & Verification](#2)\n",
    "3. [Tensors -- The Foundation](#3)\n",
    "4. [Eager Execution & GradientTape](#4)\n",
    "5. [Building Models -- Three Ways](#5)\n",
    "6. [Training Loop Deep Dive](#6)\n",
    "7. [Callbacks System](#7)\n",
    "8. [tf.data Pipeline](#8)\n",
    "9. [Practical Example: MNIST Classifier](#9)\n",
    "10. [Practical Example: Linear Regression (Updated from TF1)](#10)\n",
    "11. [Saving & Loading Models](#11)\n",
    "12. [Performance Tips](#12)\n",
    "13. [Exercises](#13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1. Welcome to Modern TensorFlow\n",
    "\n",
    "### What is TensorFlow 2.x?\n",
    "\n",
    "TensorFlow 2.x is Google's open-source platform for machine learning. It provides a comprehensive ecosystem of tools, libraries, and community resources that lets researchers push the state of the art in ML and developers easily build and deploy ML-powered applications.\n",
    "\n",
    "### Key Differences from TF1.x\n",
    "\n",
    "| Feature | TF 1.x | TF 2.x |\n",
    "|---------|--------|--------|\n",
    "| **Execution** | Graph-based (lazy) -- required `tf.Session()` | **Eager by default** -- runs immediately like NumPy |\n",
    "| **API** | Low-level, verbose, `tf.placeholder`, `tf.Variable` init | **Keras integrated** as the high-level API |\n",
    "| **Sessions** | `tf.Session().run()` to evaluate anything | **No sessions needed** -- just call functions |\n",
    "| **Variables** | `tf.global_variables_initializer()` required | Variables initialize on creation |\n",
    "| **Graphs** | Built explicitly, then executed | Built implicitly via `@tf.function` when needed |\n",
    "| **Debugging** | Painful -- print statements did not work in graph mode | **Standard Python debugging** works out of the box |\n",
    "| **API Cleanup** | Duplicated, inconsistent APIs (`tf.layers` vs `tf.keras.layers`) | **Single unified API** under `tf.keras` |\n",
    "\n",
    "### The TensorFlow Ecosystem\n",
    "\n",
    "- **TensorFlow Core** -- The main library for building and training models\n",
    "- **Keras** (`tf.keras`) -- High-level API for building neural networks (integrated into TF2)\n",
    "- **TensorFlow Lite** -- Deploy models on mobile and edge devices\n",
    "- **TensorFlow Serving** -- Production model serving with gRPC/REST APIs\n",
    "- **TensorFlow.js** -- Run models in the browser or Node.js\n",
    "- **TFX (TensorFlow Extended)** -- End-to-end ML pipelines for production\n",
    "- **TensorFlow Hub** -- Repository of pre-trained model components\n",
    "- **TensorBoard** -- Visualization toolkit for training metrics, graphs, and more\n",
    "- **TensorFlow Datasets** -- Collection of ready-to-use datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2. Setup & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {tf.keras.__version__}\")\n",
    "print(f\"Eager execution enabled: {tf.executing_eagerly()}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"GPU device(s): {tf.config.list_physical_devices('GPU')}\")\n",
    "else:\n",
    "    print(\"Running on CPU -- GPU not detected.\")\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In TF 1.x you had to call `tf.enable_eager_execution()` explicitly or work inside `tf.Session()`. In TF 2.x, eager execution is enabled by default -- code runs line by line, just like regular Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3. Tensors -- The Foundation\n",
    "\n",
    "Tensors are the central data structure in TensorFlow. They are multi-dimensional arrays with a uniform data type (`dtype`). Tensors are immutable (like NumPy arrays) -- every operation produces a new tensor.\n",
    "\n",
    "### 3.1 Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Constants (immutable) ----\n",
    "scalar = tf.constant(42)\n",
    "vector = tf.constant([1.0, 2.0, 3.0])\n",
    "matrix = tf.constant([[1, 2], [3, 4]])\n",
    "tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "print(\"=== Tensor Basics ===\")\n",
    "print(f\"Scalar:    shape={scalar.shape}, dtype={scalar.dtype}, ndim={scalar.ndim}, value={scalar.numpy()}\")\n",
    "print(f\"Vector:    shape={vector.shape}, dtype={vector.dtype}, ndim={vector.ndim}\")\n",
    "print(f\"Matrix:    shape={matrix.shape}, dtype={matrix.dtype}, ndim={matrix.ndim}\")\n",
    "print(f\"3D Tensor: shape={tensor_3d.shape}, dtype={tensor_3d.dtype}, ndim={tensor_3d.ndim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Variables (mutable -- used for model parameters) ----\n",
    "var = tf.Variable([[1.0, 2.0], [3.0, 4.0]], name=\"my_variable\")\n",
    "print(f\"Variable: {var}\")\n",
    "print(f\"Name: {var.name}, Shape: {var.shape}, Dtype: {var.dtype}\")\n",
    "\n",
    "# Variables can be updated in-place\n",
    "var.assign([[10.0, 20.0], [30.0, 40.0]])\n",
    "print(f\"After assign: {var.numpy()}\")\n",
    "\n",
    "var[0, 1].assign(99.0)\n",
    "print(f\"After element assign: {var.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Random Tensors ----\n",
    "normal = tf.random.normal([3, 3], mean=0.0, stddev=1.0)\n",
    "uniform = tf.random.uniform([3, 3], minval=0, maxval=10)\n",
    "truncated = tf.random.truncated_normal([3, 3], mean=0.0, stddev=1.0)\n",
    "\n",
    "print(\"Normal:\\n\", normal.numpy())\n",
    "print(\"\\nUniform:\\n\", uniform.numpy())\n",
    "\n",
    "# Reproducibility with seeds\n",
    "tf.random.set_seed(42)\n",
    "a = tf.random.normal([2, 2])\n",
    "tf.random.set_seed(42)\n",
    "b = tf.random.normal([2, 2])\n",
    "print(f\"\\nSame seed produces same result: {tf.reduce_all(a == b).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Special Tensors ----\n",
    "zeros = tf.zeros([3, 4])\n",
    "ones = tf.ones([2, 3])\n",
    "eye = tf.eye(4)  # Identity matrix\n",
    "filled = tf.fill([2, 3], 7.0)\n",
    "range_t = tf.range(0, 10, 2)\n",
    "linspace = tf.linspace(0.0, 1.0, 5)\n",
    "\n",
    "print(f\"Zeros: shape={zeros.shape}\")\n",
    "print(f\"Identity matrix:\\n{eye.numpy()}\")\n",
    "print(f\"Range: {range_t.numpy()}\")\n",
    "print(f\"Linspace: {linspace.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Arithmetic Operations ----\n",
    "a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "print(\"=== Arithmetic ===\")\n",
    "print(f\"Addition (a + b):\\n{(a + b).numpy()}\")\n",
    "print(f\"\\nSubtraction (a - b):\\n{(a - b).numpy()}\")\n",
    "print(f\"\\nElement-wise multiply (a * b):\\n{(a * b).numpy()}\")\n",
    "print(f\"\\nMatrix multiply (a @ b):\\n{(a @ b).numpy()}\")\n",
    "print(f\"\\nElement-wise power (a ** 2):\\n{(a ** 2).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Reduction Operations ----\n",
    "m = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "print(\"=== Reductions ===\")\n",
    "print(f\"Matrix:\\n{m.numpy()}\")\n",
    "print(f\"\\nReduce sum (all): {tf.reduce_sum(m).numpy()}\")\n",
    "print(f\"Reduce sum (axis=0, columns): {tf.reduce_sum(m, axis=0).numpy()}\")\n",
    "print(f\"Reduce sum (axis=1, rows):    {tf.reduce_sum(m, axis=1).numpy()}\")\n",
    "print(f\"Reduce mean: {tf.reduce_mean(m).numpy()}\")\n",
    "print(f\"Reduce max:  {tf.reduce_max(m).numpy()}\")\n",
    "print(f\"Reduce min:  {tf.reduce_min(m).numpy()}\")\n",
    "print(f\"Argmax (axis=1): {tf.argmax(m, axis=1).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Reshaping and Manipulation ----\n",
    "t = tf.range(12)\n",
    "print(f\"Original: shape={t.shape}, values={t.numpy()}\")\n",
    "\n",
    "reshaped = tf.reshape(t, [3, 4])\n",
    "print(f\"\\nReshaped to [3, 4]:\\n{reshaped.numpy()}\")\n",
    "\n",
    "transposed = tf.transpose(reshaped)\n",
    "print(f\"\\nTransposed to {transposed.shape}:\\n{transposed.numpy()}\")\n",
    "\n",
    "# Expand and squeeze dimensions\n",
    "expanded = tf.expand_dims(t, axis=0)  # Add batch dimension\n",
    "print(f\"\\nExpanded dims: {t.shape} -> {expanded.shape}\")\n",
    "\n",
    "squeezed = tf.squeeze(expanded)\n",
    "print(f\"Squeezed dims: {expanded.shape} -> {squeezed.shape}\")\n",
    "\n",
    "# Concatenation and stacking\n",
    "a = tf.constant([[1, 2], [3, 4]])\n",
    "b = tf.constant([[5, 6], [7, 8]])\n",
    "print(f\"\\nConcat (axis=0):\\n{tf.concat([a, b], axis=0).numpy()}\")\n",
    "print(f\"\\nConcat (axis=1):\\n{tf.concat([a, b], axis=1).numpy()}\")\n",
    "print(f\"\\nStack (new axis=0):\\n{tf.stack([a, b], axis=0).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Indexing and Slicing ----\n",
    "t = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "print(f\"Tensor:\\n{t.numpy()}\")\n",
    "\n",
    "print(f\"\\nt[0]:       {t[0].numpy()}\")         # First row\n",
    "print(f\"t[:, 0]:    {t[:, 0].numpy()}\")         # First column\n",
    "print(f\"t[1, 2]:    {t[1, 2].numpy()}\")         # Element at row 1, col 2\n",
    "print(f\"t[0:2, 1:3]:\\n{t[0:2, 1:3].numpy()}\")  # Submatrix\n",
    "print(f\"t[-1]:      {t[-1].numpy()}\")           # Last row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Broadcasting ----\n",
    "# TensorFlow follows the same broadcasting rules as NumPy\n",
    "a = tf.constant([[1], [2], [3]])   # Shape: (3, 1)\n",
    "b = tf.constant([10, 20, 30])      # Shape: (3,)\n",
    "\n",
    "print(f\"a shape: {a.shape}\")\n",
    "print(f\"b shape: {b.shape}\")\n",
    "print(f\"\\na + b (broadcasted):\\n{(a + b).numpy()}\")\n",
    "# a is broadcast along axis=1, b along axis=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 NumPy Interop & GPU Placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- NumPy <-> TensorFlow conversion ----\n",
    "np_array = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# NumPy to TensorFlow\n",
    "tf_tensor = tf.constant(np_array)\n",
    "tf_tensor_convert = tf.convert_to_tensor(np_array)\n",
    "print(f\"NumPy -> TF: {tf_tensor.dtype}, {tf_tensor.shape}\")\n",
    "\n",
    "# TensorFlow to NumPy\n",
    "back_to_np = tf_tensor.numpy()\n",
    "print(f\"TF -> NumPy: {type(back_to_np)}, {back_to_np.dtype}\")\n",
    "\n",
    "# TensorFlow ops accept NumPy arrays directly\n",
    "result = tf.multiply(np_array, 2)\n",
    "print(f\"\\nTF op on NumPy array: {result.numpy()}\")\n",
    "\n",
    "print(f\"\\n--- Important: TF tensors and NumPy arrays can share memory ---\")\n",
    "print(f\"TF tensor device: {tf_tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Device Placement ----\n",
    "# TF automatically places operations on GPU if available\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(f\"  {device}\")\n",
    "\n",
    "# Explicit placement\n",
    "with tf.device('/CPU:0'):\n",
    "    cpu_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(f\"\\nCPU tensor device: {cpu_tensor.device}\")\n",
    "\n",
    "# If GPU is available, you can place tensors on it\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    with tf.device('/GPU:0'):\n",
    "        gpu_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "    print(f\"GPU tensor device: {gpu_tensor.device}\")\n",
    "else:\n",
    "    print(\"No GPU available -- skipping GPU placement example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4. Eager Execution & GradientTape\n",
    "\n",
    "### Why Eager Execution Matters\n",
    "\n",
    "In TF 1.x, you first built a computation graph and then ran it inside a `tf.Session()`. Debugging was painful because you could not inspect intermediate values easily.\n",
    "\n",
    "In TF 2.x, **eager execution** means operations execute immediately and return concrete values. This makes TensorFlow feel like NumPy and allows standard Python debugging tools (print, pdb, etc.).\n",
    "\n",
    "### Automatic Differentiation with `tf.GradientTape`\n",
    "\n",
    "`tf.GradientTape` records operations for automatic differentiation. TensorFlow \"watches\" `tf.Variable` objects by default. For `tf.constant`, you must call `tape.watch()` explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Simple Gradient Computation ----\n",
    "# f(x) = x^2 + 2x + 1\n",
    "# f'(x) = 2x + 2\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2 + 2*x + 1  # y = x^2 + 2x + 1\n",
    "\n",
    "grad = tape.gradient(y, x)  # dy/dx = 2x + 2 = 2(3) + 2 = 8\n",
    "print(f\"x = {x.numpy()}, y = {y.numpy()}, dy/dx = {grad.numpy()}\")  # Should be 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Gradients with respect to multiple variables ----\n",
    "w = tf.Variable(2.0)\n",
    "b = tf.Variable(1.0)\n",
    "x_val = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = w * x_val + b  # y = wx + b\n",
    "\n",
    "# Compute gradients w.r.t. both w and b simultaneously\n",
    "dw, db = tape.gradient(y, [w, b])\n",
    "print(f\"y = w*x + b = {y.numpy()}\")\n",
    "print(f\"dy/dw = x = {dw.numpy()}\")  # dy/dw = x = 3.0\n",
    "print(f\"dy/db = 1 = {db.numpy()}\")  # dy/db = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Higher-Order Gradients ----\n",
    "# f(x) = x^3\n",
    "# f'(x) = 3x^2\n",
    "# f''(x) = 6x\n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as t2:\n",
    "    with tf.GradientTape() as t1:\n",
    "        y = x ** 3                  # y = x^3\n",
    "    dy_dx = t1.gradient(y, x)       # f'(x) = 3x^2\n",
    "d2y_dx2 = t2.gradient(dy_dx, x)    # f''(x) = 6x\n",
    "\n",
    "print(f\"f(x) = x^3 at x = 2\")\n",
    "print(f\"  f(2)   = {y.numpy()}\")         # 8\n",
    "print(f\"  f'(2)  = {dy_dx.numpy()}\")     # 12\n",
    "print(f\"  f''(2) = {d2y_dx2.numpy()}\")   # 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Persistent Tape (for multiple gradient calls) ----\n",
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x ** 2\n",
    "    z = x ** 3\n",
    "\n",
    "dy_dx = tape.gradient(y, x)  # 2x = 6\n",
    "dz_dx = tape.gradient(z, x)  # 3x^2 = 27\n",
    "print(f\"dy/dx = {dy_dx.numpy()}, dz/dx = {dz_dx.numpy()}\")\n",
    "\n",
    "# Important: delete persistent tapes to free resources\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Watching Constants ----\n",
    "x = tf.constant(3.0)  # Constants are NOT watched by default\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x ** 2\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(f\"Gradient of x^2 at x=3 (constant): {grad.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualizing a Gradient: Tangent Line to a Curve ----\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Compute f(x) = x^2 and its gradient at a specific point\n",
    "x_point = tf.Variable(1.5)\n",
    "with tf.GradientTape() as tape:\n",
    "    y_point = x_point ** 2\n",
    "slope = tape.gradient(y_point, x_point)\n",
    "\n",
    "# Tangent line: y = f(a) + f'(a)(x - a)\n",
    "tangent = y_point.numpy() + slope.numpy() * (x_range - x_point.numpy())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_range, x_range**2, 'b-', linewidth=2, label='$f(x) = x^2$')\n",
    "plt.plot(x_range, tangent, 'r--', linewidth=2, label=f'Tangent at x={x_point.numpy()}')\n",
    "plt.plot(x_point.numpy(), y_point.numpy(), 'ro', markersize=10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(f'GradientTape: Tangent to $x^2$ at x={x_point.numpy()}, slope={slope.numpy()}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-2, 9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5. Building Models -- Three Ways\n",
    "\n",
    "TensorFlow/Keras provides three progressively more flexible APIs for building models:\n",
    "\n",
    "| API | Best For | Flexibility | Ease |\n",
    "|-----|----------|------------|------|\n",
    "| **Sequential** | Simple linear stacks of layers | Low | High |\n",
    "| **Functional** | Multi-input/output, shared layers, residual connections | Medium | Medium |\n",
    "| **Subclassing** | Full control, dynamic architectures, research | High | Low |\n",
    "\n",
    "### 5a. Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sequential API: a simple linear stack of layers\n",
    "sequential_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "sequential_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also build it incrementally\n",
    "seq_model_v2 = tf.keras.Sequential(name='incremental_model')\n",
    "seq_model_v2.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)))\n",
    "seq_model_v2.add(tf.keras.layers.Dropout(0.2))\n",
    "seq_model_v2.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "seq_model_v2.add(tf.keras.layers.Dropout(0.2))\n",
    "seq_model_v2.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "print(f\"Model: {seq_model_v2.name}, Layers: {len(seq_model_v2.layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Functional API\n",
    "\n",
    "The Functional API allows you to build models with non-linear topology, shared layers, and multiple inputs/outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API: same architecture as above but using function-call syntax\n",
    "inputs = tf.keras.Input(shape=(784,), name='input_features')\n",
    "x = tf.keras.layers.Dense(128, activation='relu', name='dense_1')(inputs)\n",
    "x = tf.keras.layers.Dropout(0.2, name='dropout_1')(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "x = tf.keras.layers.Dropout(0.2, name='dropout_2')(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "functional_model = tf.keras.Model(inputs=inputs, outputs=outputs, name='functional_model')\n",
    "functional_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API excels at multi-input models\n",
    "# Example: a model that takes an image and metadata as separate inputs\n",
    "image_input = tf.keras.Input(shape=(784,), name='image')\n",
    "metadata_input = tf.keras.Input(shape=(10,), name='metadata')\n",
    "\n",
    "# Image branch\n",
    "x1 = tf.keras.layers.Dense(64, activation='relu')(image_input)\n",
    "x1 = tf.keras.layers.Dense(32, activation='relu')(x1)\n",
    "\n",
    "# Metadata branch\n",
    "x2 = tf.keras.layers.Dense(16, activation='relu')(metadata_input)\n",
    "\n",
    "# Merge branches\n",
    "merged = tf.keras.layers.concatenate([x1, x2])\n",
    "output = tf.keras.layers.Dense(10, activation='softmax')(merged)\n",
    "\n",
    "multi_input_model = tf.keras.Model(\n",
    "    inputs=[image_input, metadata_input],\n",
    "    outputs=output,\n",
    "    name='multi_input_model'\n",
    ")\n",
    "multi_input_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Model Subclassing API\n",
    "\n",
    "For maximum flexibility, subclass `tf.keras.Model`. This is similar to PyTorch's approach and is common in research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    \"\"\"A custom model built by subclassing tf.keras.Model.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "subclass_model = CustomModel(num_classes=10)\n",
    "\n",
    "# Subclassed models need a forward pass to build\n",
    "subclass_model.build(input_shape=(None, 784))\n",
    "subclass_model.summary()\n",
    "\n",
    "# Verify it works with a dummy input\n",
    "dummy_input = tf.random.normal([2, 784])\n",
    "output = subclass_model(dummy_input, training=False)\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Output sums to 1 (softmax): {tf.reduce_sum(output, axis=1).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6. Training Loop Deep Dive\n",
    "\n",
    "### 6.1 Training with `model.fit()` (High-Level)\n",
    "\n",
    "The simplest way to train a model. Handles batching, metrics, callbacks, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for demonstration\n",
    "np.random.seed(42)\n",
    "X_synthetic = np.random.randn(1000, 784).astype(np.float32)\n",
    "y_synthetic = np.random.randint(0, 10, size=(1000,))\n",
    "\n",
    "# Build and compile model\n",
    "fit_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "fit_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = fit_model.fit(\n",
    "    X_synthetic, y_synthetic,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal train accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final val accuracy:   {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Custom Training Loop with GradientTape\n",
    "\n",
    "When you need full control: custom losses, gradient manipulation, multi-model training (GANs), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "custom_loop_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Setup\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Create datasets\n",
    "X_train_ct = X_synthetic[:800]\n",
    "y_train_ct = y_synthetic[:800]\n",
    "X_val_ct = X_synthetic[800:]\n",
    "y_val_ct = y_synthetic[800:]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_ct, y_train_ct)).shuffle(800).batch(32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_ct, y_val_ct)).batch(32)\n",
    "\n",
    "# ---- The @tf.function decorator compiles the function into a TF graph for speed ----\n",
    "@tf.function\n",
    "def train_step(x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = custom_loop_model(x_batch, training=True)\n",
    "        loss = loss_fn(y_batch, predictions)\n",
    "    gradients = tape.gradient(loss, custom_loop_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, custom_loop_model.trainable_variables))\n",
    "    train_acc_metric.update_state(y_batch, predictions)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(x_batch, y_batch):\n",
    "    predictions = custom_loop_model(x_batch, training=False)\n",
    "    val_acc_metric.update_state(y_batch, predictions)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "\n",
    "    train_acc = train_acc_metric.result()\n",
    "\n",
    "    # Validation\n",
    "    for x_batch, y_batch in val_dataset:\n",
    "        val_step(x_batch, y_batch)\n",
    "\n",
    "    val_acc = val_acc_metric.result()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} -- \"\n",
    "          f\"loss: {loss:.4f}, \"\n",
    "          f\"train_acc: {train_acc:.4f}, \"\n",
    "          f\"val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Reset metrics\n",
    "    train_acc_metric.reset_state()\n",
    "    val_acc_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Understanding `@tf.function` and Tracing\n",
    "\n",
    "`@tf.function` converts a Python function into a TensorFlow graph. This can dramatically speed up execution because:\n",
    "\n",
    "1. The graph is optimized (constant folding, operator fusion)\n",
    "2. Operations run in C++ without Python overhead\n",
    "3. Graphs can be exported for serving\n",
    "\n",
    "**Tracing**: The first time a `@tf.function`-decorated function is called, TensorFlow \"traces\" it -- running the Python code once to record the operations. Subsequent calls with the same input signature reuse the compiled graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating @tf.function tracing\n",
    "@tf.function\n",
    "def my_function(x):\n",
    "    print(\"Tracing!\")  # Only printed during tracing, NOT on every call\n",
    "    return x * x + 2 * x + 1\n",
    "\n",
    "# First call -- traces the function\n",
    "print(\"Call 1:\")\n",
    "result1 = my_function(tf.constant(3.0))\n",
    "print(f\"  Result: {result1.numpy()}\")\n",
    "\n",
    "# Second call with same dtype/shape -- reuses the traced graph\n",
    "print(\"\\nCall 2 (same signature -- no retracing):\")\n",
    "result2 = my_function(tf.constant(4.0))\n",
    "print(f\"  Result: {result2.numpy()}\")\n",
    "\n",
    "# Call with different dtype -- triggers retracing\n",
    "print(\"\\nCall 3 (different dtype -- retraces):\")\n",
    "result3 = my_function(tf.constant(5))\n",
    "print(f\"  Result: {result3.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n",
    "## 7. Callbacks System\n",
    "\n",
    "Callbacks allow you to hook into the training process at various points. TF/Keras provides many built-in callbacks, and you can create custom ones.\n",
    "\n",
    "### 7.1 Built-in Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Create a temporary directory for checkpoints and logs\n",
    "tmpdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "    # Stop training when validation loss stops improving\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Save the best model\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(tmpdir, 'best_model.keras'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Reduce learning rate when a metric has stopped improving\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # TensorBoard logging\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(tmpdir, 'logs'),\n",
    "        histogram_freq=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "for cb in callbacks:\n",
    "    print(f\"  - {cb.__class__.__name__}\")\n",
    "print(f\"\\nCheckpoint dir: {tmpdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Custom Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetailedProgressCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Custom callback that tracks and displays detailed training progress.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.history = {'loss': [], 'val_loss': [], 'lr': []}\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Training started\")\n",
    "        print(f\"Optimizer: {self.model.optimizer.__class__.__name__}\")\n",
    "        print(f\"Trainable parameters: {self.model.count_params():,}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.history['loss'].append(logs['loss'])\n",
    "        self.history['val_loss'].append(logs.get('val_loss', 0))\n",
    "        self.history['lr'].append(lr)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"  Epoch {epoch}: loss={logs['loss']:.4f}, \"\n",
    "                  f\"val_loss={logs.get('val_loss', 0):.4f}, lr={lr:.6f}\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Training complete after {len(self.history['loss'])} epochs\")\n",
    "        print(f\"Best loss: {min(self.history['loss']):.4f}\")\n",
    "        if self.history['val_loss'] and any(v > 0 for v in self.history['val_loss']):\n",
    "            print(f\"Best val_loss: {min(v for v in self.history['val_loss'] if v > 0):.4f}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Quick demo of the custom callback\n",
    "cb_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "cb_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "progress_cb = DetailedProgressCallback()\n",
    "cb_model.fit(\n",
    "    X_synthetic, y_synthetic,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[progress_cb],\n",
    "    verbose=0  # Suppress default output; our callback handles it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n",
    "## 8. tf.data Pipeline\n",
    "\n",
    "The `tf.data` API makes it easy to build efficient, scalable input pipelines. It handles batching, shuffling, prefetching, and parallel data loading.\n",
    "\n",
    "### 8.1 Creating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- From NumPy arrays ----\n",
    "X_data = np.random.randn(100, 10).astype(np.float32)\n",
    "y_data = np.random.randint(0, 2, 100)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
    "print(f\"Dataset element spec: {dataset.element_spec}\")\n",
    "print(f\"Dataset cardinality: {dataset.cardinality().numpy()}\")\n",
    "\n",
    "# Inspect first element\n",
    "for x, y in dataset.take(1):\n",
    "    print(f\"\\nFirst element -- x shape: {x.shape}, y: {y.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- From a generator (useful for large datasets that do not fit in memory) ----\n",
    "def data_generator():\n",
    "    for i in range(50):\n",
    "        yield np.random.randn(10).astype(np.float32), np.int32(i % 5)\n",
    "\n",
    "gen_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(10,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "for x, y in gen_dataset.take(2):\n",
    "    print(f\"Generator sample -- x shape: {x.shape}, label: {y.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Transformations: map, batch, shuffle, prefetch, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Complete pipeline example ----\n",
    "def preprocess(x, y):\n",
    "    \"\"\"Example preprocessing: normalize features.\"\"\"\n",
    "    x = (x - tf.reduce_mean(x)) / (tf.math.reduce_std(x) + 1e-7)\n",
    "    return x, y\n",
    "\n",
    "def create_dataset(images, labels, batch_size=32, is_training=True):\n",
    "    \"\"\"Create an optimized tf.data pipeline.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "    # Cache the dataset in memory after the first epoch\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=10000)\n",
    "\n",
    "    # Apply preprocessing in parallel\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Batch the data\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Prefetch the next batch while the current one is being processed\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_ds = create_dataset(X_data[:80], y_data[:80], batch_size=16, is_training=True)\n",
    "val_ds = create_dataset(X_data[80:], y_data[80:], batch_size=16, is_training=False)\n",
    "\n",
    "print(\"Pipeline element spec:\")\n",
    "print(f\"  {train_ds.element_spec}\")\n",
    "\n",
    "for x_batch, y_batch in train_ds.take(1):\n",
    "    print(f\"\\nBatch shapes -- x: {x_batch.shape}, y: {y_batch.shape}\")\n",
    "    print(f\"Mean after normalization: {tf.reduce_mean(x_batch).numpy():.4f} (should be near 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Performance Tips for tf.data ----\n",
    "print(\"tf.data Performance Best Practices:\")\n",
    "print(\"=\"* 50)\n",
    "print(\"1. .cache()       -- Cache data in memory after first read\")\n",
    "print(\"2. .shuffle(N)    -- Shuffle with buffer size N (use large N for good randomness)\")\n",
    "print(\"3. .map(fn, num_parallel_calls=AUTOTUNE)\")\n",
    "print(\"                  -- Parallelize map operations\")\n",
    "print(\"4. .batch(N)      -- Batch after shuffle and map\")\n",
    "print(\"5. .prefetch(AUTOTUNE)\")\n",
    "print(\"                  -- Overlap data loading with training\")\n",
    "print()\n",
    "print(\"Recommended order: cache -> shuffle -> map -> batch -> prefetch\")\n",
    "print(f\"\\nAUTOTUNE value: {tf.data.AUTOTUNE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"9\"></a>\n",
    "## 9. Practical Example: MNIST Classifier\n",
    "\n",
    "A complete end-to-end example: load real data, build a model, train, evaluate, and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load and preprocess MNIST ----\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "print(f\"Training set:   {x_train.shape}, labels: {y_train.shape}\")\n",
    "print(f\"Test set:       {x_test.shape}, labels: {y_test.shape}\")\n",
    "print(f\"Pixel range:    [{x_train.min()}, {x_train.max()}]\")\n",
    "print(f\"Label range:    [{y_train.min()}, {y_train.max()}]\")\n",
    "print(f\"Label distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize some samples ----\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(x_train[i], cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_train[i]}\", fontsize=12)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST Sample Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Build model ----\n",
    "mnist_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "mnist_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Train ----\n",
    "mnist_history = mnist_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, verbose=1)\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot Training History ----\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(mnist_history.history['loss'], label='Train Loss', linewidth=2)\n",
    "ax1.plot(mnist_history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(mnist_history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "ax2.plot(mnist_history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training & Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('MNIST Training History', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Evaluate on test set ----\n",
    "test_loss, test_acc = mnist_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test loss:     {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Confusion Matrix ----\n",
    "y_pred_probs = mnist_model.predict(x_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(y_test, y_pred, num_classes=10).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(confusion_mtx, interpolation='nearest', cmap='Blues')\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(10)\n",
    "plt.xticks(tick_marks, tick_marks)\n",
    "plt.yticks(tick_marks, tick_marks)\n",
    "\n",
    "# Add text annotations\n",
    "thresh = confusion_mtx.max() / 2.0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        plt.text(j, i, format(confusion_mtx[i, j], 'd'),\n",
    "                 ha='center', va='center',\n",
    "                 color='white' if confusion_mtx[i, j] > thresh else 'black')\n",
    "\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for cls in range(10):\n",
    "    cls_mask = y_test == cls\n",
    "    cls_acc = np.mean(y_pred[cls_mask] == y_test[cls_mask])\n",
    "    print(f\"  Digit {cls}: {cls_acc:.4f} ({np.sum(cls_mask)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize Predictions ----\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "\n",
    "# Pick some random test samples\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(len(x_test), 10, replace=False)\n",
    "\n",
    "for i, (ax, idx) in enumerate(zip(axes.flat, indices)):\n",
    "    ax.imshow(x_test[idx], cmap='gray')\n",
    "    pred_label = y_pred[idx]\n",
    "    true_label = y_test[idx]\n",
    "    confidence = y_pred_probs[idx][pred_label] * 100\n",
    "\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    ax.set_title(f\"Pred: {pred_label} ({confidence:.1f}%)\\nTrue: {true_label}\",\n",
    "                 fontsize=10, color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Predictions (green=correct, red=incorrect)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"10\"></a>\n",
    "## 10. Practical Example: Linear Regression (Updated from TF1)\n",
    "\n",
    "The original TF1.x notebook used `tf.placeholder`, `tf.Session`, and manual optimizer calls. Here is the modern TF2 equivalent.\n",
    "\n",
    "### Original TF1.x Code (for reference)\n",
    "```python\n",
    "# TF 1.x -- DO NOT RUN\n",
    "W = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x: [1,2,3,4], y: [0,-1,-2,-3]})\n",
    "```\n",
    "\n",
    "### Modern TF2 Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Modern Linear Regression with Keras ----\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([0, -1, -2, -3], dtype=np.float32)\n",
    "\n",
    "# A single Dense layer with 1 unit IS a linear regression: y = Wx + b\n",
    "lr_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(1,))\n",
    "])\n",
    "\n",
    "lr_model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "lr_history = lr_model.fit(X, Y, epochs=1000, verbose=0)\n",
    "\n",
    "W, b = lr_model.layers[0].get_weights()\n",
    "print(f\"Learned parameters:\")\n",
    "print(f\"  W: {W.flatten()[0]:.4f}  (expected: -1.0)\")\n",
    "print(f\"  b: {b[0]:.4f}  (expected: 1.0)\")\n",
    "print(f\"  Final loss: {lr_history.history['loss'][-1]:.6f}\")\n",
    "\n",
    "# Predictions\n",
    "predictions = lr_model.predict(X, verbose=0)\n",
    "print(f\"\\nPredictions vs Ground Truth:\")\n",
    "for xi, yi, pi in zip(X, Y, predictions.flatten()):\n",
    "    print(f\"  x={xi:.0f}: predicted={pi:.4f}, actual={yi:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize the fit ----\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot the regression line\n",
    "x_plot = np.linspace(0, 5, 100)\n",
    "y_plot = W.flatten()[0] * x_plot + b[0]\n",
    "\n",
    "ax1.scatter(X, Y, c='red', s=100, zorder=5, label='Data points')\n",
    "ax1.plot(x_plot, y_plot, 'b-', linewidth=2, label=f'Fit: y = {W.flatten()[0]:.3f}x + {b[0]:.3f}')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Linear Regression Fit')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot the loss curve\n",
    "ax2.plot(lr_history.history['loss'], linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MSE Loss')\n",
    "ax2.set_title('Training Loss Over Epochs')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Linear Regression (Modern TF2)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Linear Regression with GradientTape (manual approach) ----\n",
    "# This shows the low-level approach equivalent to the TF1.x version\n",
    "\n",
    "W_manual = tf.Variable(0.3, dtype=tf.float32, name='W')\n",
    "b_manual = tf.Variable(-0.3, dtype=tf.float32, name='b')\n",
    "\n",
    "X_t = tf.constant([1.0, 2.0, 3.0, 4.0])\n",
    "Y_t = tf.constant([0.0, -1.0, -2.0, -3.0])\n",
    "\n",
    "optimizer_manual = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "losses_manual = []\n",
    "for step in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = W_manual * X_t + b_manual\n",
    "        loss = tf.reduce_sum(tf.square(predictions - Y_t))\n",
    "\n",
    "    gradients = tape.gradient(loss, [W_manual, b_manual])\n",
    "    optimizer_manual.apply_gradients(zip(gradients, [W_manual, b_manual]))\n",
    "    losses_manual.append(loss.numpy())\n",
    "\n",
    "print(\"Manual GradientTape Linear Regression:\")\n",
    "print(f\"  W: {W_manual.numpy():.4f}  (expected: -1.0)\")\n",
    "print(f\"  b: {b_manual.numpy():.4f}  (expected: 1.0)\")\n",
    "print(f\"  Final loss: {losses_manual[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"11\"></a>\n",
    "## 11. Saving & Loading Models\n",
    "\n",
    "TensorFlow provides multiple ways to save models depending on your use case.\n",
    "\n",
    "### 11.1 SavedModel Format (Recommended for Production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "save_dir = tempfile.mkdtemp()\n",
    "\n",
    "# ---- SavedModel format (TF's native format) ----\n",
    "# Includes: architecture, weights, optimizer state, computation graph\n",
    "savedmodel_path = os.path.join(save_dir, 'mnist_savedmodel')\n",
    "mnist_model.save(savedmodel_path)\n",
    "print(f\"SavedModel saved to: {savedmodel_path}\")\n",
    "\n",
    "# Load it back\n",
    "loaded_savedmodel = tf.keras.models.load_model(savedmodel_path)\n",
    "loaded_loss, loaded_acc = loaded_savedmodel.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Loaded SavedModel -- Test accuracy: {loaded_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Keras Format (.keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Keras native format (.keras) ----\n",
    "keras_path = os.path.join(save_dir, 'mnist_model.keras')\n",
    "mnist_model.save(keras_path)\n",
    "print(f\"Keras model saved to: {keras_path}\")\n",
    "\n",
    "loaded_keras = tf.keras.models.load_model(keras_path)\n",
    "keras_loss, keras_acc = loaded_keras.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Loaded Keras model -- Test accuracy: {keras_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Saving Weights Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Weights only ----\n",
    "# Useful when you want to save just the learned parameters\n",
    "weights_path = os.path.join(save_dir, 'mnist_weights.weights.h5')\n",
    "mnist_model.save_weights(weights_path)\n",
    "print(f\"Weights saved to: {weights_path}\")\n",
    "\n",
    "# To load weights, you need to recreate the model architecture first\n",
    "new_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "new_model.load_weights(weights_path)\n",
    "new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "wt_loss, wt_acc = new_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Weights-only reload -- Test accuracy: {wt_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 TF Lite Conversion Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- TF Lite conversion ----\n",
    "# Convert a saved model to TF Lite for mobile/edge deployment\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(savedmodel_path)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = os.path.join(save_dir, 'mnist_model.tflite')\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "original_size = os.path.getsize(keras_path)\n",
    "tflite_size = os.path.getsize(tflite_path)\n",
    "\n",
    "print(f\"TF Lite model saved to: {tflite_path}\")\n",
    "print(f\"Keras model size:  {original_size / 1024:.1f} KB\")\n",
    "print(f\"TF Lite model size: {tflite_size / 1024:.1f} KB\")\n",
    "print(f\"Size reduction: {(1 - tflite_size / original_size) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Summary of save formats ----\n",
    "print(\"Model Saving Formats Summary\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Format':<20} {'Includes':<30} {'Use Case'}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'SavedModel':<20} {'Arch + Weights + Graph':<30} {'Production serving'}\")\n",
    "print(f\"{'.keras':<20} {'Arch + Weights + Config':<30} {'General purpose'}\")\n",
    "print(f\"{'Weights (.h5)':<20} {'Weights only':<30} {'Transfer learning'}\")\n",
    "print(f\"{'TF Lite (.tflite)':<20} {'Optimized weights + graph':<30} {'Mobile / Edge'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"12\"></a>\n",
    "## 12. Performance Tips\n",
    "\n",
    "### 12.1 `@tf.function` Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ---- Comparing eager vs @tf.function performance ----\n",
    "def eager_computation(x):\n",
    "    for _ in range(100):\n",
    "        x = tf.nn.relu(tf.matmul(x, tf.random.normal([100, 100])))\n",
    "    return x\n",
    "\n",
    "@tf.function\n",
    "def graph_computation(x):\n",
    "    for _ in range(100):\n",
    "        x = tf.nn.relu(tf.matmul(x, tf.random.normal([100, 100])))\n",
    "    return x\n",
    "\n",
    "x_perf = tf.random.normal([10, 100])\n",
    "\n",
    "# Warm up the graph function (first call traces)\n",
    "_ = graph_computation(x_perf)\n",
    "\n",
    "# Time eager execution\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = eager_computation(x_perf)\n",
    "eager_time = time.time() - start\n",
    "\n",
    "# Time graph execution\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = graph_computation(x_perf)\n",
    "graph_time = time.time() - start\n",
    "\n",
    "print(f\"Eager execution:  {eager_time:.4f}s\")\n",
    "print(f\"@tf.function:     {graph_time:.4f}s\")\n",
    "print(f\"Speedup:          {eager_time / graph_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- @tf.function Best Practices ----\n",
    "print(\"@tf.function Best Practices\")\n",
    "print(\"=\" * 55)\n",
    "print()\n",
    "print(\"DO:\")\n",
    "print(\"  - Use for training steps, inference, and any\")\n",
    "print(\"    computation-heavy function\")\n",
    "print(\"  - Use tf.TensorSpec or input_signature to control\")\n",
    "print(\"    retracing\")\n",
    "print(\"  - Use tf.print() instead of print() inside\")\n",
    "print(\"    @tf.function\")\n",
    "print(\"  - Use tf.cond/tf.while_loop for control flow\")\n",
    "print()\n",
    "print(\"DO NOT:\")\n",
    "print(\"  - Use Python side effects (list.append, dict update)\")\n",
    "print(\"  - Create tf.Variable inside @tf.function\")\n",
    "print(\"  - Pass Python objects that change between calls\")\n",
    "print(\"  - Use Python print() for runtime debugging (only\")\n",
    "print(\"    runs during tracing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Mixed Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Mixed Precision Training ----\n",
    "# Uses float16 for computation and float32 for accumulation.\n",
    "# Can provide 2-3x speedup on modern GPUs (V100, A100, T4).\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Check current policy\n",
    "print(f\"Current dtype policy: {mixed_precision.global_policy()}\")\n",
    "\n",
    "# To enable mixed precision (uncomment on GPU):\n",
    "# mixed_precision.set_global_policy('mixed_float16')\n",
    "# print(f\"New dtype policy: {mixed_precision.global_policy()}\")\n",
    "\n",
    "# Build a model with mixed precision\n",
    "# The compute dtype will be float16 but the variable dtype remains float32\n",
    "# You MUST use a float32 output (or cast) for numerical stability in the loss\n",
    "\n",
    "print(\"\\nMixed Precision Notes:\")\n",
    "print(\"  - Speeds up training on GPUs with Tensor Cores (V100, A100, T4)\")\n",
    "print(\"  - Forward/backward pass uses float16 for speed\")\n",
    "print(\"  - Weight updates use float32 for numerical stability\")\n",
    "print(\"  - The final Dense layer should output float32\")\n",
    "print(\"  - Use tf.keras.mixed_precision.set_global_policy('mixed_float16')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 XLA Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- XLA (Accelerated Linear Algebra) ----\n",
    "# XLA compiles TF operations into optimized machine code.\n",
    "# It can fuse operations, eliminate dead code, and optimize memory.\n",
    "\n",
    "# Method 1: jit_compile on tf.function\n",
    "@tf.function(jit_compile=True)\n",
    "def xla_computation(x):\n",
    "    return tf.nn.relu(tf.matmul(x, tf.ones([100, 100])) + 1.0)\n",
    "\n",
    "result = xla_computation(tf.random.normal([5, 100]))\n",
    "print(f\"XLA computation result shape: {result.shape}\")\n",
    "\n",
    "# Method 2: Set jit_compile in model.compile\n",
    "# model.compile(optimizer='adam', loss='mse', jit_compile=True)\n",
    "\n",
    "print(\"\\nXLA Compilation Notes:\")\n",
    "print(\"  - Fuses multiple operations into single kernels\")\n",
    "print(\"  - Reduces memory overhead\")\n",
    "print(\"  - Works best with fixed-shape tensors\")\n",
    "print(\"  - Can cause slowdowns if shapes change frequently\")\n",
    "print(\"  - Enable with @tf.function(jit_compile=True) or\")\n",
    "print(\"    model.compile(..., jit_compile=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4 Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- GPU Memory Management ----\n",
    "print(\"GPU Memory Management Strategies\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "1. Memory Growth (recommended):\n",
    "   Allow GPU memory to grow as needed instead of\n",
    "   pre-allocating all memory.\n",
    "\n",
    "   gpus = tf.config.list_physical_devices('GPU')\n",
    "   for gpu in gpus:\n",
    "       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "2. Memory Limit:\n",
    "   Restrict TensorFlow to a fixed amount of GPU memory.\n",
    "\n",
    "   tf.config.set_logical_device_configuration(\n",
    "       gpus[0],\n",
    "       [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]  # 4 GB\n",
    "   )\n",
    "\n",
    "3. Gradient Checkpointing:\n",
    "   Trade compute for memory by recomputing activations\n",
    "   during the backward pass instead of storing them.\n",
    "\n",
    "   # Apply to specific layers:\n",
    "   tf.recompute_grad(my_function)\n",
    "\n",
    "4. Reduce Batch Size:\n",
    "   The simplest way to reduce memory usage.\n",
    "\n",
    "5. Use tf.data Pipelines:\n",
    "   Load data in batches instead of all at once.\n",
    "\n",
    "IMPORTANT: Memory growth must be set BEFORE any GPU\n",
    "operations are performed (typically at the top of\n",
    "your script).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"13\"></a>\n",
    "## 13. Exercises\n",
    "\n",
    "Practice what you have learned by completing the following exercises.\n",
    "\n",
    "### Exercise 1: Implement a Custom Layer\n",
    "\n",
    "Create a custom Dense layer from scratch by subclassing `tf.keras.layers.Layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"A custom dense (fully connected) layer built from scratch.\n",
    "    \n",
    "    Exercise: Complete the implementation.\n",
    "    The layer should:\n",
    "    1. Create weight matrix W and bias vector b in build()\n",
    "    2. Compute output = activation(input @ W + b) in call()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # TODO: Create self.w with shape (input_shape[-1], self.units)\n",
    "        #       using self.add_weight(). Use 'glorot_uniform' initializer.\n",
    "        # TODO: Create self.b with shape (self.units,)\n",
    "        #       using self.add_weight(). Use 'zeros' initializer.\n",
    "        pass\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # TODO: Compute z = inputs @ self.w + self.b\n",
    "        # TODO: Apply activation if it exists\n",
    "        # TODO: Return the result\n",
    "        pass\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'units': self.units, 'activation': self.activation})\n",
    "        return config\n",
    "\n",
    "\n",
    "# Test your implementation:\n",
    "# layer = MyDenseLayer(64, activation='relu')\n",
    "# output = layer(tf.random.normal([2, 128]))\n",
    "# print(f\"Output shape: {output.shape}\")  # Should be (2, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Multi-Input Model\n",
    "\n",
    "Build a model using the Functional API that takes two inputs:\n",
    "- Numerical features: shape (10,)\n",
    "- Categorical embedding: shape (5,)\n",
    "\n",
    "The model should merge both branches and output a single value (regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a multi-input model\n",
    "#\n",
    "# numerical_input = tf.keras.Input(shape=(10,), name='numerical')\n",
    "# categorical_input = tf.keras.Input(shape=(5,), name='categorical')\n",
    "#\n",
    "# Numerical branch: Dense(32, relu) -> Dense(16, relu)\n",
    "# Categorical branch: Dense(16, relu)\n",
    "# Merge: concatenate both branches\n",
    "# Output head: Dense(16, relu) -> Dense(1)  (no activation for regression)\n",
    "#\n",
    "# multi_model = tf.keras.Model(\n",
    "#     inputs=[numerical_input, categorical_input],\n",
    "#     outputs=output\n",
    "# )\n",
    "# multi_model.summary()\n",
    "\n",
    "print(\"Exercise: Implement the multi-input model described above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Learning Rate Finder\n",
    "\n",
    "Implement a learning rate finder that trains for one epoch, gradually increasing the learning rate, and plots loss vs. learning rate to find the optimal range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateFinder(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Finds the optimal learning rate range.\n",
    "    \n",
    "    Exercise: Complete the implementation.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Start with a very small learning rate (e.g., 1e-7)\n",
    "    2. Increase it exponentially after each batch\n",
    "    3. Record the loss at each step\n",
    "    4. Stop when the loss diverges (e.g., > 4x the minimum loss)\n",
    "    5. Plot loss vs. learning rate\n",
    "    6. The optimal LR is typically just before the minimum loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_lr=1e-7, max_lr=1.0):\n",
    "        super().__init__()\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # TODO: Calculate the multiplication factor for the LR schedule\n",
    "        # TODO: Set the initial learning rate\n",
    "        pass\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        # TODO: Record the current LR and loss\n",
    "        # TODO: Increase the LR exponentially\n",
    "        # TODO: Stop training if loss diverges\n",
    "        pass\n",
    "\n",
    "    def plot(self):\n",
    "        # TODO: Plot loss vs learning rate (use log scale for LR)\n",
    "        pass\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# lr_finder = LearningRateFinder(min_lr=1e-7, max_lr=1.0)\n",
    "# model.fit(x_train, y_train, epochs=1, callbacks=[lr_finder])\n",
    "# lr_finder.plot()\n",
    "\n",
    "print(\"Exercise: Implement the LearningRateFinder callback.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Custom Training Loop with Gradient Accumulation\n",
    "\n",
    "Implement a training loop that accumulates gradients over multiple mini-batches before applying an update. This is useful when you want a large effective batch size but do not have enough GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_gradient_accumulation(\n",
    "    model, dataset, optimizer, loss_fn,\n",
    "    accumulation_steps=4, epochs=5\n",
    "):\n",
    "    \"\"\"Train a model with gradient accumulation.\n",
    "    \n",
    "    Exercise: Complete the implementation.\n",
    "    \n",
    "    The effective batch size = mini_batch_size * accumulation_steps.\n",
    "    \n",
    "    Steps:\n",
    "    1. Initialize gradient accumulators (zeros_like for each trainable var)\n",
    "    2. For each mini-batch:\n",
    "       a. Compute gradients\n",
    "       b. Add them to the accumulators\n",
    "    3. Every `accumulation_steps` batches:\n",
    "       a. Divide accumulated gradients by accumulation_steps\n",
    "       b. Apply gradients to the model\n",
    "       c. Reset accumulators to zero\n",
    "    \"\"\"\n",
    "    # TODO: Implement gradient accumulation training loop\n",
    "    pass\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# train_with_gradient_accumulation(\n",
    "#     model=my_model,\n",
    "#     dataset=train_dataset,\n",
    "#     optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "#     loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#     accumulation_steps=4,\n",
    "#     epochs=5\n",
    "# )\n",
    "\n",
    "print(\"Exercise: Implement the gradient accumulation training loop.\")\n",
    "print(\"\\nHint: The key insight is that gradients are additive.\")\n",
    "print(\"Accumulating gradients from 4 batches of size 32 is equivalent\")\n",
    "print(\"to computing gradients on a single batch of size 128.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered the fundamentals of modern TensorFlow 2.x:\n",
    "\n",
    "| Section | Key Takeaway |\n",
    "|---------|-------------|\n",
    "| **Tensors** | Multi-dimensional arrays -- the fundamental data structure |\n",
    "| **Eager Execution** | Operations run immediately; no sessions needed |\n",
    "| **GradientTape** | Automatic differentiation for computing gradients |\n",
    "| **Model Building** | Sequential (simple), Functional (flexible), Subclassing (full control) |\n",
    "| **Training** | `model.fit()` for simplicity, custom loops for control |\n",
    "| **Callbacks** | Hook into training: EarlyStopping, Checkpoints, custom callbacks |\n",
    "| **tf.data** | Efficient input pipelines with shuffle, batch, prefetch |\n",
    "| **Saving** | SavedModel (production), .keras (general), weights-only (transfer) |\n",
    "| **Performance** | `@tf.function`, mixed precision, XLA, memory management |\n",
    "\n",
    "### Where to Go Next\n",
    "\n",
    "- **CNNs**: `tf.keras.layers.Conv2D` for image tasks\n",
    "- **RNNs/Transformers**: `tf.keras.layers.LSTM`, `tf.keras.layers.MultiHeadAttention`\n",
    "- **Transfer Learning**: Load pre-trained models from TF Hub or `tf.keras.applications`\n",
    "- **Distributed Training**: `tf.distribute.MirroredStrategy` for multi-GPU\n",
    "- **TF Extended (TFX)**: End-to-end production ML pipelines\n",
    "- **TensorFlow Probability**: Probabilistic reasoning and statistical analysis\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [TensorFlow Official Tutorials](https://www.tensorflow.org/tutorials)\n",
    "- [Keras Documentation](https://keras.io)\n",
    "- [TF Guide: Effective TF2](https://www.tensorflow.org/guide/effective_tf2)\n",
    "- [TF Guide: tf.function](https://www.tensorflow.org/guide/function)\n",
    "- [TF Guide: tf.data](https://www.tensorflow.org/guide/data)"
   ]
  }
 ]
}