{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced CNN Architectures: A Deep Dive\n",
    "\n",
    "**From LeNet to Vision Transformers -- The Complete Evolution of Visual Recognition**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a comprehensive, implementation-focused tour of the most influential\n",
    "convolutional neural network architectures. Each section includes:\n",
    "\n",
    "- The core **insight** that made the architecture important\n",
    "- A **from-scratch implementation** in TensorFlow / Keras\n",
    "- Practical **design guidelines** for your own projects\n",
    "\n",
    "**Audience:** Intermediate deep-learning practitioners through production ML engineers.\n",
    "\n",
    "**Prerequisites:** Basic understanding of convolutions, pooling, and backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Evolution Timeline\n",
    "\n",
    "```\n",
    "1998        2012        2014        2014        2015        2017        2019        2022        2020+\n",
    " |           |           |           |           |           |           |           |           |\n",
    "LeNet --> AlexNet --> VGG ----> GoogLeNet --> ResNet --> DenseNet --> EfficientNet --> ConvNeXt --> ViT / Swin\n",
    " |           |           |           |           |           |           |           |           |\n",
    "First     GPU +       Deeper     Multi-      Skip       Dense      Compound    Modernized   Self-\n",
    "practical Dropout +   uniform    scale       connections feature    scaling     CNNs with    attention\n",
    "CNN       ReLU +      3x3        feature     solve      reuse      balances    Transformer  replaces\n",
    "          Data Aug    stacks     extraction  degradation            W x D x R  ideas        convolution\n",
    "```\n",
    "\n",
    "### Key Innovations at Each Stage\n",
    "\n",
    "| Architecture | Year | Key Innovation | Parameters | ImageNet Top-1 Acc |\n",
    "|:------------|:----:|:---------------|:----------:|:------------------:|\n",
    "| **LeNet-5** | 1998 | First practical CNN (digit recognition) | 60 K | -- |\n",
    "| **AlexNet** | 2012 | GPU training, ReLU, Dropout, Data Augmentation | 61 M | 63.3 % |\n",
    "| **VGG-16** | 2014 | Uniform 3x3 convolutions, deeper networks | 138 M | 74.4 % |\n",
    "| **GoogLeNet / Inception v1** | 2014 | Multi-scale (Inception) modules, 1x1 conv | 6.8 M | 74.8 % |\n",
    "| **ResNet-50** | 2015 | Residual (skip) connections | 25.6 M | 76.0 % |\n",
    "| **DenseNet-121** | 2017 | Dense connections -- every layer connects to every later layer | 8 M | 74.9 % |\n",
    "| **EfficientNet-B0** | 2019 | Compound scaling of width, depth, resolution | 5.3 M | 77.1 % |\n",
    "| **EfficientNet-B7** | 2019 | Scaled-up compound model | 66 M | 84.3 % |\n",
    "| **ConvNeXt-T** | 2022 | Transformer design principles applied to pure CNNs | 29 M | 82.1 % |\n",
    "| **ViT-B/16** | 2020 | Pure self-attention on image patches | 86 M | 77.9 % (ImageNet-1K only) |\n",
    "| **Swin-T** | 2021 | Shifted-window local attention + hierarchical features | 29 M | 81.3 % |\n",
    "\n",
    "> **Takeaway:** More parameters do not automatically mean higher accuracy. Architectural\n",
    "> innovations -- skip connections, multi-scale processing, attention, and principled scaling --\n",
    "> matter more than raw size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(f\"TensorFlow : {tf.__version__}\")\n",
    "print(f\"Keras      : {keras.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. VGG -- \"Deeper Is Better\" with Uniform 3x3 Convolutions\n",
    "\n",
    "### Core Insight\n",
    "\n",
    "The VGG team (Simonyan & Zisserman, 2014) showed that **stacking many small (3x3)\n",
    "convolution filters** is more effective than using fewer large filters:\n",
    "\n",
    "- Two stacked 3x3 convolutions have the **same receptive field** as one 5x5\n",
    "  convolution, but use fewer parameters (2 x 3x3x C^2 = 18C^2 vs. 25C^2) and\n",
    "  introduce an extra non-linearity.\n",
    "- Three stacked 3x3 convolutions match a 7x7 receptive field.\n",
    "\n",
    "### Architecture Pattern\n",
    "\n",
    "```\n",
    "[Conv3x3 -> Conv3x3 -> MaxPool] x 2   (64, 128 filters)\n",
    "[Conv3x3 -> Conv3x3 -> Conv3x3 -> MaxPool] x 3   (256, 512, 512 filters)\n",
    "Flatten -> FC-4096 -> FC-4096 -> FC-1000 -> Softmax\n",
    "```\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **138 M parameters** -- most in the fully-connected layers\n",
    "- No skip connections, so gradients vanish beyond ~20 layers\n",
    "- No batch normalization (published before BN was introduced)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_vgg16(input_shape=(224, 224, 3), num_classes=1000):\n",
    "    \"\"\"Build VGG-16 from scratch using the Keras Functional API.\"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu', name='block1_conv1')(inputs)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu', name='block1_conv2')(x)\n",
    "    x = layers.MaxPooling2D(2, strides=2, name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu', name='block2_conv1')(x)\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu', name='block2_conv2')(x)\n",
    "    x = layers.MaxPooling2D(2, strides=2, name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(256, 3, padding='same', activation='relu', name='block3_conv1')(x)\n",
    "    x = layers.Conv2D(256, 3, padding='same', activation='relu', name='block3_conv2')(x)\n",
    "    x = layers.Conv2D(256, 3, padding='same', activation='relu', name='block3_conv3')(x)\n",
    "    x = layers.MaxPooling2D(2, strides=2, name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(512, 3, padding='same', activation='relu', name='block4_conv1')(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', activation='relu', name='block4_conv2')(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', activation='relu', name='block4_conv3')(x)\n",
    "    x = layers.MaxPooling2D(2, strides=2, name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = layers.Conv2D(512, 3, padding='same', activation='relu', name='block5_conv1')(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', activation='relu', name='block5_conv2')(x)\n",
    "    x = layers.Conv2D(512, 3, padding='same', activation='relu', name='block5_conv3')(x)\n",
    "    x = layers.MaxPooling2D(2, strides=2, name='block5_pool')(x)\n",
    "\n",
    "    # Classifier head\n",
    "    x = layers.Flatten(name='flatten')(x)\n",
    "    x = layers.Dense(4096, activation='relu', name='fc1')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(4096, activation='relu', name='fc2')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    return Model(inputs, outputs, name='VGG16')\n",
    "\n",
    "\n",
    "vgg16 = build_vgg16()\n",
    "vgg16.summary(show_trainable=True, expand_nested=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Parameter distribution analysis\n",
    "conv_params = sum(l.count_params() for l in vgg16.layers if 'conv' in l.name)\n",
    "fc_params   = sum(l.count_params() for l in vgg16.layers if 'fc' in l.name or 'predictions' in l.name)\n",
    "total       = vgg16.count_params()\n",
    "\n",
    "print(f\"Convolutional parameters : {conv_params:>12,}  ({100*conv_params/total:.1f}%)\")\n",
    "print(f\"Fully-connected parameters: {fc_params:>12,}  ({100*fc_params/total:.1f}%)\")\n",
    "print(f\"Total parameters          : {total:>12,}\")\n",
    "print()\n",
    "print(\"Note: ~90% of VGG-16 parameters live in the FC layers.\")\n",
    "print(\"This is one reason modern architectures replaced FC layers with Global Average Pooling.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ResNet -- The Skip Connection Revolution\n",
    "\n",
    "### The Degradation Problem\n",
    "\n",
    "Before ResNet, researchers observed a surprising phenomenon: **adding more layers to\n",
    "a deep network eventually _increased_ the training error** (not just test error).\n",
    "This is not caused by overfitting -- it is a fundamental optimization difficulty.\n",
    "\n",
    "If a shallower network can reach a certain accuracy, a deeper network should be able\n",
    "to do at least as well by learning identity mappings in the extra layers. In practice,\n",
    "optimizers struggle to learn these identity mappings through stacked non-linear layers.\n",
    "\n",
    "### The Residual Learning Solution\n",
    "\n",
    "He et al. (2015) introduced **skip connections** (also called shortcut or residual\n",
    "connections):\n",
    "\n",
    "```\n",
    "x ----+----> Conv -> BN -> ReLU -> Conv -> BN ---(+)---> ReLU -> output\n",
    "      |                                           |\n",
    "      +-------------- identity --------------------+\n",
    "```\n",
    "\n",
    "Instead of learning a mapping `H(x)` directly, the network learns the **residual**\n",
    "`F(x) = H(x) - x`. If the identity mapping is optimal, the network only needs to\n",
    "drive `F(x)` to zero, which is much easier.\n",
    "\n",
    "### Two Types of Shortcuts\n",
    "\n",
    "1. **Identity shortcut** -- used when input and output dimensions match.\n",
    "2. **Projection shortcut** -- a 1x1 convolution with stride to match dimensions\n",
    "   when spatial size or channel count changes.\n",
    "\n",
    "### ResNet Bottleneck Block (used in ResNet-50/101/152)\n",
    "\n",
    "```\n",
    "1x1 Conv (reduce channels) -> 3x3 Conv -> 1x1 Conv (expand channels)\n",
    "```\n",
    "\n",
    "This reduces computation compared to using two 3x3 convolutions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ResidualBlock(layers.Layer):\n",
    "    \"\"\"Bottleneck residual block for ResNet-50/101/152.\n",
    "    \n",
    "    Structure: 1x1 -> 3x3 -> 1x1 with optional projection shortcut.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filters, stride=1, use_projection=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.stride = stride\n",
    "\n",
    "        # Bottleneck layers\n",
    "        self.conv1 = layers.Conv2D(filters, 1, strides=stride, padding='same', use_bias=False)\n",
    "        self.bn1   = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2D(filters, 3, padding='same', use_bias=False)\n",
    "        self.bn2   = layers.BatchNormalization()\n",
    "        self.conv3 = layers.Conv2D(filters * 4, 1, padding='same', use_bias=False)\n",
    "        self.bn3   = layers.BatchNormalization()\n",
    "\n",
    "        # Projection shortcut (when dimensions change)\n",
    "        self.projection = None\n",
    "        if use_projection:\n",
    "            self.projection = keras.Sequential([\n",
    "                layers.Conv2D(filters * 4, 1, strides=stride, use_bias=False),\n",
    "                layers.BatchNormalization()\n",
    "            ])\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        shortcut = x\n",
    "\n",
    "        x = tf.nn.relu(self.bn1(self.conv1(x), training=training))\n",
    "        x = tf.nn.relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = self.bn3(self.conv3(x), training=training)  # No ReLU before addition\n",
    "\n",
    "        if self.projection is not None:\n",
    "            shortcut = self.projection(shortcut, training=training)\n",
    "\n",
    "        return tf.nn.relu(x + shortcut)\n",
    "\n",
    "\n",
    "def build_resnet50(input_shape=(224, 224, 3), num_classes=1000):\n",
    "    \"\"\"Build ResNet-50 from scratch.\"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Stem\n",
    "    x = layers.Conv2D(64, 7, strides=2, padding='same', use_bias=False, name='stem_conv')(inputs)\n",
    "    x = layers.BatchNormalization(name='stem_bn')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "\n",
    "    # Residual stages: (filters, num_blocks, first_block_stride)\n",
    "    stage_configs = [\n",
    "        (64,  3, 1),   # Stage 2  -- no downsampling (already pooled)\n",
    "        (128, 4, 2),   # Stage 3\n",
    "        (256, 6, 2),   # Stage 4\n",
    "        (512, 3, 2),   # Stage 5\n",
    "    ]\n",
    "\n",
    "    for stage_idx, (filters, num_blocks, stride) in enumerate(stage_configs):\n",
    "        # First block may change dimensions\n",
    "        x = ResidualBlock(filters, stride=stride, use_projection=True,\n",
    "                          name=f'stage{stage_idx+2}_block1')(x)\n",
    "        for block_idx in range(1, num_blocks):\n",
    "            x = ResidualBlock(filters, name=f'stage{stage_idx+2}_block{block_idx+1}')(x)\n",
    "\n",
    "    # Head\n",
    "    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    return Model(inputs, outputs, name='ResNet50')\n",
    "\n",
    "\n",
    "resnet50 = build_resnet50()\n",
    "print(f\"ResNet-50 total parameters: {resnet50.count_params():,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Gradient Flow: With vs. Without Skip Connections\n",
    "\n",
    "The following simulation illustrates how gradients propagate through a deep network\n",
    "**with** and **without** skip connections."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def simulate_gradient_flow(depth, use_skip=False, weight_scale=0.5):\n",
    "    \"\"\"Simulate gradient magnitudes through a chain of layers.\"\"\"\n",
    "    np.random.seed(0)\n",
    "    gradient = 1.0\n",
    "    gradients = [gradient]\n",
    "    for _ in range(depth):\n",
    "        layer_jacobian = np.random.randn() * weight_scale\n",
    "        if use_skip:\n",
    "            gradient = gradient * layer_jacobian + gradient  # residual path\n",
    "        else:\n",
    "            gradient = gradient * layer_jacobian\n",
    "        gradients.append(abs(gradient))\n",
    "    return gradients\n",
    "\n",
    "depth = 50\n",
    "grads_plain = simulate_gradient_flow(depth, use_skip=False)\n",
    "grads_skip  = simulate_gradient_flow(depth, use_skip=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].semilogy(grads_plain, 'r-', linewidth=2)\n",
    "axes[0].set_title('Plain Network (no skip connections)', fontsize=13)\n",
    "axes[0].set_xlabel('Layer depth')\n",
    "axes[0].set_ylabel('Gradient magnitude (log scale)')\n",
    "axes[0].axhline(1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_ylim([1e-20, 1e20])\n",
    "\n",
    "axes[1].semilogy(grads_skip, 'b-', linewidth=2)\n",
    "axes[1].set_title('Residual Network (with skip connections)', fontsize=13)\n",
    "axes[1].set_xlabel('Layer depth')\n",
    "axes[1].set_ylabel('Gradient magnitude (log scale)')\n",
    "axes[1].axhline(1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_ylim([1e-20, 1e20])\n",
    "\n",
    "plt.suptitle('Gradient Flow Simulation (50 layers)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plain network  -- final gradient magnitude: {grads_plain[-1]:.2e}\")\n",
    "print(f\"ResNet         -- final gradient magnitude: {grads_skip[-1]:.2e}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inception / GoogLeNet -- Multi-Scale Feature Extraction\n",
    "\n",
    "### Core Insight\n",
    "\n",
    "Szegedy et al. (2014) asked: _why choose a single filter size when you can use\n",
    "multiple sizes in parallel?_\n",
    "\n",
    "The **Inception module** applies 1x1, 3x3, and 5x5 convolutions **plus** max\n",
    "pooling simultaneously, then concatenates the results along the channel axis.\n",
    "\n",
    "### The 1x1 Convolution Trick\n",
    "\n",
    "Naive concatenation of multi-scale outputs explodes the channel count. The key\n",
    "innovation is using **1x1 convolutions as bottlenecks** to reduce dimensionality\n",
    "_before_ the expensive 3x3 and 5x5 operations:\n",
    "\n",
    "```\n",
    "Input (28x28x256)\n",
    "  |         |              |              |\n",
    "1x1(64)   1x1(96)        1x1(16)       MaxPool3x3\n",
    "  |       3x3(128)       5x5(32)        1x1(32)\n",
    "  |         |              |              |\n",
    "  +-------- Concat along channels --------+\n",
    "Output (28x28x256)  <-- controlled growth\n",
    "```\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "- GoogLeNet achieved **higher accuracy than VGG with 12x fewer parameters** (6.8M vs 138M)\n",
    "- The multi-scale philosophy influenced all later architectures\n",
    "- 1x1 convolutions became a standard building block"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class InceptionModule(layers.Layer):\n",
    "    \"\"\"Inception module with dimension reduction (Inception v1).\n",
    "    \n",
    "    Args:\n",
    "        f1:    filters for the 1x1 branch\n",
    "        f3_r:  filters for the 1x1 reduction before 3x3\n",
    "        f3:    filters for the 3x3 branch\n",
    "        f5_r:  filters for the 1x1 reduction before 5x5\n",
    "        f5:    filters for the 5x5 branch\n",
    "        fpool: filters for the 1x1 after max-pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f1, f3_r, f3, f5_r, f5, fpool, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Branch 1: 1x1\n",
    "        self.branch1 = layers.Conv2D(f1, 1, activation='relu', padding='same')\n",
    "\n",
    "        # Branch 2: 1x1 -> 3x3\n",
    "        self.branch2_reduce = layers.Conv2D(f3_r, 1, activation='relu', padding='same')\n",
    "        self.branch2_conv   = layers.Conv2D(f3,   3, activation='relu', padding='same')\n",
    "\n",
    "        # Branch 3: 1x1 -> 5x5\n",
    "        self.branch3_reduce = layers.Conv2D(f5_r, 1, activation='relu', padding='same')\n",
    "        self.branch3_conv   = layers.Conv2D(f5,   5, activation='relu', padding='same')\n",
    "\n",
    "        # Branch 4: MaxPool -> 1x1\n",
    "        self.branch4_pool = layers.MaxPooling2D(3, strides=1, padding='same')\n",
    "        self.branch4_conv = layers.Conv2D(fpool, 1, activation='relu', padding='same')\n",
    "\n",
    "    def call(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2_conv(self.branch2_reduce(x))\n",
    "        b3 = self.branch3_conv(self.branch3_reduce(x))\n",
    "        b4 = self.branch4_conv(self.branch4_pool(x))\n",
    "        return tf.concat([b1, b2, b3, b4], axis=-1)\n",
    "\n",
    "\n",
    "# Quick test -- Inception module with GoogLeNet inception_3a config\n",
    "inp = keras.Input(shape=(28, 28, 192))\n",
    "out = InceptionModule(64, 96, 128, 16, 32, 32, name='inception_3a')(inp)\n",
    "test_model = Model(inp, out)\n",
    "print(f\"Input shape : {inp.shape}\")\n",
    "print(f\"Output shape: {test_model.output_shape}\")\n",
    "print(f\"Parameters  : {test_model.count_params():,}\")\n",
    "print(f\"Output channels = 64 + 128 + 32 + 32 = {64+128+32+32}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DenseNet -- Dense Connections and Feature Reuse\n",
    "\n",
    "### Core Insight\n",
    "\n",
    "Huang et al. (2017) took skip connections to their logical extreme: **every layer\n",
    "receives feature maps from _all_ preceding layers** in its dense block.\n",
    "\n",
    "```\n",
    "x0 ----+---------+---------+---------->\n",
    "       |         |         |\n",
    "       v         |         |\n",
    "     [BN-ReLU-Conv]        |         |\n",
    "       = x1      |         |\n",
    "       |  +------+         |\n",
    "       v  v                |\n",
    "     [BN-ReLU-Conv]        |\n",
    "       = x2                |\n",
    "       |  +---+---+--------+\n",
    "       v  v   v   v\n",
    "     [BN-ReLU-Conv]\n",
    "       = x3\n",
    "```\n",
    "\n",
    "Each layer concatenates (not adds) all previous feature maps.\n",
    "\n",
    "### Growth Rate (k)\n",
    "\n",
    "Each layer produces `k` new feature maps (the **growth rate**). After `L` layers,\n",
    "the total channels are `k0 + L * k`. Typical `k = 32`.\n",
    "\n",
    "### Bottleneck Layer (DenseNet-B)\n",
    "\n",
    "To control computation: `BN -> ReLU -> 1x1 Conv(4k) -> BN -> ReLU -> 3x3 Conv(k)`\n",
    "\n",
    "### Transition Layer\n",
    "\n",
    "Between dense blocks: `BN -> 1x1 Conv (compression) -> 2x2 AvgPool`\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- Strong gradient flow (every layer has direct access to the loss gradient)\n",
    "- Feature reuse reduces parameter count\n",
    "- Implicit deep supervision\n",
    "- DenseNet-121 achieves comparable accuracy to ResNet with **fewer parameters**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DenseLayer(layers.Layer):\n",
    "    \"\"\"Single layer within a DenseNet dense block (BN-ReLU-1x1-BN-ReLU-3x3).\"\"\"\n",
    "\n",
    "    def __init__(self, growth_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.bn1   = layers.BatchNormalization()\n",
    "        self.conv1 = layers.Conv2D(4 * growth_rate, 1, use_bias=False, padding='same')\n",
    "        self.bn2   = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2D(growth_rate, 3, use_bias=False, padding='same')\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        out = self.conv1(tf.nn.relu(self.bn1(x, training=training)))\n",
    "        out = self.conv2(tf.nn.relu(self.bn2(out, training=training)))\n",
    "        return tf.concat([x, out], axis=-1)  # Dense concatenation\n",
    "\n",
    "\n",
    "class DenseBlock(layers.Layer):\n",
    "    \"\"\"A dense block consisting of `num_layers` dense layers.\"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, growth_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense_layers = [\n",
    "            DenseLayer(growth_rate, name=f'dense_layer_{i}')\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransitionLayer(layers.Layer):\n",
    "    \"\"\"Transition between dense blocks: 1x1 conv + 2x2 avg pool.\"\"\"\n",
    "\n",
    "    def __init__(self, out_channels, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.bn   = layers.BatchNormalization()\n",
    "        self.conv = layers.Conv2D(out_channels, 1, use_bias=False, padding='same')\n",
    "        self.pool = layers.AveragePooling2D(2, strides=2)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv(tf.nn.relu(self.bn(x, training=training)))\n",
    "        return self.pool(x)\n",
    "\n",
    "\n",
    "# Demonstrate a dense block\n",
    "inp = keras.Input(shape=(32, 32, 64))\n",
    "out = DenseBlock(num_layers=6, growth_rate=32, name='dense_block_1')(inp)\n",
    "demo = Model(inp, out)\n",
    "print(f\"Input channels : 64\")\n",
    "print(f\"Growth rate    : 32\")\n",
    "print(f\"Layers         : 6\")\n",
    "print(f\"Output channels: {demo.output_shape[-1]}  (64 + 6*32 = {64 + 6*32})\")\n",
    "print(f\"Parameters     : {demo.count_params():,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. EfficientNet -- Compound Scaling\n",
    "\n",
    "### Core Insight\n",
    "\n",
    "Tan & Le (2019) observed that network **width** (channels), **depth** (layers),\n",
    "and input **resolution** should be scaled together, not independently.\n",
    "\n",
    "### Compound Scaling Rule\n",
    "\n",
    "Given a compound coefficient `phi`:\n",
    "\n",
    "```\n",
    "depth    = alpha ^ phi\n",
    "width    = beta  ^ phi\n",
    "resolution = gamma ^ phi\n",
    "\n",
    "subject to: alpha * beta^2 * gamma^2 ~ 2\n",
    "```\n",
    "\n",
    "For EfficientNet: alpha=1.2, beta=1.1, gamma=1.15\n",
    "\n",
    "### Building Blocks\n",
    "\n",
    "EfficientNet uses **MBConv** (Mobile Inverted Bottleneck) blocks with:\n",
    "\n",
    "1. **Expansion** -- 1x1 conv to expand channels (expansion ratio, e.g., 6x)\n",
    "2. **Depthwise convolution** -- 3x3 or 5x5 depthwise separable conv\n",
    "3. **Squeeze-and-Excitation (SE)** -- channel attention mechanism\n",
    "4. **Projection** -- 1x1 conv to reduce channels back\n",
    "5. **Skip connection** -- if input and output shapes match\n",
    "\n",
    "```\n",
    "Input -> 1x1 Expand -> DWConv -> SE -> 1x1 Project -> (+) -> Output\n",
    "  |                                                     |\n",
    "  +---------------- skip connection --------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SqueezeExcitation(layers.Layer):\n",
    "    \"\"\"Squeeze-and-Excitation block: learns per-channel attention weights.\n",
    "    \n",
    "    1. Squeeze: Global Average Pooling reduces HxW to 1x1\n",
    "    2. Excitation: Two FC layers learn channel importance\n",
    "    3. Scale: Reweight original channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filters, ratio=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.gap    = layers.GlobalAveragePooling2D()\n",
    "        self.dense1 = layers.Dense(filters // ratio, activation='relu')\n",
    "        self.dense2 = layers.Dense(filters, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        # Squeeze\n",
    "        scale = self.gap(x)                      # (B, C)\n",
    "        # Excitation\n",
    "        scale = self.dense1(scale)                # (B, C//r)\n",
    "        scale = self.dense2(scale)                # (B, C)\n",
    "        # Scale\n",
    "        scale = tf.reshape(scale, [-1, 1, 1, self.filters])  # (B, 1, 1, C)\n",
    "        return x * scale\n",
    "\n",
    "\n",
    "class MBConvBlock(layers.Layer):\n",
    "    \"\"\"Mobile Inverted Bottleneck Convolution block (MBConv).\n",
    "    \n",
    "    Used in EfficientNet and MobileNetV2+.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio=6,\n",
    "                 kernel_size=3, stride=1, se_ratio=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.use_residual = (stride == 1 and in_channels == out_channels)\n",
    "        expanded = in_channels * expand_ratio\n",
    "\n",
    "        block_layers = []\n",
    "\n",
    "        # Expansion phase (skip if expand_ratio == 1)\n",
    "        if expand_ratio != 1:\n",
    "            block_layers += [\n",
    "                layers.Conv2D(expanded, 1, use_bias=False, padding='same'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.Activation('swish'),\n",
    "            ]\n",
    "\n",
    "        # Depthwise convolution\n",
    "        block_layers += [\n",
    "            layers.DepthwiseConv2D(kernel_size, strides=stride,\n",
    "                                   padding='same', use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('swish'),\n",
    "        ]\n",
    "\n",
    "        self.pre_se = keras.Sequential(block_layers)\n",
    "\n",
    "        # Squeeze-and-Excitation\n",
    "        self.se = SqueezeExcitation(expanded, ratio=se_ratio)\n",
    "\n",
    "        # Projection phase\n",
    "        self.project = keras.Sequential([\n",
    "            layers.Conv2D(out_channels, 1, use_bias=False, padding='same'),\n",
    "            layers.BatchNormalization(),\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        residual = x\n",
    "        x = self.pre_se(x, training=training)\n",
    "        x = self.se(x)\n",
    "        x = self.project(x, training=training)\n",
    "        if self.use_residual:\n",
    "            x = x + residual\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test the MBConv block\n",
    "inp = keras.Input(shape=(32, 32, 32))\n",
    "out = MBConvBlock(32, 32, expand_ratio=6, kernel_size=3, stride=1, name='mbconv_test')(inp)\n",
    "mbconv_model = Model(inp, out)\n",
    "print(f\"MBConv block -- Input: {inp.shape}, Output: {mbconv_model.output_shape}\")\n",
    "print(f\"Parameters: {mbconv_model.count_params():,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ConvNeXt -- Modernizing CNNs with Transformer Ideas\n",
    "\n",
    "### Core Insight\n",
    "\n",
    "Liu et al. (2022) asked: _can a pure CNN match Vision Transformer performance if we\n",
    "adopt the design choices that made transformers successful?_\n",
    "\n",
    "Starting from a standard ResNet, they applied a series of modernizations:\n",
    "\n",
    "| Modification | Inspiration | Effect |\n",
    "|:------------|:------------|:-------|\n",
    "| Patchify stem (4x4 stride-4 conv) | ViT patch embedding | Better initial downsampling |\n",
    "| Inverted bottleneck (expand 4x) | Transformer FFN | More computation in high-dim space |\n",
    "| Large kernel (7x7 depthwise) | ViT global attention | Larger receptive field |\n",
    "| LayerNorm instead of BatchNorm | Transformer convention | Better training stability |\n",
    "| GELU activation | Transformer convention | Smoother non-linearity |\n",
    "| Fewer activation functions | Transformer block design | Cleaner gradient flow |\n",
    "\n",
    "The result: **ConvNeXt matches or exceeds Swin Transformer** at all model sizes,\n",
    "while remaining a pure convolutional network.\n",
    "\n",
    "### ConvNeXt Block Structure\n",
    "\n",
    "```\n",
    "Input -> 7x7 DepthwiseConv -> LayerNorm -> 1x1 Conv (4x expand) -> GELU -> 1x1 Conv (project) -> (+) -> Output\n",
    "  |                                                                                                |\n",
    "  +------------------------------- skip connection ------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ConvNeXtBlock(layers.Layer):\n",
    "    \"\"\"A single ConvNeXt block.\n",
    "    \n",
    "    Combines depthwise convolution, inverted bottleneck,\n",
    "    LayerNorm, and GELU -- design principles borrowed from Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, drop_rate=0.0, layer_scale_init=1e-6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "\n",
    "        # Depthwise convolution with large 7x7 kernel\n",
    "        self.dwconv = layers.DepthwiseConv2D(7, padding='same')\n",
    "        self.norm   = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Inverted bottleneck: expand -> GELU -> project\n",
    "        self.pwconv1 = layers.Dense(4 * dim)       # Expand\n",
    "        self.act     = layers.Activation('gelu')\n",
    "        self.pwconv2 = layers.Dense(dim)            # Project\n",
    "\n",
    "        # Layer Scale (learnable per-channel scaling)\n",
    "        self.layer_scale_init = layer_scale_init\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(\n",
    "            name='layer_scale',\n",
    "            shape=(self.dim,),\n",
    "            initializer=tf.keras.initializers.Constant(self.layer_scale_init),\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        x = self.gamma * x  # Layer scale\n",
    "\n",
    "        if self.drop_rate > 0.0 and training:\n",
    "            x = tf.nn.dropout(x, rate=self.drop_rate)\n",
    "\n",
    "        return x + shortcut\n",
    "\n",
    "\n",
    "# Test ConvNeXt block\n",
    "inp = keras.Input(shape=(56, 56, 96))\n",
    "out = ConvNeXtBlock(96, name='convnext_block')(inp)\n",
    "demo = Model(inp, out)\n",
    "print(f\"ConvNeXt block -- Input: {inp.shape}, Output: {demo.output_shape}\")\n",
    "print(f\"Parameters: {demo.count_params():,}\")\n",
    "print(\"\\nNote: Compared to ResNet, ConvNeXt uses LayerNorm, GELU, and 7x7 depthwise conv.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Vision Transformer (ViT) -- Self-Attention for Images\n",
    "\n",
    "### Core Insight\n",
    "\n",
    "Dosovitskiy et al. (2020) showed that a **pure Transformer** (no convolutions at all)\n",
    "can achieve state-of-the-art image classification when pre-trained on large datasets.\n",
    "\n",
    "### How ViT Works\n",
    "\n",
    "1. **Patch Embedding**: Split the image into fixed-size patches (e.g., 16x16) and\n",
    "   linearly project each patch to an embedding vector.\n",
    "   - A 224x224 image with 16x16 patches yields 196 patch tokens.\n",
    "\n",
    "2. **[CLS] Token**: Prepend a learnable classification token.\n",
    "\n",
    "3. **Position Embedding**: Add learnable position embeddings (1D) to retain\n",
    "   spatial information.\n",
    "\n",
    "4. **Transformer Encoder**: Apply `L` standard Transformer blocks:\n",
    "   ```\n",
    "   LayerNorm -> Multi-Head Self-Attention -> Residual\n",
    "   LayerNorm -> MLP (GELU) -> Residual\n",
    "   ```\n",
    "\n",
    "5. **Classification Head**: The final [CLS] token representation is passed through\n",
    "   a linear classifier.\n",
    "\n",
    "### Key Differences from CNNs\n",
    "\n",
    "| Property | CNN | ViT |\n",
    "|:---------|:----|:----|\n",
    "| Inductive bias | Strong (locality, translation equiv.) | Weak (learns from data) |\n",
    "| Receptive field | Grows with depth | Global from layer 1 |\n",
    "| Data efficiency | Better with small data | Needs large pre-training data |\n",
    "| Compute scaling | O(H*W*C*K^2) | O((H*W/P^2)^2 * D) |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class PatchEmbedding(layers.Layer):\n",
    "    \"\"\"Convert image to a sequence of patch embeddings.\n",
    "    \n",
    "    Uses a Conv2D with kernel_size=stride=patch_size for efficient extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim  = embed_dim\n",
    "        self.projection = layers.Conv2D(\n",
    "            embed_dim, kernel_size=patch_size, strides=patch_size\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: (B, H, W, C)\n",
    "        x = self.projection(x)         # (B, H/P, W/P, embed_dim)\n",
    "        B = tf.shape(x)[0]\n",
    "        H, W, C = x.shape[1], x.shape[2], x.shape[3]\n",
    "        x = tf.reshape(x, [B, H * W, C])  # (B, num_patches, embed_dim)\n",
    "        return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"Standard Transformer encoder block: MHSA + MLP with pre-norm.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.attn  = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim // num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = keras.Sequential([\n",
    "            layers.Dense(mlp_dim, activation='gelu'),\n",
    "            layers.Dropout(dropout),\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Dropout(dropout),\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # Multi-Head Self-Attention with pre-norm and residual\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out = self.attn(x_norm, x_norm, training=training)\n",
    "        x = x + attn_out\n",
    "\n",
    "        # Feed-forward MLP with pre-norm and residual\n",
    "        x_norm = self.norm2(x)\n",
    "        mlp_out = self.mlp(x_norm, training=training)\n",
    "        x = x + mlp_out\n",
    "\n",
    "        return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class VisionTransformer(Model):\n",
    "    \"\"\"Complete Vision Transformer (ViT) implementation.\n",
    "    \n",
    "    Args:\n",
    "        image_size:  Input image dimension (assumes square).\n",
    "        patch_size:  Size of each image patch.\n",
    "        num_classes: Number of output classes.\n",
    "        embed_dim:   Embedding dimension.\n",
    "        depth:       Number of Transformer blocks.\n",
    "        num_heads:   Number of attention heads.\n",
    "        mlp_dim:     Hidden dim in the MLP.\n",
    "        dropout:     Dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=224, patch_size=16, num_classes=1000,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_dim=3072,\n",
    "                 dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(patch_size, embed_dim)\n",
    "\n",
    "        # Learnable [CLS] token\n",
    "        self.cls_token = self.add_weight(\n",
    "            name='cls_token',\n",
    "            shape=(1, 1, embed_dim),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        # Learnable position embeddings (for num_patches + 1 CLS token)\n",
    "        self.pos_embed = self.add_weight(\n",
    "            name='pos_embed',\n",
    "            shape=(1, num_patches + 1, embed_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.pos_drop = layers.Dropout(dropout)\n",
    "\n",
    "        # Transformer encoder blocks\n",
    "        self.blocks = [\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout,\n",
    "                             name=f'transformer_block_{i}')\n",
    "            for i in range(depth)\n",
    "        ]\n",
    "\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = layers.Dense(num_classes, name='head')\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        B = tf.shape(x)[0]\n",
    "\n",
    "        # Create patch embeddings\n",
    "        x = self.patch_embed(x)                        # (B, N, D)\n",
    "\n",
    "        # Prepend [CLS] token\n",
    "        cls_tokens = tf.broadcast_to(self.cls_token, [B, 1, x.shape[-1]])\n",
    "        x = tf.concat([cls_tokens, x], axis=1)         # (B, N+1, D)\n",
    "\n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x, training=training)\n",
    "\n",
    "        # Transformer encoder\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "\n",
    "        # Classification on [CLS] token\n",
    "        x = self.norm(x)\n",
    "        cls_output = x[:, 0]                            # (B, D)\n",
    "        return self.classifier(cls_output)\n",
    "\n",
    "\n",
    "# Build ViT-Tiny for demonstration (smaller than ViT-B for faster testing)\n",
    "vit_tiny = VisionTransformer(\n",
    "    image_size=224, patch_size=16, num_classes=1000,\n",
    "    embed_dim=192, depth=12, num_heads=3, mlp_dim=768,\n",
    "    dropout=0.1, name='ViT_Tiny'\n",
    ")\n",
    "\n",
    "# Build the model by passing a dummy input\n",
    "dummy_input = tf.random.normal((1, 224, 224, 3))\n",
    "dummy_output = vit_tiny(dummy_input)\n",
    "\n",
    "print(f\"ViT-Tiny\")\n",
    "print(f\"  Input shape     : (B, 224, 224, 3)\")\n",
    "print(f\"  Num patches     : {(224//16)**2}\")\n",
    "print(f\"  Embed dim       : 192\")\n",
    "print(f\"  Transformer depth: 12\")\n",
    "print(f\"  Output shape    : {dummy_output.shape}\")\n",
    "print(f\"  Total parameters: {vit_tiny.count_params():,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Swin Transformer -- Shifted Windows for Efficient Attention\n",
    "\n",
    "### Problem with Standard ViT\n",
    "\n",
    "Standard ViT computes **global** self-attention over all patches, which has\n",
    "**O(N^2)** complexity where N is the number of patches. For high-resolution images,\n",
    "this becomes prohibitively expensive.\n",
    "\n",
    "### Swin Transformer Solution (Liu et al., 2021)\n",
    "\n",
    "1. **Window-based attention**: Divide the feature map into non-overlapping\n",
    "   local windows (e.g., 7x7 patches) and compute attention **within** each window.\n",
    "   This reduces complexity to O(N) (linear in image size).\n",
    "\n",
    "2. **Shifted windows**: In alternating layers, shift the window partition by\n",
    "   half the window size. This creates cross-window connections without global attention.\n",
    "\n",
    "```\n",
    "Layer L:     Regular windows         Layer L+1:   Shifted windows\n",
    "+---+---+---+---+                    +--+----+--+\n",
    "|   |   |   |   |                    |  |    |  |\n",
    "+---+---+---+---+     shift by       +--+----+--+\n",
    "|   |   |   |   |  -> (M/2, M/2) -> |  |    |  |\n",
    "+---+---+---+---+                    +--+----+--+\n",
    "|   |   |   |   |                    |  |    |  |\n",
    "+---+---+---+---+                    +--+----+--+\n",
    "```\n",
    "\n",
    "3. **Hierarchical feature maps**: Unlike ViT (which maintains a single resolution),\n",
    "   Swin uses **patch merging** layers to progressively reduce spatial resolution\n",
    "   and increase channels -- similar to a CNN feature pyramid.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "Image -> Patch Partition (4x4) -> Stage 1: Swin Blocks\n",
    "      -> Patch Merging (2x down) -> Stage 2: Swin Blocks\n",
    "      -> Patch Merging (2x down) -> Stage 3: Swin Blocks\n",
    "      -> Patch Merging (2x down) -> Stage 4: Swin Blocks\n",
    "      -> Global Average Pool -> Classifier\n",
    "```\n",
    "\n",
    "### Comparison with Standard ViT\n",
    "\n",
    "| Property | ViT | Swin Transformer |\n",
    "|:---------|:----|:-----------------|\n",
    "| Attention scope | Global (all patches) | Local windows + shifted windows |\n",
    "| Complexity | O(N^2) | O(N) -- linear |\n",
    "| Feature hierarchy | Single-scale | Multi-scale (like FPN) |\n",
    "| Dense prediction | Difficult (needs adaptation) | Natural fit |\n",
    "| Pre-training data needed | Very large (JFT-300M) | ImageNet-1K sufficient |\n",
    "\n",
    "### Why Swin Matters\n",
    "\n",
    "- First Transformer backbone that works well for **both** classification and\n",
    "  dense tasks (detection, segmentation)\n",
    "- Linear complexity makes it practical for high-resolution inputs\n",
    "- Swin-T (29M params) achieves 81.3% Top-1 on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class WindowAttention(layers.Layer):\n",
    "    \"\"\"Window-based multi-head self-attention (W-MSA).\n",
    "    \n",
    "    Computes self-attention within local windows for efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = layers.Dense(dim * 3, use_bias=True)\n",
    "        self.proj = layers.Dense(dim)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        B_windows, N, C = tf.shape(x)[0], x.shape[1], x.shape[2]\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv(x)                                          # (B*nW, N, 3*C)\n",
    "        qkv = tf.reshape(qkv, [B_windows, N, 3, self.num_heads, C // self.num_heads])\n",
    "        qkv = tf.transpose(qkv, perm=[2, 0, 3, 1, 4])             # (3, B*nW, nH, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale      # (B*nW, nH, N, N)\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "\n",
    "        x = tf.matmul(attn, v)                                     # (B*nW, nH, N, head_dim)\n",
    "        x = tf.transpose(x, perm=[0, 2, 1, 3])                    # (B*nW, N, nH, head_dim)\n",
    "        x = tf.reshape(x, [B_windows, N, C])                      # (B*nW, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Demonstrate window attention\n",
    "# Simulating 4 windows, each 7x7 = 49 tokens, with embed_dim=96\n",
    "test_input = tf.random.normal((4, 49, 96))\n",
    "window_attn = WindowAttention(dim=96, window_size=7, num_heads=3)\n",
    "test_output = window_attn(test_input)\n",
    "print(f\"Window Attention\")\n",
    "print(f\"  Input  : {test_input.shape}  (4 windows, 49 tokens each, dim=96)\")\n",
    "print(f\"  Output : {test_output.shape}\")\n",
    "print(f\"  Each window attends only to its own 49 tokens -- O(49^2) per window, not O(196^2) globally.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Architecture Comparison and Selection Guide\n",
    "\n",
    "### Comprehensive Comparison\n",
    "\n",
    "| Architecture | Params | FLOPs (G) | Top-1 (%) | Inference (ms)* | Key Strength |\n",
    "|:------------|:------:|:---------:|:---------:|:---------------:|:-------------|\n",
    "| MobileNetV2 | 3.4 M | 0.3 | 72.0 | 5 | Mobile / edge deployment |\n",
    "| EfficientNet-B0 | 5.3 M | 0.4 | 77.1 | 8 | Best accuracy/param trade-off |\n",
    "| ResNet-50 | 25.6 M | 4.1 | 76.0 | 12 | Reliable baseline, well-studied |\n",
    "| DenseNet-121 | 8.0 M | 2.9 | 74.9 | 15 | Parameter efficiency |\n",
    "| ConvNeXt-T | 29 M | 4.5 | 82.1 | 14 | Modern CNN, strong all-around |\n",
    "| ViT-B/16 | 86 M | 17.6 | 77.9** | 18 | Scales well with data |\n",
    "| Swin-T | 29 M | 4.5 | 81.3 | 16 | Multi-scale, detection-friendly |\n",
    "| EfficientNet-B7 | 66 M | 37 | 84.3 | 45 | Maximum accuracy (single model) |\n",
    "\n",
    "\\* Approximate GPU inference time for 224x224 input (batch=1, V100).  \n",
    "\\** ViT-B achieves ~85%+ when pre-trained on larger datasets (JFT-300M).\n",
    "\n",
    "### Decision Matrix: When to Use Which Architecture\n",
    "\n",
    "| Scenario | Recommended | Rationale |\n",
    "|:---------|:------------|:----------|\n",
    "| **Mobile / Edge** (< 5M params) | MobileNetV2, EfficientNet-B0 | Low latency, small model size |\n",
    "| **General baseline** | ResNet-50 | Well-understood, enormous ecosystem |\n",
    "| **Maximum accuracy (medium budget)** | ConvNeXt-T/S or Swin-T/S | Best accuracy at ~30M params |\n",
    "| **Maximum accuracy (large budget)** | EfficientNet-B5-B7 or Swin-L | Push the accuracy frontier |\n",
    "| **Object detection / Segmentation** | Swin-T or ConvNeXt-T as backbone | Hierarchical features essential |\n",
    "| **Transfer learning (small dataset)** | ResNet-50 or EfficientNet-B0 | Strong inductive bias helps |\n",
    "| **Transfer learning (large dataset)** | ViT-B or Swin-B | Self-attention excels with data |\n",
    "| **Research / Custom tasks** | Start with ResNet-50, then ConvNeXt | Easy to modify, well-documented |\n",
    "\n",
    "### Cost-Benefit Analysis for Production\n",
    "\n",
    "```\n",
    "Accuracy\n",
    "  ^      *EfficientNet-B7\n",
    "  |                    *Swin-L\n",
    "  |            *ConvNeXt-T   *Swin-T\n",
    "  |     *EfficientNet-B0\n",
    "  |          *ResNet-50\n",
    "  |   *MobileNetV2\n",
    "  +--------------------------------------> Cost (FLOPs / Latency)\n",
    "```\n",
    "\n",
    "**Key takeaway:** The \"sweet spot\" for most production applications is\n",
    "**EfficientNet-B0 to B3** or **ConvNeXt-Tiny/Small**. Going beyond B3 / Small\n",
    "gives diminishing returns for the added compute cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Practical: Benchmark Pre-Trained Architectures\n",
    "\n",
    "We use `tf.keras.applications` to load pre-trained models and compare their\n",
    "inference time, memory usage, and output characteristics on the same input."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load pre-trained models (feature extractors)\n",
    "models_to_compare = {\n",
    "    'MobileNetV2': tf.keras.applications.MobileNetV2(\n",
    "        weights=None, include_top=False, input_shape=(224, 224, 3)\n",
    "    ),\n",
    "    'ResNet50': tf.keras.applications.ResNet50(\n",
    "        weights=None, include_top=False, input_shape=(224, 224, 3)\n",
    "    ),\n",
    "    'EfficientNetB0': tf.keras.applications.EfficientNetB0(\n",
    "        weights=None, include_top=False, input_shape=(224, 224, 3)\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"Models loaded (without pre-trained weights for environment compatibility).\")\n",
    "print(\"In production, set weights='imagenet' for pre-trained weights.\\n\")\n",
    "\n",
    "for name, model in models_to_compare.items():\n",
    "    print(f\"{name:20s} -- Params: {model.count_params():>10,}   \"\n",
    "          f\"Output shape: {model.output_shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Benchmark inference time\n",
    "dummy_batch = tf.random.normal((8, 224, 224, 3))\n",
    "\n",
    "results = {}\n",
    "for name, model in models_to_compare.items():\n",
    "    # Warm-up run\n",
    "    _ = model(dummy_batch, training=False)\n",
    "    \n",
    "    # Timed runs\n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time.time()\n",
    "        _ = model(dummy_batch, training=False)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_ms = np.mean(times) * 1000\n",
    "    std_ms = np.std(times) * 1000\n",
    "    results[name] = {'avg_ms': avg_ms, 'std_ms': std_ms,\n",
    "                     'params': model.count_params()}\n",
    "    print(f\"{name:20s} -- Avg: {avg_ms:7.1f} ms (+/- {std_ms:.1f} ms) for batch of 8\")\n",
    "\n",
    "print(\"\\nNote: Times depend heavily on hardware. GPU vs CPU makes a large difference.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Parameters vs Inference Time\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "names = list(results.keys())\n",
    "params = [results[n]['params'] / 1e6 for n in names]\n",
    "times  = [results[n]['avg_ms'] for n in names]\n",
    "\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "\n",
    "# Bar chart: parameter counts\n",
    "bars1 = axes[0].bar(names, params, color=colors, edgecolor='black', linewidth=0.5)\n",
    "axes[0].set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "axes[0].set_title('Model Size Comparison', fontsize=13, fontweight='bold')\n",
    "for bar, p in zip(bars1, params):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.3,\n",
    "                 f'{p:.1f}M', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Bar chart: inference time\n",
    "bars2 = axes[1].bar(names, times, color=colors, edgecolor='black', linewidth=0.5)\n",
    "axes[1].set_ylabel('Inference Time (ms, batch=8)', fontsize=12)\n",
    "axes[1].set_title('Inference Speed Comparison', fontsize=13, fontweight='bold')\n",
    "for bar, t in zip(bars2, times):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                 f'{t:.1f}ms', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- MobileNetV2 has the fewest parameters and fastest inference\")\n",
    "print(\"- ResNet50 is the largest but benefits from highly optimized implementations\")\n",
    "print(\"- EfficientNetB0 achieves higher accuracy with fewer params than ResNet50\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Exercises\n",
    "\n",
    "### Exercise 1: Design a Lightweight Architecture (< 1M Parameters)\n",
    "\n",
    "**Goal:** Build a custom CNN for CIFAR-10 that achieves > 90% test accuracy\n",
    "with fewer than 1 million parameters.\n",
    "\n",
    "**Hints:**\n",
    "- Use depthwise separable convolutions (MobileNet-style) to reduce params\n",
    "- Use Global Average Pooling instead of Flatten + Dense\n",
    "- Consider using SE blocks for better feature selection\n",
    "- Batch Normalization + data augmentation are essential"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exercise 1 - Starter code\n",
    "def build_lightweight_model(input_shape=(32, 32, 3), num_classes=10):\n",
    "    \"\"\"Design your lightweight architecture here.\n",
    "    \n",
    "    Target: < 1M parameters, > 90% accuracy on CIFAR-10.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # ------ YOUR ARCHITECTURE HERE ------\n",
    "    # Suggestion: Start with a small conv stem, then stack depthwise separable\n",
    "    # blocks with increasing channels and occasional stride-2 downsampling.\n",
    "\n",
    "    # Example stem\n",
    "    x = layers.Conv2D(32, 3, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    # Depthwise separable block (repeat and modify)\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.SeparableConv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "    # Head\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs, name='LightweightCNN')\n",
    "\n",
    "\n",
    "light_model = build_lightweight_model()\n",
    "param_count = light_model.count_params()\n",
    "print(f\"Parameters: {param_count:,}\")\n",
    "print(f\"Under 1M?  {'YES' if param_count < 1_000_000 else 'NO -- reduce model size!'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Hybrid CNN-Transformer\n",
    "\n",
    "**Goal:** Build a model that uses a CNN backbone for local feature extraction\n",
    "followed by a Transformer encoder for global reasoning.\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Image -> CNN Stem (downsample to 7x7 feature map)\n",
    "      -> Reshape to sequence of 49 tokens\n",
    "      -> Add position embeddings\n",
    "      -> Transformer Encoder (2-4 blocks)\n",
    "      -> Global Average Pool over tokens\n",
    "      -> Classification Head\n",
    "```\n",
    "\n",
    "**Hints:**\n",
    "- Use a small ResNet or ConvNeXt as the CNN stem\n",
    "- The CNN handles local patterns; the Transformer handles global relationships\n",
    "- This is the approach used by models like CoAtNet and CvT"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exercise 2 - Starter code\n",
    "def build_hybrid_model(input_shape=(224, 224, 3), num_classes=1000,\n",
    "                       embed_dim=256, num_heads=8, num_blocks=4):\n",
    "    \"\"\"Build a hybrid CNN-Transformer model.\"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # CNN Stem -- downsample aggressively\n",
    "    x = layers.Conv2D(64, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "\n",
    "    # A few residual-style blocks\n",
    "    for filters in [64, 128, embed_dim]:\n",
    "        x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2D(filters, 3, strides=2, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "    # Reshape to sequence: (B, H*W, C)\n",
    "    B = tf.shape(x)[0]\n",
    "    H, W, C = x.shape[1], x.shape[2], x.shape[3]\n",
    "    x = tf.reshape(x, [B, H * W, C])   # (B, num_tokens, embed_dim)\n",
    "\n",
    "    # Add learnable position embeddings\n",
    "    num_tokens = H * W\n",
    "    pos_emb = tf.Variable(\n",
    "        tf.random.normal((1, num_tokens, embed_dim), stddev=0.02),\n",
    "        trainable=True, name='hybrid_pos_embed'\n",
    "    )\n",
    "    x = x + pos_emb\n",
    "\n",
    "    # Transformer Encoder blocks\n",
    "    for i in range(num_blocks):\n",
    "        x = TransformerBlock(\n",
    "            embed_dim, num_heads, mlp_dim=embed_dim * 4,\n",
    "            dropout=0.1, name=f'hybrid_block_{i}'\n",
    "        )(x)\n",
    "\n",
    "    # Global average pool over tokens\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = tf.reduce_mean(x, axis=1)   # (B, embed_dim)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs, name='Hybrid_CNN_Transformer')\n",
    "\n",
    "\n",
    "hybrid = build_hybrid_model(input_shape=(224, 224, 3), num_classes=100,\n",
    "                            embed_dim=256, num_heads=8, num_blocks=4)\n",
    "dummy = tf.random.normal((1, 224, 224, 3))\n",
    "out = hybrid(dummy)\n",
    "print(f\"Hybrid CNN-Transformer\")\n",
    "print(f\"  Output shape: {out.shape}\")\n",
    "print(f\"  Parameters  : {hybrid.count_params():,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Architecture Ablation Study\n",
    "\n",
    "**Goal:** Systematically study the effect of individual design choices.\n",
    "\n",
    "Pick a baseline architecture (e.g., a simple ResNet-18) and measure the effect\n",
    "of each modification independently:\n",
    "\n",
    "| Experiment | Modification |\n",
    "|:-----------|:-------------|\n",
    "| Baseline | Plain ResNet-18 |\n",
    "| + SE blocks | Add Squeeze-Excitation to each residual block |\n",
    "| + Stochastic depth | Randomly drop entire residual blocks during training |\n",
    "| + Large kernel (7x7) | Replace 3x3 conv with 7x7 depthwise conv |\n",
    "| + GELU | Replace ReLU with GELU |\n",
    "| + LayerNorm | Replace BatchNorm with LayerNorm |\n",
    "| All combined | Apply all modifications (approximates ConvNeXt) |\n",
    "\n",
    "**Suggested dataset:** CIFAR-10 or CIFAR-100 (fast iteration).\n",
    "\n",
    "**What to measure:**\n",
    "- Final test accuracy\n",
    "- Training convergence speed (epochs to reach X% accuracy)\n",
    "- Parameter count and inference time\n",
    "- Training stability (variance across 3 seeds)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exercise 3 - Ablation study template\n",
    "\n",
    "# Define your experiments as a dictionary\n",
    "experiments = {\n",
    "    'baseline':       {'se': False, 'large_kernel': False, 'activation': 'relu',  'norm': 'batch'},\n",
    "    '+ SE blocks':    {'se': True,  'large_kernel': False, 'activation': 'relu',  'norm': 'batch'},\n",
    "    '+ Large kernel': {'se': False, 'large_kernel': True,  'activation': 'relu',  'norm': 'batch'},\n",
    "    '+ GELU':         {'se': False, 'large_kernel': False, 'activation': 'gelu',  'norm': 'batch'},\n",
    "    '+ LayerNorm':    {'se': False, 'large_kernel': False, 'activation': 'relu',  'norm': 'layer'},\n",
    "    'All combined':   {'se': True,  'large_kernel': True,  'activation': 'gelu',  'norm': 'layer'},\n",
    "}\n",
    "\n",
    "print(\"Ablation Study Experiments:\")\n",
    "print(\"=\" * 80)\n",
    "for name, config in experiments.items():\n",
    "    print(f\"  {name:20s} -> {config}\")\n",
    "\n",
    "print(\"\\nTo run the study:\")\n",
    "print(\"  1. Implement a configurable build_model(config) function\")\n",
    "print(\"  2. Train each variant for N epochs on CIFAR-10/100\")\n",
    "print(\"  3. Log accuracy, loss, and timing metrics\")\n",
    "print(\"  4. Plot comparative learning curves\")\n",
    "print(\"  5. Repeat with 3 different seeds for statistical significance\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "1. **VGG** -- Proved that uniform 3x3 convolutions scale to deep networks, but\n",
    "   hit the parameter wall with fully-connected layers.\n",
    "\n",
    "2. **ResNet** -- Solved the degradation problem with skip connections, enabling\n",
    "   training of 100+ layer networks and becoming the most cited architecture.\n",
    "\n",
    "3. **Inception / GoogLeNet** -- Introduced multi-scale parallel processing and\n",
    "   1x1 convolution bottlenecks, achieving great accuracy with far fewer parameters.\n",
    "\n",
    "4. **DenseNet** -- Maximized feature reuse through dense connections, achieving\n",
    "   strong performance with compact models.\n",
    "\n",
    "5. **EfficientNet** -- Showed that principled compound scaling of width, depth,\n",
    "   and resolution is better than ad-hoc scaling.\n",
    "\n",
    "6. **ConvNeXt** -- Demonstrated that pure CNNs can match Transformers when\n",
    "   modernized with Transformer design principles.\n",
    "\n",
    "7. **Vision Transformer (ViT)** -- Proved that pure self-attention works for\n",
    "   vision, especially at scale.\n",
    "\n",
    "8. **Swin Transformer** -- Made Transformers practical for vision with linear\n",
    "   complexity and hierarchical features.\n",
    "\n",
    "### Design Principles That Stood the Test of Time\n",
    "\n",
    "| Principle | First Appeared | Still Used |\n",
    "|:----------|:--------------|:-----------|\n",
    "| Skip / residual connections | ResNet (2015) | Everywhere |\n",
    "| Batch Normalization | Inception v2 (2015) | Most CNNs |\n",
    "| 1x1 convolution bottlenecks | GoogLeNet (2014) | All modern CNNs |\n",
    "| Global Average Pooling | NIN (2013) | Replaced FC layers |\n",
    "| Depthwise separable convolutions | MobileNet (2017) | Efficient models |\n",
    "| Squeeze-and-Excitation | SENet (2017) | EfficientNet, ConvNeXt |\n",
    "| Multi-head self-attention | Transformer (2017) | ViT, Swin, hybrids |\n",
    "| Layer Scale | CaiT (2021) | ConvNeXt, modern ViTs |\n",
    "\n",
    "### Recommended Reading\n",
    "\n",
    "- Simonyan & Zisserman, \"Very Deep Convolutional Networks\" (VGG), 2014\n",
    "- He et al., \"Deep Residual Learning for Image Recognition\" (ResNet), 2015\n",
    "- Szegedy et al., \"Going Deeper with Convolutions\" (GoogLeNet), 2014\n",
    "- Huang et al., \"Densely Connected Convolutional Networks\" (DenseNet), 2017\n",
    "- Tan & Le, \"EfficientNet: Rethinking Model Scaling\" 2019\n",
    "- Liu et al., \"A ConvNet for the 2020s\" (ConvNeXt), 2022\n",
    "- Dosovitskiy et al., \"An Image is Worth 16x16 Words\" (ViT), 2020\n",
    "- Liu et al., \"Swin Transformer\" 2021\n",
    "\n",
    "---\n",
    "\n",
    "*End of notebook.*"
   ]
  }
 ]
}