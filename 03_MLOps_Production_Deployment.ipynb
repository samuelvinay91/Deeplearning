{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps & Production Deployment for Deep Learning\n## From Notebook to Production: The Complete Guide\n\n**Skill Level:** Advanced | Architect\n\nThis notebook covers everything needed to take a deep learning model from experimental notebook to production deployment, including experiment tracking, model optimization, serving infrastructure, monitoring, and CI/CD pipelines.\n\n### Table of Contents\n1. [Introduction to MLOps](#1)\n2. [Experiment Configuration & Reproducibility](#2)\n3. [Production Data Pipelines](#3)\n4. [Model Versioning & Registry](#4)\n5. [Model Optimization for Deployment](#5)\n6. [Serving Architecture](#6)\n7. [Monitoring & Drift Detection](#7)\n8. [Automated Model Testing](#8)\n9. [CI/CD for ML](#9)\n10. [Distributed Training & Scaling](#10)\n11. [End-to-End Production Pipeline](#11)\n12. [Architect-Level Exercises](#12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n## 1. Introduction to MLOps\n\n### What is MLOps?\n\nMLOps (Machine Learning Operations) is the practice of applying DevOps principles to ML systems. It bridges the gap between ML development and production operations.\n\n### The ML Lifecycle\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Data    \u2502\u2500\u2500\u2500\u2500\u2192\u2502  Train   \u2502\u2500\u2500\u2500\u2500\u2192\u2502 Evaluate \u2502\u2500\u2500\u2500\u2500\u2192\u2502 Deploy \u2502\u2500\u2500\u2500\u2500\u2192\u2502 Monitor \u2502\n\u2502 Pipeline \u2502     \u2502  Model   \u2502     \u2502 & Test   \u2502     \u2502& Serve \u2502     \u2502& Alert  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2191                                                                \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Retrain Trigger \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### MLOps Maturity Levels (Google)\n\n| Level | Description | Characteristics |\n|-------|-------------|-----------------|\n| **0** | Manual | No automation, notebook-driven, no monitoring |\n| **1** | ML Pipeline | Automated training, basic CI/CD, manual deployment |\n| **2** | CI/CD + CT | Automated training & deployment, continuous monitoring, auto-retraining |\n\n### Key Principles\n- **Reproducibility**: Same data + code + config = same model\n- **Automation**: Minimize manual steps in the ML lifecycle\n- **Monitoring**: Track model performance in production continuously\n- **Versioning**: Version data, code, models, and configs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport json, os, time, tempfile, shutil\nfrom dataclasses import dataclass, asdict, field\nfrom typing import List, Dict, Optional\nfrom datetime import datetime\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n\n# Set seeds for reproducibility\ndef set_seeds(seed=42):\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n## 2. Experiment Configuration & Reproducibility\n\n### Why Configuration Management Matters\n- Reproduce any experiment exactly\n- Compare experiments systematically\n- Track what changed between versions\n- Enable automated hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\nclass ExperimentConfig:\n    \"\"\"Complete experiment configuration for reproducibility.\"\"\"\n\n    # Model\n    model_name: str = \"efficientnet_b0\"\n    num_classes: int = 2\n    image_size: int = 224\n    dropout_rate: float = 0.2\n\n    # Training\n    learning_rate: float = 1e-3\n    batch_size: int = 32\n    epochs: int = 50\n    optimizer: str = \"adamw\"\n    weight_decay: float = 1e-4\n\n    # Data\n    augmentation: bool = True\n    validation_split: float = 0.2\n    shuffle_buffer: int = 1000\n\n    # Infrastructure\n    seed: int = 42\n    mixed_precision: bool = False\n    xla_compile: bool = False\n\n    # Metadata\n    experiment_name: str = \"default\"\n    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n\n    def to_dict(self):\n        return asdict(self)\n\n    def save(self, path):\n        with open(path, 'w') as f:\n            json.dump(self.to_dict(), f, indent=2)\n        print(f\"Config saved to {path}\")\n\n    @classmethod\n    def load(cls, path):\n        with open(path) as f:\n            data = json.load(f)\n        return cls(**{k: v for k, v in data.items() if k in cls.__dataclass_fields__})\n\n    def diff(self, other):\n        \"\"\"Show differences between two configs.\"\"\"\n        diffs = {}\n        for key in self.__dataclass_fields__:\n            v1, v2 = getattr(self, key), getattr(other, key)\n            if v1 != v2:\n                diffs[key] = {'current': v1, 'other': v2}\n        return diffs\n\n# Demo\nconfig_v1 = ExperimentConfig(experiment_name=\"baseline\")\nconfig_v2 = ExperimentConfig(\n    experiment_name=\"improved\",\n    learning_rate=3e-4,\n    batch_size=64,\n    augmentation=True,\n    dropout_rate=0.3,\n    optimizer=\"adamw\"\n)\n\nprint(\"Config V1:\", json.dumps(config_v1.to_dict(), indent=2)[:500])\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\nDifferences (V1 vs V2):\")\nfor k, v in config_v1.diff(config_v2).items():\n    print(f\"  {k}: {v['current']} \u2192 {v['other']}\")\n\n# Save & reload test\ntmp_path = '/tmp/test_config.json'\nconfig_v1.save(tmp_path)\nconfig_loaded = ExperimentConfig.load(tmp_path)\nprint(f\"\\nConfig reload test: {'PASS' if config_loaded.to_dict() == config_v1.to_dict() else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n## 3. Production Data Pipelines\n\n### Design Principles\n1. **Deterministic**: Same seed \u2192 same data order (for reproducibility)\n2. **Efficient**: Prefetch, cache, parallelize\n3. **Robust**: Handle corrupted images, missing data\n4. **Monitored**: Track data statistics for drift detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionDataPipeline:\n    \"\"\"Production-grade tf.data pipeline with monitoring.\"\"\"\n\n    def __init__(self, config: ExperimentConfig):\n        self.config = config\n        self.data_stats = {}\n        self.AUTOTUNE = tf.data.AUTOTUNE\n\n    def _parse_image(self, file_path, label):\n        \"\"\"Robust image parsing with error handling.\"\"\"\n        img = tf.io.read_file(file_path)\n        img = tf.image.decode_jpeg(img, channels=3)\n        img = tf.image.resize(img, [self.config.image_size, self.config.image_size])\n        img = tf.cast(img, tf.float32) / 255.0\n        return img, label\n\n    def _augment(self, image, label):\n        \"\"\"Data augmentation pipeline.\"\"\"\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_brightness(image, 0.2)\n        image = tf.image.random_contrast(image, 0.8, 1.2)\n        image = tf.image.random_saturation(image, 0.8, 1.2)\n        image = tf.clip_by_value(image, 0.0, 1.0)\n        return image, label\n\n    def build(self, images, labels, is_training=True):\n        \"\"\"Build optimized tf.data pipeline.\"\"\"\n        dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n\n        if is_training:\n            dataset = dataset.shuffle(\n                buffer_size=self.config.shuffle_buffer,\n                seed=self.config.seed,\n                reshuffle_each_iteration=True\n            )\n\n        dataset = dataset.batch(self.config.batch_size)\n\n        if is_training and self.config.augmentation:\n            dataset = dataset.map(\n                lambda x, y: (tf.map_fn(lambda img: self._augment(img, tf.constant(0))[0], x), y),\n                num_parallel_calls=self.AUTOTUNE\n            )\n\n        dataset = dataset.prefetch(self.AUTOTUNE)\n        return dataset\n\n    def compute_stats(self, dataset, name=\"dataset\"):\n        \"\"\"Compute and store dataset statistics for drift detection.\"\"\"\n        all_means, all_stds = [], []\n        for batch, _ in dataset.take(10):\n            all_means.append(tf.reduce_mean(batch, axis=[0, 1, 2]).numpy())\n            all_stds.append(tf.math.reduce_std(batch, axis=[0, 1, 2]).numpy())\n\n        stats = {\n            'mean': np.mean(all_means, axis=0).tolist(),\n            'std': np.mean(all_stds, axis=0).tolist(),\n            'num_batches_sampled': len(all_means)\n        }\n        self.data_stats[name] = stats\n        return stats\n\n# Demo with synthetic data\nconfig = ExperimentConfig()\npipeline = ProductionDataPipeline(config)\n\n# Create dummy data\ndummy_images = np.random.rand(100, 224, 224, 3).astype(np.float32)\ndummy_labels = np.random.randint(0, 2, 100)\n\ntrain_ds = pipeline.build(dummy_images[:80], dummy_labels[:80], is_training=True)\nval_ds = pipeline.build(dummy_images[80:], dummy_labels[80:], is_training=False)\n\nstats = pipeline.compute_stats(train_ds, \"training\")\nprint(f\"Training data statistics:\")\nprint(f\"  Channel means: [{stats['mean'][0]:.4f}, {stats['mean'][1]:.4f}, {stats['mean'][2]:.4f}]\")\nprint(f\"  Channel stds:  [{stats['std'][0]:.4f}, {stats['std'][1]:.4f}, {stats['std'][2]:.4f}]\")\n\nfor batch_x, batch_y in train_ds.take(1):\n    print(f\"\\nBatch shape: {batch_x.shape}\")\n    print(f\"Label shape: {batch_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n## 4. Model Versioning & Registry\n\n### Why Version Models?\n- Track which model is in production\n- Roll back to previous versions if new model degrades\n- Compare model versions objectively\n- Audit trail for compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRegistry:\n    \"\"\"Simple model registry for versioning and managing models.\"\"\"\n\n    def __init__(self, base_dir='model_registry'):\n        self.base_dir = base_dir\n        os.makedirs(base_dir, exist_ok=True)\n\n    def save_model(self, model, config, metrics, version=None):\n        \"\"\"Save model with config and metrics.\"\"\"\n        if version is None:\n            existing = [d for d in os.listdir(self.base_dir) if d.startswith('v')]\n            version = f\"v{len(existing) + 1}\"\n\n        version_dir = os.path.join(self.base_dir, version)\n        os.makedirs(version_dir, exist_ok=True)\n\n        # Save model weights (faster than full SavedModel for registry)\n        model.save_weights(os.path.join(version_dir, 'model_weights.h5'))\n\n        # Save model architecture as JSON\n        model_json = model.to_json()\n        with open(os.path.join(version_dir, 'architecture.json'), 'w') as f:\n            f.write(model_json)\n\n        # Save config\n        config.save(os.path.join(version_dir, 'config.json'))\n\n        # Save metrics\n        with open(os.path.join(version_dir, 'metrics.json'), 'w') as f:\n            json.dump(metrics, f, indent=2)\n\n        # Save model summary\n        summary_lines = []\n        model.summary(print_fn=lambda x: summary_lines.append(x))\n        with open(os.path.join(version_dir, 'summary.txt'), 'w') as f:\n            f.write('\\n'.join(summary_lines))\n\n        print(f\"Model saved: {version_dir}\")\n        return version\n\n    def load_model(self, version):\n        \"\"\"Load a specific model version.\"\"\"\n        version_dir = os.path.join(self.base_dir, version)\n\n        with open(os.path.join(version_dir, 'architecture.json')) as f:\n            model = keras.models.model_from_json(f.read())\n        model.load_weights(os.path.join(version_dir, 'model_weights.h5'))\n\n        with open(os.path.join(version_dir, 'metrics.json')) as f:\n            metrics = json.load(f)\n\n        config = ExperimentConfig.load(os.path.join(version_dir, 'config.json'))\n        return model, config, metrics\n\n    def list_versions(self):\n        \"\"\"List all model versions with their metrics.\"\"\"\n        versions = []\n        for v in sorted(os.listdir(self.base_dir)):\n            metrics_path = os.path.join(self.base_dir, v, 'metrics.json')\n            if os.path.exists(metrics_path):\n                with open(metrics_path) as f:\n                    metrics = json.load(f)\n                versions.append({'version': v, **metrics})\n        return versions\n\n    def get_best_model(self, metric='val_accuracy', higher_is_better=True):\n        \"\"\"Get the best model version by a specific metric.\"\"\"\n        versions = self.list_versions()\n        if not versions:\n            return None\n        key_fn = lambda x: x.get(metric, 0)\n        best = max(versions, key=key_fn) if higher_is_better else min(versions, key=key_fn)\n        return best['version']\n\n# Demo\nregistry = ModelRegistry('/tmp/model_registry')\n\n# Create and save a simple model\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu', input_shape=(224*224*3,)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(2, activation='softmax')\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nconfig = ExperimentConfig(experiment_name=\"baseline_v1\", learning_rate=1e-3)\nmetrics_v1 = {'val_accuracy': 0.85, 'val_loss': 0.42, 'train_accuracy': 0.92}\nregistry.save_model(model, config, metrics_v1, version='v1')\n\nconfig_v2 = ExperimentConfig(experiment_name=\"improved_v2\", learning_rate=3e-4, dropout_rate=0.3)\nmetrics_v2 = {'val_accuracy': 0.91, 'val_loss': 0.28, 'train_accuracy': 0.94}\nregistry.save_model(model, config_v2, metrics_v2, version='v2')\n\n# List and compare\nprint(\"\\nAll model versions:\")\nfor v in registry.list_versions():\n    print(f\"  {v['version']}: val_acc={v['val_accuracy']:.2f}, val_loss={v['val_loss']:.2f}\")\n\nbest = registry.get_best_model('val_accuracy')\nprint(f\"\\nBest model: {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n## 5. Model Optimization for Deployment\n\n### Why Optimize?\n- **Reduce latency**: Faster inference for real-time applications\n- **Reduce model size**: Smaller models for mobile/edge deployment\n- **Reduce cost**: Less compute = lower cloud bills\n\n### Optimization Techniques\n\n| Technique | Size Reduction | Speed Improvement | Accuracy Impact |\n|-----------|---------------|-------------------|-----------------|\n| **Dynamic Quantization** | 2-4x | 2-3x | Minimal (<1%) |\n| **Float16 Quantization** | 2x | 1.5-2x | Minimal |\n| **Int8 Quantization** | 4x | 2-4x | Small (1-3%) |\n| **Pruning** | 2-10x | Variable | Small-Medium |\n| **Knowledge Distillation** | Variable | Variable | Small |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(model, optimization='none', representative_data=None):\n    \"\"\"Convert Keras model to TF Lite with various optimizations.\"\"\"\n\n    # Save model to temp dir first\n    tmp_dir = tempfile.mkdtemp()\n    model.save(os.path.join(tmp_dir, 'saved_model'))\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(tmp_dir, 'saved_model'))\n\n    if optimization == 'dynamic':\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    elif optimization == 'float16':\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        converter.target_spec.supported_types = [tf.float16]\n    elif optimization == 'int8' and representative_data is not None:\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        converter.representative_dataset = representative_data\n\n    tflite_model = converter.convert()\n\n    # Cleanup\n    shutil.rmtree(tmp_dir)\n\n    return tflite_model\n\n\ndef benchmark_model(model_or_tflite, test_input, num_runs=50, is_tflite=False):\n    \"\"\"Benchmark inference speed and model size.\"\"\"\n\n    if is_tflite:\n        interpreter = tf.lite.Interpreter(model_content=model_or_tflite)\n        interpreter.allocate_tensors()\n        input_details = interpreter.get_input_details()\n        output_details = interpreter.get_output_details()\n\n        # Warmup\n        sample = test_input[:1].astype(np.float32)\n        interpreter.resize_tensor_input(input_details[0]['index'], sample.shape)\n        interpreter.allocate_tensors()\n\n        times = []\n        for _ in range(num_runs):\n            interpreter.set_tensor(input_details[0]['index'], sample)\n            start = time.time()\n            interpreter.invoke()\n            times.append((time.time() - start) * 1000)\n\n        size_mb = len(model_or_tflite) / (1024 * 1024)\n    else:\n        # Keras model\n        sample = test_input[:1]\n        _ = model_or_tflite(sample)  # Warmup\n\n        times = []\n        for _ in range(num_runs):\n            start = time.time()\n            _ = model_or_tflite(sample, training=False)\n            times.append((time.time() - start) * 1000)\n\n        # Estimate size\n        size_mb = model_or_tflite.count_params() * 4 / (1024 * 1024)\n\n    return {\n        'mean_latency_ms': np.mean(times),\n        'p50_latency_ms': np.percentile(times, 50),\n        'p95_latency_ms': np.percentile(times, 95),\n        'p99_latency_ms': np.percentile(times, 99),\n        'model_size_mb': size_mb\n    }\n\n# Build a proper model for benchmarking\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(32, 32, 3)),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n], name='benchmark_model')\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n\ntest_input = np.random.rand(10, 32, 32, 3).astype(np.float32)\n\n# Benchmark original\nprint(\"Benchmarking original Keras model...\")\noriginal_results = benchmark_model(model, test_input, is_tflite=False)\n\n# Convert and benchmark TFLite variants\nresults = {'Original (float32)': original_results}\n\nfor opt_name in ['none', 'dynamic', 'float16']:\n    print(f\"Converting to TFLite ({opt_name})...\")\n    try:\n        tflite_model = convert_to_tflite(model, optimization=opt_name)\n        bench = benchmark_model(tflite_model, test_input.reshape(10, 32, 32, 3), is_tflite=True)\n        results[f'TFLite ({opt_name})'] = bench\n    except Exception as e:\n        print(f\"  Error: {e}\")\n\n# Display results\nprint(f\"\\n{'Model':<25} {'Size (MB)':<12} {'Mean (ms)':<12} {'P95 (ms)':<12} {'P99 (ms)':<12}\")\nprint(\"-\" * 75)\nfor name, r in results.items():\n    print(f\"{name:<25} {r['model_size_mb']:<12.2f} {r['mean_latency_ms']:<12.2f} {r['p95_latency_ms']:<12.2f} {r['p99_latency_ms']:<12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n## 6. Serving Architecture\n\n### Serving Options\n\n| Method | Latency | Throughput | Complexity | Best For |\n|--------|---------|-----------|------------|----------|\n| **TF Serving** | Low | High | Medium | Production REST/gRPC |\n| **TF Lite** | Very Low | Medium | Low | Mobile/Edge |\n| **ONNX Runtime** | Low | High | Medium | Cross-framework |\n| **Triton** | Low | Very High | High | Multi-model, multi-GPU |\n| **Flask/FastAPI** | Medium | Low | Low | Prototyping |\n\n### TF Serving Architecture\n```\nClient \u2192 Load Balancer \u2192 TF Serving Container(s) \u2192 Model Registry\n                              \u2193\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502  REST (8501)   \u2502\n                     \u2502  gRPC (8500)   \u2502\n                     \u2502                \u2502\n                     \u2502  Model Manager \u2502\n                     \u2502  \u251c\u2500\u2500 v1 (warm) \u2502\n                     \u2502  \u251c\u2500\u2500 v2 (warm) \u2502\n                     \u2502  \u2514\u2500\u2500 v3 (live) \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Serving deployment code examples\n\n# 1. Export model for TF Serving\ndef export_for_serving(model, export_path, version=1):\n    \"\"\"Export model in SavedModel format for TF Serving.\"\"\"\n    versioned_path = os.path.join(export_path, str(version))\n    model.save(versioned_path)\n    print(f\"Model exported to: {versioned_path}\")\n    return versioned_path\n\n# 2. Docker deployment command\ntf_serving_docker = '''\n# Pull TF Serving image\ndocker pull tensorflow/serving:latest\n\n# Run TF Serving\ndocker run -d --name tf_serving \\\n  -p 8501:8501 \\\n  -p 8500:8500 \\\n  --mount type=bind,source=/path/to/models/pet_classifier,target=/models/pet_classifier \\\n  -e MODEL_NAME=pet_classifier \\\n  tensorflow/serving:latest\n'''\n\n# 3. Client code for REST API\ndef predict_rest_api(images, model_name=\"pet_classifier\", server_url=\"http://localhost:8501\"):\n    \"\"\"Send prediction request to TF Serving REST API.\"\"\"\n    import requests\n\n    url = f\"{server_url}/v1/models/{model_name}:predict\"\n    payload = {\n        \"signature_name\": \"serving_default\",\n        \"instances\": images.tolist()\n    }\n\n    response = requests.post(url, json=payload, timeout=10)\n    response.raise_for_status()\n    return response.json()['predictions']\n\n# 4. Client code for gRPC API\ngrpc_client_code = '''\nimport grpc\nimport tensorflow as tf\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\n\nchannel = grpc.insecure_channel('localhost:8500')\nstub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n\nrequest = predict_pb2.PredictRequest()\nrequest.model_spec.name = 'pet_classifier'\nrequest.model_spec.signature_name = 'serving_default'\nrequest.inputs['input_1'].CopyFrom(\n    tf.make_tensor_proto(images, shape=images.shape)\n)\n\nresponse = stub.Predict(request, timeout=10.0)\npredictions = tf.make_ndarray(response.outputs['dense_1'])\n'''\n\n# 5. A/B Testing configuration\nab_config = {\n    \"model_config_list\": {\n        \"config\": [\n            {\"name\": \"pet_classifier\", \"base_path\": \"/models/pet_classifier\",\n             \"model_version_policy\": {\"specific\": {\"versions\": [2, 3]}}}\n        ]\n    }\n}\n\nprint(\"=== TF Serving Deployment Guide ===\")\nprint(\"\\n1. Docker command:\")\nprint(tf_serving_docker)\nprint(\"\\n2. A/B Testing Config (model.config):\")\nprint(json.dumps(ab_config, indent=2))\n\nprint(\"\\n3. Health check: curl http://localhost:8501/v1/models/pet_classifier\")\nprint(\"4. Predict:      curl -d '{\\\"instances\\\": [...]}' http://localhost:8501/v1/models/pet_classifier:predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n## 7. Monitoring & Drift Detection\n\n### What to Monitor\n\n| Metric | Type | Alert Threshold | Action |\n|--------|------|-----------------|--------|\n| Prediction latency | Performance | P95 > 100ms | Scale up / optimize |\n| Error rate | Reliability | > 1% | Investigate / rollback |\n| Prediction distribution | Data drift | KL divergence > 0.1 | Investigate data |\n| Feature statistics | Data drift | > 3\u03c3 from baseline | Retrain trigger |\n| Confidence score | Model health | Mean < 0.7 | Retrain / check data |\n\n### Types of Drift\n- **Data Drift**: Input distribution changes (e.g., new camera, different lighting)\n- **Concept Drift**: Relationship between features and labels changes\n- **Prediction Drift**: Output distribution shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMonitor:\n    \"\"\"Production model monitoring with drift detection.\"\"\"\n\n    def __init__(self, reference_stats=None):\n        self.predictions_log = []\n        self.reference_stats = reference_stats or {}\n        self.alerts = []\n\n    def log_prediction(self, prediction, confidence, latency_ms, metadata=None):\n        \"\"\"Log a single prediction event.\"\"\"\n        self.predictions_log.append({\n            'timestamp': time.time(),\n            'prediction': int(prediction),\n            'confidence': float(confidence),\n            'latency_ms': float(latency_ms),\n            'metadata': metadata or {}\n        })\n\n    def detect_data_drift(self, current_batch_stats):\n        \"\"\"Detect data drift using statistical comparison.\"\"\"\n        if not self.reference_stats:\n            return {'drift_detected': False, 'message': 'No reference stats set'}\n\n        ref_mean = np.array(self.reference_stats['mean'])\n        ref_std = np.array(self.reference_stats['std'])\n        cur_mean = np.array(current_batch_stats['mean'])\n\n        # Z-score based drift detection\n        z_scores = np.abs(cur_mean - ref_mean) / (ref_std + 1e-7)\n        drift_detected = bool(np.any(z_scores > 3.0))\n\n        result = {\n            'drift_detected': drift_detected,\n            'z_scores': z_scores.tolist(),\n            'max_z_score': float(np.max(z_scores)),\n            'drifted_channels': [i for i, z in enumerate(z_scores) if z > 3.0]\n        }\n\n        if drift_detected:\n            self.alerts.append({\n                'type': 'DATA_DRIFT',\n                'severity': 'HIGH' if np.max(z_scores) > 5.0 else 'MEDIUM',\n                'timestamp': time.time(),\n                'details': result\n            })\n\n        return result\n\n    def detect_prediction_drift(self, window_size=100):\n        \"\"\"Detect drift in prediction distribution.\"\"\"\n        if len(self.predictions_log) < window_size * 2:\n            return {'drift_detected': False, 'message': 'Not enough data'}\n\n        recent = [p['confidence'] for p in self.predictions_log[-window_size:]]\n        baseline = [p['confidence'] for p in self.predictions_log[-window_size*2:-window_size]]\n\n        recent_mean = np.mean(recent)\n        baseline_mean = np.mean(baseline)\n        baseline_std = np.std(baseline)\n\n        drift_score = abs(recent_mean - baseline_mean) / max(baseline_std, 1e-7)\n\n        recent_class_dist = np.bincount([p['prediction'] for p in self.predictions_log[-window_size:]], minlength=2) / window_size\n        baseline_class_dist = np.bincount([p['prediction'] for p in self.predictions_log[-window_size*2:-window_size]], minlength=2) / window_size\n\n        return {\n            'drift_detected': drift_score > 2.0,\n            'drift_score': float(drift_score),\n            'recent_confidence': float(recent_mean),\n            'baseline_confidence': float(baseline_mean),\n            'recent_class_distribution': recent_class_dist.tolist(),\n            'baseline_class_distribution': baseline_class_dist.tolist()\n        }\n\n    def get_performance_summary(self, last_n=None):\n        \"\"\"Get performance metrics summary.\"\"\"\n        logs = self.predictions_log[-last_n:] if last_n else self.predictions_log\n        if not logs:\n            return {}\n\n        latencies = [l['latency_ms'] for l in logs]\n        confidences = [l['confidence'] for l in logs]\n\n        return {\n            'total_predictions': len(logs),\n            'latency_mean_ms': float(np.mean(latencies)),\n            'latency_p50_ms': float(np.percentile(latencies, 50)),\n            'latency_p95_ms': float(np.percentile(latencies, 95)),\n            'latency_p99_ms': float(np.percentile(latencies, 99)),\n            'confidence_mean': float(np.mean(confidences)),\n            'confidence_min': float(np.min(confidences)),\n            'low_confidence_ratio': float(np.mean(np.array(confidences) < 0.7)),\n            'num_alerts': len(self.alerts)\n        }\n\n# Demo: Simulate production monitoring\nmonitor = ModelMonitor(reference_stats={'mean': [0.5, 0.5, 0.5], 'std': [0.2, 0.2, 0.2]})\n\n# Simulate normal predictions\nnp.random.seed(42)\nfor i in range(200):\n    pred = np.random.choice([0, 1])\n    conf = np.random.uniform(0.7, 0.99)\n    latency = np.random.exponential(20) + 10\n    monitor.log_prediction(pred, conf, latency)\n\n# Simulate drift (lower confidence in recent predictions)\nfor i in range(100):\n    pred = np.random.choice([0, 1], p=[0.3, 0.7])  # Shifted distribution\n    conf = np.random.uniform(0.5, 0.85)  # Lower confidence\n    latency = np.random.exponential(30) + 15  # Higher latency\n    monitor.log_prediction(pred, conf, latency)\n\n# Check for drift\npred_drift = monitor.detect_prediction_drift(window_size=100)\ndata_drift = monitor.detect_data_drift({'mean': [0.65, 0.48, 0.52]})\n\nprint(\"=== Performance Summary ===\")\nsummary = monitor.get_performance_summary()\nfor k, v in summary.items():\n    print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n\nprint(f\"\\n=== Prediction Drift ===\")\nprint(f\"  Drift detected: {pred_drift['drift_detected']}\")\nprint(f\"  Drift score: {pred_drift['drift_score']:.3f}\")\nprint(f\"  Recent confidence: {pred_drift['recent_confidence']:.3f}\")\nprint(f\"  Baseline confidence: {pred_drift['baseline_confidence']:.3f}\")\n\nprint(f\"\\n=== Data Drift ===\")\nprint(f\"  Drift detected: {data_drift['drift_detected']}\")\nprint(f\"  Max Z-score: {data_drift['max_z_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n## 8. Automated Model Testing\n\n### Test Categories\n\n| Category | What It Tests | When to Run |\n|----------|-------------|-------------|\n| **Unit Tests** | Individual components (data loading, preprocessing) | Every commit |\n| **Model Tests** | Accuracy, latency, size thresholds | Before promotion |\n| **Integration Tests** | End-to-end pipeline (data \u2192 prediction) | Before deployment |\n| **Regression Tests** | Performance vs. previous version | Before replacing model |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTestSuite:\n    \"\"\"Automated model testing for CI/CD pipelines.\"\"\"\n\n    def __init__(self, model, config):\n        self.model = model\n        self.config = config\n        self.results = []\n\n    def _record(self, test_name, passed, details=\"\"):\n        self.results.append({'test': test_name, 'passed': passed, 'details': details})\n        status = \"PASS\" if passed else \"FAIL\"\n        print(f\"  [{status}] {test_name}: {details}\")\n\n    def test_model_output_shape(self, input_shape):\n        \"\"\"Test that model produces expected output shape.\"\"\"\n        dummy = np.random.rand(1, *input_shape).astype(np.float32)\n        output = self.model(dummy, training=False)\n        expected = (1, self.config.num_classes)\n        passed = tuple(output.shape) == expected\n        self._record(\"Output Shape\", passed, f\"Expected {expected}, got {tuple(output.shape)}\")\n\n    def test_output_probabilities(self, input_shape):\n        \"\"\"Test that outputs are valid probabilities (sum to 1).\"\"\"\n        dummy = np.random.rand(2, *input_shape).astype(np.float32)\n        output = self.model(dummy, training=False).numpy()\n        sums = output.sum(axis=1)\n        passed = np.allclose(sums, 1.0, atol=1e-5)\n        self._record(\"Output Probabilities\", passed, f\"Sums: {sums}\")\n\n    def test_inference_latency(self, input_shape, max_latency_ms=100):\n        \"\"\"Test inference latency meets requirements.\"\"\"\n        dummy = np.random.rand(1, *input_shape).astype(np.float32)\n        _ = self.model(dummy, training=False)  # Warmup\n\n        times = []\n        for _ in range(30):\n            start = time.time()\n            _ = self.model(dummy, training=False)\n            times.append((time.time() - start) * 1000)\n\n        p95 = np.percentile(times, 95)\n        passed = p95 <= max_latency_ms\n        self._record(\"Inference Latency\", passed, f\"P95={p95:.1f}ms (max={max_latency_ms}ms)\")\n\n    def test_model_size(self, max_size_mb=500):\n        \"\"\"Test model size meets requirements.\"\"\"\n        size_mb = self.model.count_params() * 4 / (1024 * 1024)\n        passed = size_mb <= max_size_mb\n        self._record(\"Model Size\", passed, f\"{size_mb:.1f}MB (max={max_size_mb}MB)\")\n\n    def test_prediction_consistency(self, input_shape, num_runs=5):\n        \"\"\"Test that predictions are deterministic.\"\"\"\n        dummy = np.random.rand(1, *input_shape).astype(np.float32)\n        predictions = [self.model(dummy, training=False).numpy() for _ in range(num_runs)]\n        consistent = all(np.allclose(predictions[0], p, atol=1e-5) for p in predictions[1:])\n        self._record(\"Prediction Consistency\", consistent, f\"Checked {num_runs} runs\")\n\n    def test_handles_batch_sizes(self, input_shape, batch_sizes=[1, 4, 16, 32]):\n        \"\"\"Test model handles various batch sizes.\"\"\"\n        all_passed = True\n        for bs in batch_sizes:\n            try:\n                dummy = np.random.rand(bs, *input_shape).astype(np.float32)\n                output = self.model(dummy, training=False)\n                assert output.shape[0] == bs\n            except Exception as e:\n                all_passed = False\n                break\n        self._record(\"Batch Size Handling\", all_passed, f\"Tested batches: {batch_sizes}\")\n\n    def run_all(self, input_shape):\n        \"\"\"Run all tests.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"MODEL TEST SUITE\")\n        print(\"=\"*60)\n\n        self.test_model_output_shape(input_shape)\n        self.test_output_probabilities(input_shape)\n        self.test_inference_latency(input_shape)\n        self.test_model_size()\n        self.test_prediction_consistency(input_shape)\n        self.test_handles_batch_sizes(input_shape)\n\n        passed = sum(1 for r in self.results if r['passed'])\n        total = len(self.results)\n        print(f\"\\n{'='*60}\")\n        print(f\"Results: {passed}/{total} tests passed\")\n        print(f\"{'='*60}\")\n\n        return all(r['passed'] for r in self.results)\n\n# Run tests\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(32, 32, 3)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(2, activation='softmax')\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n\nconfig = ExperimentConfig(num_classes=2)\ntest_suite = ModelTestSuite(model, config)\nall_passed = test_suite.run_all(input_shape=(32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"9\"></a>\n## 9. CI/CD for ML\n\n### Pipeline Architecture\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Code    \u2502\u2500\u2500\u2500\u2192\u2502  Train &     \u2502\u2500\u2500\u2500\u2192\u2502  Test    \u2502\u2500\u2500\u2500\u2192\u2502  Stage    \u2502\u2500\u2500\u2500\u2192\u2502Production\u2502\n\u2502  Commit  \u2502    \u2502  Evaluate    \u2502    \u2502  Suite   \u2502    \u2502  Deploy   \u2502    \u2502  Deploy  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502                  \u2502                \u2502                \u2502\n                  TensorBoard        Pass/Fail      Canary Test     Full Traffic\n                  Experiment ID      Gate            A/B Testing     Monitoring\n```\n\n### GitHub Actions Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Actions workflow for ML CI/CD\ngithub_actions_yaml = '''\nname: ML Pipeline\n\non:\n  push:\n    branches: [main]\n    paths: ['src/**', 'configs/**', 'data/**']\n\njobs:\n  train-and-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: pip install -r requirements.txt\n\n      - name: Train model\n        run: python src/train.py --config configs/production.json\n        env:\n          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}\n\n      - name: Run model tests\n        run: python -m pytest tests/model_tests.py -v\n\n      - name: Run inference benchmarks\n        run: python src/benchmark.py --model outputs/model\n\n      - name: Upload model artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: trained-model\n          path: outputs/model/\n\n  deploy-staging:\n    needs: train-and-test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    steps:\n      - name: Download model\n        uses: actions/download-artifact@v4\n        with:\n          name: trained-model\n\n      - name: Deploy to staging\n        run: |\n          # Push to model registry\n          # Deploy canary (10% traffic)\n          echo \"Deploying to staging...\"\n\n  promote-production:\n    needs: deploy-staging\n    runs-on: ubuntu-latest\n    environment: production\n    steps:\n      - name: Promote to production\n        run: |\n          # Run smoke tests against staging\n          # Gradually shift traffic (10% -> 50% -> 100%)\n          echo \"Promoting to production...\"\n'''\n\n# Model promotion gates\npromotion_gates = {\n    \"staging\": {\n        \"accuracy_threshold\": 0.90,\n        \"latency_p95_ms\": 100,\n        \"model_size_mb\": 200,\n        \"tests_pass\": True,\n        \"no_data_drift\": True\n    },\n    \"production\": {\n        \"accuracy_threshold\": 0.92,\n        \"canary_error_rate\": 0.01,\n        \"canary_latency_p99_ms\": 200,\n        \"canary_duration_hours\": 24,\n        \"rollback_on_regression\": True\n    }\n}\n\nprint(\"=== GitHub Actions ML Pipeline ===\")\nprint(github_actions_yaml[:500] + \"\\n...\")\n\nprint(\"\\n=== Promotion Gates ===\")\nprint(json.dumps(promotion_gates, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"10\"></a>\n## 10. Distributed Training & Scaling\n\n### Distribution Strategies\n\n| Strategy | Use Case | Communication |\n|----------|----------|---------------|\n| **MirroredStrategy** | Single machine, multi-GPU | All-reduce |\n| **MultiWorkerMirrored** | Multiple machines | All-reduce |\n| **TPUStrategy** | Google TPU pods | Custom interconnect |\n| **ParameterServer** | Large-scale, heterogeneous | Param server |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed training setup\nprint(\"=== Distributed Training Strategies ===\\n\")\n\n# 1. MirroredStrategy (most common - single machine, multi-GPU)\nstrategy = tf.distribute.MirroredStrategy()\nprint(f\"MirroredStrategy: {strategy.num_replicas_in_sync} device(s)\")\n\n# 2. Using strategy to build and train model\ndef train_with_strategy(strategy, config):\n    \"\"\"Train model with distribution strategy.\"\"\"\n    with strategy.scope():\n        model = keras.Sequential([\n            keras.layers.Flatten(input_shape=(32, 32, 3)),\n            keras.layers.Dense(256, activation='relu'),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(config.dropout_rate),\n            keras.layers.Dense(128, activation='relu'),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dense(config.num_classes, activation='softmax')\n        ])\n\n        # Scale learning rate by number of replicas\n        scaled_lr = config.learning_rate * strategy.num_replicas_in_sync\n        optimizer = keras.optimizers.Adam(learning_rate=scaled_lr)\n\n        model.compile(\n            optimizer=optimizer,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n    return model\n\nconfig = ExperimentConfig(num_classes=10)\nmodel = train_with_strategy(strategy, config)\nprint(f\"\\nModel built with {strategy.num_replicas_in_sync} replicas\")\nprint(f\"Effective batch size: {config.batch_size} \u00d7 {strategy.num_replicas_in_sync} = {config.batch_size * strategy.num_replicas_in_sync}\")\nprint(f\"Scaled learning rate: {config.learning_rate} \u00d7 {strategy.num_replicas_in_sync} = {config.learning_rate * strategy.num_replicas_in_sync}\")\n\n# 3. Mixed precision training\nmixed_precision_code = '''\n# Enable mixed precision for faster training on modern GPUs\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\n\n# Your model will automatically use float16 for compute\n# and float32 for variable storage\nmodel = build_model(config)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-3),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Note: The output layer should still use float32\n# Add a cast layer if needed:\n# outputs = layers.Activation('softmax', dtype='float32')(x)\n'''\n\nprint(\"\\n=== Mixed Precision Training ===\")\nprint(mixed_precision_code)\n\n# 4. Cost optimization tips\nprint(\"\\n=== Cost Optimization Tips ===\")\ntips = [\n    \"1. Use spot/preemptible instances (60-90% savings)\",\n    \"2. Start with smaller models and scale up only if needed\",\n    \"3. Use mixed precision training (2x faster on modern GPUs)\",\n    \"4. Cache preprocessed data (avoid recomputing augmentations)\",\n    \"5. Use gradient accumulation instead of larger GPUs\",\n    \"6. Profile before optimizing (use TF Profiler)\",\n    \"7. Implement early stopping to avoid wasted training\",\n    \"8. Use transfer learning instead of training from scratch\",\n]\nfor tip in tips:\n    print(f\"  {tip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"11\"></a>\n## 11. End-to-End Production Pipeline\n\n### Complete System Architecture\n\n```\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                   ML Platform                           \u2502\n                    \u2502                                                         \u2502\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n  \u2502  Raw     \u2502\u2500\u2500\u2500\u2500\u2192\u2502  \u2502  Data    \u2502\u2500\u2500\u2192\u2502  Feature \u2502\u2500\u2500\u2192\u2502    Training       \u2502   \u2502\n  \u2502  Data    \u2502     \u2502  \u2502  Pipeline\u2502   \u2502  Store   \u2502   \u2502    Pipeline       \u2502   \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n                    \u2502                                          \u2502             \u2502\n                    \u2502                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n                    \u2502                                 \u2502  Model Registry  \u2502   \u2502\n                    \u2502                                 \u2502  \u251c\u2500\u2500 v1          \u2502   \u2502\n                    \u2502                                 \u2502  \u251c\u2500\u2500 v2          \u2502   \u2502\n                    \u2502                                 \u2502  \u2514\u2500\u2500 v3 (latest) \u2502   \u2502\n                    \u2502                                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n                    \u2502                                          \u2502             \u2502\n                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502  \u2502  API     \u2502\u2190\u2500\u2500\u2502  Model   \u2502\u2190\u2500\u2500\u2502  Automated       \u2502   \u2502\n  \u2502  Client  \u2502\u2190\u2500\u2500\u2500\u2192\u2502  \u2502  Gateway \u2502   \u2502  Server  \u2502   \u2502  Tests           \u2502   \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n                    \u2502        \u2502                                              \u2502\n                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n                    \u2502  \u2502            Monitoring & Alerting             \u2502    \u2502\n                    \u2502  \u2502  \u2022 Latency metrics  \u2022 Data drift detection   \u2502    \u2502\n                    \u2502  \u2502  \u2022 Error rates      \u2022 Prediction drift       \u2502    \u2502\n                    \u2502  \u2502  \u2022 Throughput        \u2022 Retrain triggers       \u2502    \u2502\n                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"12\"></a>\n## 12. Architect-Level Exercises\n\n### Exercise 1: Design for Scale\nDesign a system that handles **10,000 predictions per second** for image classification:\n- What serving infrastructure would you use?\n- How would you handle cold starts?\n- What's your caching strategy?\n- How do you handle model updates without downtime?\n\n### Exercise 2: Automated Retraining Pipeline\nCreate a complete retraining pipeline that:\n- Monitors for data drift continuously\n- Triggers retraining when drift exceeds threshold\n- Trains new model with latest data\n- Runs automated test suite\n- Deploys via canary (10% \u2192 50% \u2192 100%)\n- Rolls back automatically if metrics degrade\n\n### Exercise 3: Cost Optimization\nGiven a startup with \\$1000/month compute budget:\n- Design the most cost-effective training pipeline\n- Choose architecture that maximizes accuracy per dollar\n- Implement efficient serving for 100 req/sec\n- Plan for scaling to 10x traffic\n\n### Exercise 4: Multi-Model System\nDesign a system serving 5 different ML models:\n- Image classifier (200ms SLA)\n- Text sentiment (50ms SLA)\n- Recommendation engine (100ms SLA)\n- Fraud detection (10ms SLA)\n- Content moderation (500ms SLA)\n\nHow would you architect this? Consider:\n- Shared vs. dedicated infrastructure\n- Resource allocation and priority\n- Monitoring across models\n- Unified deployment pipeline\n\n---\n\n**Congratulations!** You've covered the complete MLOps lifecycle from experiment configuration to production deployment at scale.\n\n### Recommended Tools for Production\n| Category | Tools |\n|----------|-------|\n| **Experiment Tracking** | MLflow, Weights & Biases, TensorBoard |\n| **Model Registry** | MLflow, Vertex AI, SageMaker |\n| **Serving** | TF Serving, Triton, Seldon, BentoML |\n| **Monitoring** | Prometheus + Grafana, WhyLabs, Evidently |\n| **Orchestration** | Kubeflow, Airflow, Prefect |\n| **CI/CD** | GitHub Actions, GitLab CI, Jenkins |"
   ]
  }
 ]
}