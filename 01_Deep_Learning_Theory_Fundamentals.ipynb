{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Theory & Mathematical Foundations\n## From Neurons to Attention: A Complete Guide\n\n**Skill Levels Covered:** Beginner | Intermediate | Advanced | Architect\n\nThis notebook builds deep intuition for how neural networks work by implementing everything from scratch using NumPy before touching any framework. By the end, you'll understand not just *what* deep learning does, but *why* it works mathematically.\n\n### Table of Contents\n1. [The Neuron - From Biology to Math](#1)\n2. [Neural Network Architecture](#2)\n3. [Backpropagation - The Heart of Deep Learning](#3)\n4. [Loss Functions Deep Dive](#4)\n5. [Optimization Algorithms](#5)\n6. [Regularization Techniques](#6)\n7. [Convolution Operations Explained](#7)\n8. [Feature Maps & Receptive Fields](#8)\n9. [Weight Initialization Strategies](#9)\n10. [The Mathematics of Attention](#10)\n11. [Practical Exercises](#11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n## 1. The Neuron - From Biology to Math\n\n### 1.1 Biological Inspiration\n\nA biological neuron:\n- Receives signals through **dendrites** (inputs)\n- Processes them in the **cell body** (weighted sum + activation)\n- Fires a signal through the **axon** (output) if threshold is exceeded\n\n### 1.2 The Mathematical Model (Perceptron)\n\n$$y = f\\left(\\sum_{i=1}^{n} w_i \\cdot x_i + b\\right) = f(\\mathbf{w}^T \\mathbf{x} + b)$$\n\nWhere:\n- $\\mathbf{x} = [x_1, x_2, ..., x_n]$ \u2014 input features\n- $\\mathbf{w} = [w_1, w_2, ..., w_n]$ \u2014 learnable weights\n- $b$ \u2014 bias term\n- $f$ \u2014 activation function (non-linearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport matplotlib.pyplot as plt\n\n# A single neuron implementation\nclass Neuron:\n    def __init__(self, n_inputs):\n        self.weights = np.random.randn(n_inputs) * 0.01\n        self.bias = 0.0\n\n    def forward(self, x, activation='relu'):\n        z = np.dot(x, self.weights) + self.bias\n        if activation == 'relu':\n            return np.maximum(0, z)\n        elif activation == 'sigmoid':\n            return 1 / (1 + np.exp(-z))\n        return z\n\n# Demo: single neuron\nneuron = Neuron(3)\nx = np.array([1.0, 2.0, 3.0])\nprint(f\"Input: {x}\")\nprint(f\"Weights: {neuron.weights}\")\nprint(f\"Bias: {neuron.bias}\")\nprint(f\"Output (ReLU): {neuron.forward(x, 'relu'):.4f}\")\nprint(f\"Output (Sigmoid): {neuron.forward(x, 'sigmoid'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Activation Functions\n\nActivation functions introduce **non-linearity**, enabling neural networks to learn complex patterns. Without them, a network of any depth would collapse to a single linear transformation.\n\n| Function | Formula | Range | Pros | Cons |\n|----------|---------|-------|------|------|\n| **Sigmoid** | $\\sigma(x) = \\frac{1}{1+e^{-x}}$ | (0, 1) | Smooth, probabilistic | Vanishing gradients, not zero-centered |\n| **Tanh** | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1) | Zero-centered | Vanishing gradients |\n| **ReLU** | $\\max(0, x)$ | [0, \u221e) | Fast, sparse activation | Dead neurons |\n| **Leaky ReLU** | $\\max(0.01x, x)$ | (-\u221e, \u221e) | No dead neurons | Small negative slope is arbitrary |\n| **GELU** | $x \\cdot \\Phi(x)$ | (-\u221e, \u221e) | Smooth ReLU, used in Transformers | Computationally expensive |\n| **Swish** | $x \\cdot \\sigma(x)$ | (-\u221e, \u221e) | Self-gated, smooth | Slightly expensive |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions visualization\ndef sigmoid(x): return 1 / (1 + np.exp(-x))\ndef relu(x): return np.maximum(0, x)\ndef tanh_fn(x): return np.tanh(x)\ndef leaky_relu(x): return np.where(x > 0, x, 0.01 * x)\ndef gelu(x): return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\ndef swish(x): return x * sigmoid(x)\n\n# Derivatives\ndef sigmoid_deriv(x): s = sigmoid(x); return s * (1 - s)\ndef relu_deriv(x): return np.where(x > 0, 1.0, 0.0)\ndef tanh_deriv(x): return 1 - np.tanh(x)**2\n\nx = np.linspace(-5, 5, 1000)\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 9))\nfunctions = [\n    (sigmoid, sigmoid_deriv, 'Sigmoid', 'blue'),\n    (relu, relu_deriv, 'ReLU', 'red'),\n    (tanh_fn, tanh_deriv, 'Tanh', 'green'),\n    (gelu, None, 'GELU', 'purple'),\n    (swish, None, 'Swish', 'orange'),\n    (leaky_relu, None, 'Leaky ReLU', 'brown')\n]\n\nfor ax, (fn, deriv, name, color) in zip(axes.flat, functions):\n    ax.plot(x, fn(x), linewidth=2.5, color=color, label=f'{name}(x)')\n    if deriv is not None:\n        ax.plot(x, deriv(x), linewidth=1.5, color=color, linestyle='--', alpha=0.6, label=f\"{name}'(x)\")\n    ax.set_title(name, fontsize=14, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n    ax.axhline(y=0, color='k', linewidth=0.5)\n    ax.axvline(x=0, color='k', linewidth=0.5)\n    ax.legend(fontsize=9)\n    ax.set_xlim(-5, 5)\n\nplt.tight_layout()\nplt.suptitle('Activation Functions & Their Derivatives', fontsize=16, fontweight='bold', y=1.02)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n## 2. Neural Network Architecture\n\n### 2.1 Layer Types\n\n```\nInput Layer    Hidden Layer(s)    Output Layer\n   [x\u2081] ----\\                 /---- [\u0177\u2081]\n   [x\u2082] -----> [h\u2081] [h\u2082] [h\u2083] --> [\u0177\u2082]\n   [x\u2083] ----/                 \\---- [\u0177\u2083]\n```\n\n### 2.2 Forward Propagation\n\nFor a network with $L$ layers:\n\n$$\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]} \\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$$\n$$\\mathbf{a}^{[l]} = f^{[l]}(\\mathbf{z}^{[l]})$$\n\nwhere $\\mathbf{a}^{[0]} = \\mathbf{x}$ (the input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n    \"\"\"A complete neural network built from scratch with NumPy.\"\"\"\n\n    def __init__(self, layer_sizes):\n        self.num_layers = len(layer_sizes)\n        self.params = {}\n        self.cache = {}\n\n        # He initialization for ReLU networks\n        for l in range(1, self.num_layers):\n            self.params[f'W{l}'] = np.random.randn(layer_sizes[l], layer_sizes[l-1]) * np.sqrt(2.0 / layer_sizes[l-1])\n            self.params[f'b{l}'] = np.zeros((layer_sizes[l], 1))\n            print(f\"Layer {l}: W{l} shape = {self.params[f'W{l}'].shape}, b{l} shape = {self.params[f'b{l}'].shape}\")\n\n    def relu(self, Z):\n        return np.maximum(0, Z)\n\n    def softmax(self, Z):\n        exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n        return exp_Z / exp_Z.sum(axis=0, keepdims=True)\n\n    def forward(self, X):\n        self.cache['A0'] = X\n        A = X\n\n        # Hidden layers use ReLU\n        for l in range(1, self.num_layers - 1):\n            Z = self.params[f'W{l}'] @ A + self.params[f'b{l}']\n            A = self.relu(Z)\n            self.cache[f'Z{l}'] = Z\n            self.cache[f'A{l}'] = A\n\n        # Output layer uses softmax\n        l = self.num_layers - 1\n        Z = self.params[f'W{l}'] @ A + self.params[f'b{l}']\n        A = self.softmax(Z)\n        self.cache[f'Z{l}'] = Z\n        self.cache[f'A{l}'] = A\n\n        return A\n\n    def compute_loss(self, Y_pred, Y_true):\n        m = Y_true.shape[1]\n        loss = -np.sum(Y_true * np.log(Y_pred + 1e-8)) / m\n        return loss\n\n# Demo\nnn = NeuralNetwork([784, 128, 64, 10])\nX_dummy = np.random.randn(784, 5)  # 5 samples, 784 features\noutput = nn.forward(X_dummy)\nprint(f\"\\nInput shape: {X_dummy.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Output sums to 1 (softmax check): {output.sum(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n## 3. Backpropagation - The Heart of Deep Learning\n\n### 3.1 The Chain Rule\n\nBackpropagation is simply the **chain rule of calculus** applied systematically through the network.\n\nFor a composition $f(g(h(x)))$:\n\n$$\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dh} \\cdot \\frac{dh}{dx}$$\n\n### 3.2 Computational Graph\n\n```\nForward Pass (left to right):\nx \u2500\u2500\u2192 [\u00d7W\u2081] \u2500\u2500\u2192 z\u2081 \u2500\u2500\u2192 [ReLU] \u2500\u2500\u2192 a\u2081 \u2500\u2500\u2192 [\u00d7W\u2082] \u2500\u2500\u2192 z\u2082 \u2500\u2500\u2192 [Softmax] \u2500\u2500\u2192 \u0177 \u2500\u2500\u2192 [Loss] \u2500\u2500\u2192 L\n\nBackward Pass (right to left):\ndL/dx \u2190\u2500\u2500 dL/dW\u2081 \u2190\u2500\u2500 dL/dz\u2081 \u2190\u2500\u2500 dL/da\u2081 \u2190\u2500\u2500 dL/dW\u2082 \u2190\u2500\u2500 dL/dz\u2082 \u2190\u2500\u2500 dL/d\u0177 \u2190\u2500\u2500 dL/dL = 1\n```\n\n### 3.3 Gradient Formulas\n\nFor the output layer:\n$$\\frac{\\partial L}{\\partial \\mathbf{z}^{[L]}} = \\hat{\\mathbf{y}} - \\mathbf{y}$$\n\nFor hidden layers:\n$$\\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} = (\\mathbf{W}^{[l+1]})^T \\frac{\\partial L}{\\partial \\mathbf{z}^{[l+1]}} \\odot f'^{[l]}(\\mathbf{z}^{[l]})$$\n\nParameter gradients:\n$$\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}} = \\frac{1}{m} \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} (\\mathbf{a}^{[l-1]})^T$$\n$$\\frac{\\partial L}{\\partial \\mathbf{b}^{[l]}} = \\frac{1}{m} \\sum \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableNeuralNetwork(NeuralNetwork):\n    \"\"\"Neural network with backpropagation and training.\"\"\"\n\n    def relu_derivative(self, Z):\n        return (Z > 0).astype(float)\n\n    def backward(self, Y_true):\n        m = Y_true.shape[1]\n        grads = {}\n        L = self.num_layers - 1\n\n        # Output layer gradient (softmax + cross-entropy shortcut)\n        dZ = self.cache[f'A{L}'] - Y_true\n\n        for l in range(L, 0, -1):\n            grads[f'dW{l}'] = (1/m) * dZ @ self.cache[f'A{l-1}'].T\n            grads[f'db{l}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n\n            if l > 1:\n                dA = self.params[f'W{l}'].T @ dZ\n                dZ = dA * self.relu_derivative(self.cache[f'Z{l-1}'])\n\n        return grads\n\n    def update_params(self, grads, learning_rate=0.01):\n        for l in range(1, self.num_layers):\n            self.params[f'W{l}'] -= learning_rate * grads[f'dW{l}']\n            self.params[f'b{l}'] -= learning_rate * grads[f'db{l}']\n\n    def train(self, X, Y, epochs=100, learning_rate=0.01, verbose=True):\n        losses = []\n        for epoch in range(epochs):\n            # Forward\n            Y_pred = self.forward(X)\n            loss = self.compute_loss(Y_pred, Y)\n            losses.append(loss)\n\n            # Backward\n            grads = self.backward(Y)\n            self.update_params(grads, learning_rate)\n\n            if verbose and epoch % 20 == 0:\n                acc = np.mean(np.argmax(Y_pred, axis=0) == np.argmax(Y, axis=0))\n                print(f\"Epoch {epoch:4d} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\")\n\n        return losses\n\n# Train on synthetic spiral data\nnp.random.seed(42)\nN = 100  # points per class\nD = 2    # dimensions\nK = 3    # classes\n\nX_spiral = np.zeros((N*K, D))\nY_spiral = np.zeros(N*K, dtype=int)\n\nfor j in range(K):\n    ix = range(N*j, N*(j+1))\n    r = np.linspace(0.0, 1, N)\n    t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2\n    X_spiral[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n    Y_spiral[ix] = j\n\n# One-hot encode\nY_onehot = np.eye(K)[Y_spiral].T\nX_T = X_spiral.T\n\n# Train\nnn = TrainableNeuralNetwork([2, 64, 32, 3])\nlosses = nn.train(X_T, Y_onehot, epochs=200, learning_rate=0.5)\n\n# Plot results\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss curve\naxes[0].plot(losses, linewidth=2, color='blue')\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Loss', fontsize=12)\naxes[0].set_title('Training Loss', fontsize=14, fontweight='bold')\naxes[0].grid(True, alpha=0.3)\n\n# Decision boundary\nh = 0.02\nx_min, x_max = X_spiral[:, 0].min() - 0.5, X_spiral[:, 0].max() + 0.5\ny_min, y_max = X_spiral[:, 1].min() - 0.5, X_spiral[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\ngrid = np.c_[xx.ravel(), yy.ravel()].T\nprobs = nn.forward(grid)\nZ = np.argmax(probs, axis=0).reshape(xx.shape)\n\naxes[1].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\naxes[1].scatter(X_spiral[:, 0], X_spiral[:, 1], c=Y_spiral, cmap='RdYlBu', edgecolors='black', s=30)\naxes[1].set_title('Decision Boundary (Spiral Dataset)', fontsize=14, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Vanishing & Exploding Gradients\n\n**Vanishing Gradients**: When using sigmoid/tanh, gradients shrink exponentially as they flow back through layers. Since $\\sigma'(x) \\leq 0.25$, after $n$ layers: $\\prod_{i=1}^{n} 0.25 \\to 0$\n\n**Exploding Gradients**: When weights are large, gradients grow exponentially. Solutions include gradient clipping.\n\n**Solutions:**\n- Use **ReLU** (derivative is 0 or 1, no shrinkage)\n- Use **skip connections** (ResNet)\n- Use **batch normalization**\n- Proper **weight initialization** (He, Xavier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n## 4. Loss Functions Deep Dive\n\n### 4.1 Mean Squared Error (MSE)\n$$L_{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n\nBest for: **Regression** problems\n\n### 4.2 Binary Cross-Entropy\n$$L_{BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n\nBest for: **Binary classification**\n\n### 4.3 Categorical Cross-Entropy\n$$L_{CCE} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$\n\nBest for: **Multi-class classification**\n\n### 4.4 Focal Loss\n$$L_{FL} = -\\alpha_t (1-p_t)^\\gamma \\log(p_t)$$\n\nBest for: **Imbalanced datasets** (reduces easy example contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions implementation and visualization\ndef mse(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\ndef binary_cross_entropy(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\ndef focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n    focal_weight = (1 - p_t) ** gamma\n    return -np.mean(alpha * focal_weight * np.log(p_t))\n\n# Visualize loss landscapes\ny_pred_range = np.linspace(0.01, 0.99, 200)\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# MSE for y_true=1\naxes[0].plot(y_pred_range, (1 - y_pred_range)**2, linewidth=2.5, color='blue', label='y_true=1')\naxes[0].plot(y_pred_range, y_pred_range**2, linewidth=2.5, color='red', label='y_true=0')\naxes[0].set_title('Mean Squared Error', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Predicted', fontsize=12)\naxes[0].set_ylabel('Loss', fontsize=12)\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\n\n# BCE for y_true=1\naxes[1].plot(y_pred_range, -np.log(y_pred_range), linewidth=2.5, color='blue', label='y_true=1')\naxes[1].plot(y_pred_range, -np.log(1 - y_pred_range), linewidth=2.5, color='red', label='y_true=0')\naxes[1].set_title('Binary Cross-Entropy', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Predicted', fontsize=12)\naxes[1].set_ylabel('Loss', fontsize=12)\naxes[1].set_ylim(0, 5)\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\n\n# Focal loss for different gamma\nfor gamma in [0, 0.5, 1, 2, 5]:\n    p_t = y_pred_range\n    fl = -((1 - p_t) ** gamma) * np.log(p_t)\n    axes[2].plot(y_pred_range, fl, linewidth=2, label=f'gamma={gamma}')\naxes[2].set_title('Focal Loss (y_true=1)', fontsize=14, fontweight='bold')\naxes[2].set_xlabel('Predicted', fontsize=12)\naxes[2].set_ylabel('Loss', fontsize=12)\naxes[2].legend(fontsize=10)\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n## 5. Optimization Algorithms\n\n### 5.1 Gradient Descent Variants\n\n**SGD (Stochastic Gradient Descent):**\n$$\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)$$\n\n**Momentum:**\n$$v_t = \\beta v_{t-1} + \\eta \\nabla L(\\theta_t)$$\n$$\\theta_{t+1} = \\theta_t - v_t$$\n\n**RMSProp:**\n$$s_t = \\beta s_{t-1} + (1-\\beta)(\\nabla L)^2$$\n$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{s_t + \\epsilon}} \\nabla L$$\n\n**Adam (Adaptive Moment Estimation):**\n$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla L$$\n$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla L)^2$$\n$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n\n**AdamW (Adam with decoupled Weight Decay):**\nSame as Adam, but adds $\\lambda \\theta_t$ directly to the update instead of through the gradient. This is the **modern default** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer implementations and visualization on 2D loss surface\nclass SGD:\n    def __init__(self, lr=0.01):\n        self.lr = lr\n    def step(self, params, grads):\n        return params - self.lr * grads\n\nclass Momentum:\n    def __init__(self, lr=0.01, beta=0.9):\n        self.lr, self.beta = lr, beta\n        self.v = 0\n    def step(self, params, grads):\n        self.v = self.beta * self.v + self.lr * grads\n        return params - self.v\n\nclass RMSProp:\n    def __init__(self, lr=0.01, beta=0.99, eps=1e-8):\n        self.lr, self.beta, self.eps = lr, beta, eps\n        self.s = 0\n    def step(self, params, grads):\n        self.s = self.beta * self.s + (1 - self.beta) * grads**2\n        return params - self.lr * grads / (np.sqrt(self.s) + self.eps)\n\nclass Adam:\n    def __init__(self, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n        self.lr, self.beta1, self.beta2, self.eps = lr, beta1, beta2, eps\n        self.m, self.v, self.t = 0, 0, 0\n    def step(self, params, grads):\n        self.t += 1\n        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n        self.v = self.beta2 * self.v + (1 - self.beta2) * grads**2\n        m_hat = self.m / (1 - self.beta1**self.t)\n        v_hat = self.v / (1 - self.beta2**self.t)\n        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n\n# Beale's function (challenging optimization surface)\ndef beale(x, y):\n    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n\ndef beale_grad(x, y):\n    dx = 2*(1.5 - x + x*y)*(-1 + y) + 2*(2.25 - x + x*y**2)*(-1 + y**2) + 2*(2.625 - x + x*y**3)*(-1 + y**3)\n    dy = 2*(1.5 - x + x*y)*x + 2*(2.25 - x + x*y**2)*(2*x*y) + 2*(2.625 - x + x*y**3)*(3*x*y**2)\n    return np.array([dx, dy])\n\n# Run optimizers\noptimizers = {\n    'SGD (lr=0.0001)': SGD(lr=0.0001),\n    'Momentum (lr=0.0001)': Momentum(lr=0.0001, beta=0.9),\n    'RMSProp (lr=0.005)': RMSProp(lr=0.005),\n    'Adam (lr=0.01)': Adam(lr=0.01)\n}\n\nstart = np.array([0.5, -1.5])\npaths = {}\n\nfor name, opt in optimizers.items():\n    pos = start.copy()\n    path = [pos.copy()]\n    for _ in range(300):\n        grad = beale_grad(pos[0], pos[1])\n        pos = opt.step(pos, grad)\n        path.append(pos.copy())\n    paths[name] = np.array(path)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 9))\nx_grid = np.linspace(-2, 4.5, 300)\ny_grid = np.linspace(-2, 2.5, 300)\nX, Y = np.meshgrid(x_grid, y_grid)\nZ = beale(X, Y)\n\nax.contour(X, Y, Z, levels=np.logspace(-1, 3.5, 30), cmap='viridis', alpha=0.6)\n\ncolors = ['red', 'blue', 'green', 'orange']\nfor (name, path), color in zip(paths.items(), colors):\n    ax.plot(path[:, 0], path[:, 1], '-o', color=color, markersize=2, linewidth=1.5, label=name)\n\nax.plot(3, 0.5, '*', color='gold', markersize=20, markeredgecolor='black', label='Global Minimum')\nax.set_title(\"Optimizer Trajectories on Beale's Function\", fontsize=14, fontweight='bold')\nax.set_xlabel('x', fontsize=12)\nax.set_ylabel('y', fontsize=12)\nax.legend(fontsize=10, loc='upper left')\nax.set_xlim(-2, 4.5)\nax.set_ylim(-2, 2.5)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Learning Rate Schedules\n\n| Schedule | Formula | Best For |\n|----------|---------|----------|\n| **Step Decay** | $\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t/s \\rfloor}$ | Simple training |\n| **Cosine Annealing** | $\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max}-\\eta_{min})(1+\\cos(\\frac{t}{T}\\pi))$ | Modern best practice |\n| **Warmup + Cosine** | Linear warmup then cosine decay | Transformers, large models |\n| **OneCycle** | Ramp up then ramp down | Fast convergence |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule visualization\nepochs = np.arange(100)\n\n# Step decay\nstep_lr = 0.1 * (0.5 ** (epochs // 30))\n\n# Cosine annealing\ncosine_lr = 0.001 + 0.5 * (0.1 - 0.001) * (1 + np.cos(np.pi * epochs / 100))\n\n# Warmup + cosine\nwarmup_epochs = 10\nwarmup_cosine = np.where(\n    epochs < warmup_epochs,\n    0.1 * epochs / warmup_epochs,\n    0.001 + 0.5 * (0.1 - 0.001) * (1 + np.cos(np.pi * (epochs - warmup_epochs) / (100 - warmup_epochs)))\n)\n\n# Exponential decay\nexp_lr = 0.1 * np.exp(-0.03 * epochs)\n\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(epochs, step_lr, linewidth=2, label='Step Decay')\nax.plot(epochs, cosine_lr, linewidth=2, label='Cosine Annealing')\nax.plot(epochs, warmup_cosine, linewidth=2, label='Warmup + Cosine')\nax.plot(epochs, exp_lr, linewidth=2, label='Exponential Decay')\nax.set_xlabel('Epoch', fontsize=12)\nax.set_ylabel('Learning Rate', fontsize=12)\nax.set_title('Learning Rate Schedules Comparison', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n## 6. Regularization Techniques\n\nRegularization prevents **overfitting** \u2014 when a model memorizes training data instead of learning generalizable patterns.\n\n### 6.1 L1 & L2 Regularization\n\n**L2 (Ridge / Weight Decay):**\n$$L_{total} = L_{data} + \\frac{\\lambda}{2m} \\sum ||\\mathbf{W}||^2_2$$\n\nEffect: Pushes all weights toward zero, but doesn't make them exactly zero.\n\n**L1 (Lasso):**\n$$L_{total} = L_{data} + \\frac{\\lambda}{m} \\sum ||\\mathbf{W}||_1$$\n\nEffect: Drives some weights to exactly zero \u2192 **sparse models**.\n\n### 6.2 Dropout\n\nDuring training, randomly set activations to zero with probability $p$:\n$$\\tilde{a}^{[l]} = \\frac{\\mathbf{m}^{[l]}}{1-p} \\odot a^{[l]}$$\n\nwhere $\\mathbf{m}^{[l]}$ is a binary mask. The $\\frac{1}{1-p}$ scaling (inverted dropout) keeps expected values consistent.\n\n### 6.3 Batch Normalization\n\nNormalize activations within a mini-batch:\n$$\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n$$y_i = \\gamma \\hat{x}_i + \\beta$$\n\nWhere $\\gamma$ (scale) and $\\beta$ (shift) are **learnable parameters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overfitting vs regularization\nnp.random.seed(42)\n\n# Generate noisy polynomial data\nx_data = np.linspace(-3, 3, 30)\ny_data = 0.5 * x_data**2 - x_data + np.random.randn(30) * 1.5\n\n# Fit with different polynomial degrees\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\nfor ax, degree, title in zip(axes, [2, 15, 15],\n    ['Good Fit (degree=2)', 'Overfitting (degree=15)', 'Regularized (degree=15, L2)']):\n\n    ax.scatter(x_data, y_data, color='blue', s=30, alpha=0.7, label='Data')\n\n    if 'Regularized' in title:\n        # Ridge regression (L2 regularized)\n        X_poly = np.vander(x_data, degree + 1)\n        lam = 10.0\n        coeffs = np.linalg.solve(X_poly.T @ X_poly + lam * np.eye(degree + 1), X_poly.T @ y_data)\n        x_plot = np.linspace(-3, 3, 200)\n        X_plot = np.vander(x_plot, degree + 1)\n        y_plot = X_plot @ coeffs\n    else:\n        coeffs = np.polyfit(x_data, y_data, degree)\n        x_plot = np.linspace(-3, 3, 200)\n        y_plot = np.polyval(coeffs, x_plot)\n\n    ax.plot(x_plot, y_plot, color='red', linewidth=2, label='Model')\n    ax.set_title(title, fontsize=13, fontweight='bold')\n    ax.set_ylim(-10, 20)\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n## 7. Convolution Operations Explained\n\n### 7.1 What is a Convolution?\n\nA convolution slides a **kernel** (filter) across an image, computing element-wise products and summing them at each position.\n\n**Output size formula:**\n$$O = \\frac{I - K + 2P}{S} + 1$$\n\nWhere: $I$ = input size, $K$ = kernel size, $P$ = padding, $S$ = stride\n\n### 7.2 Types of Convolutions\n\n| Type | Description | Use Case |\n|------|-------------|----------|\n| **Standard** | Full KxK convolution | General feature extraction |\n| **Depthwise** | Separate filter per channel | Efficient mobile models |\n| **Pointwise (1x1)** | 1x1 convolution | Channel mixing / dimension change |\n| **Depthwise Separable** | Depthwise + Pointwise | MobileNet, EfficientNet |\n| **Dilated** | Gaps between kernel elements | Large receptive field without pooling |\n| **Transposed** | \"Upsampling\" convolution | Decoder, segmentation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_manual(image, kernel, stride=1, padding=0):\n    \"\"\"Manual 2D convolution implementation from scratch.\"\"\"\n    if padding > 0:\n        image = np.pad(image, padding, mode='constant')\n\n    h, w = image.shape\n    kh, kw = kernel.shape\n    out_h = (h - kh) // stride + 1\n    out_w = (w - kw) // stride + 1\n    output = np.zeros((out_h, out_w))\n\n    for i in range(out_h):\n        for j in range(out_w):\n            region = image[i*stride:i*stride+kh, j*stride:j*stride+kw]\n            output[i, j] = np.sum(region * kernel)\n    return output\n\n# Create sample image (checkerboard)\nimage = np.zeros((8, 8))\nimage[::2, ::2] = 1\nimage[1::2, 1::2] = 1\n\n# Define common kernels\nkernels = {\n    'Edge Detection\\n(Horizontal)': np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]),\n    'Edge Detection\\n(Vertical)': np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]),\n    'Sharpen': np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),\n    'Gaussian Blur': np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16.0,\n    'Sobel X': np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),\n    'Identity': np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n}\n\nfig, axes = plt.subplots(2, len(kernels), figsize=(18, 7))\n\nfor idx, (name, kernel) in enumerate(kernels.items()):\n    # Show kernel\n    axes[0, idx].imshow(kernel, cmap='RdBu_r', vmin=-2, vmax=2)\n    axes[0, idx].set_title(f'Kernel: {name}', fontsize=9, fontweight='bold')\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            axes[0, idx].text(j, i, f'{kernel[i,j]:.1f}', ha='center', va='center', fontsize=8)\n    axes[0, idx].set_xticks([]); axes[0, idx].set_yticks([])\n\n    # Show result\n    result = conv2d_manual(image, kernel, padding=1)\n    axes[1, idx].imshow(result, cmap='gray')\n    axes[1, idx].set_title('Output', fontsize=9)\n    axes[1, idx].set_xticks([]); axes[1, idx].set_yticks([])\n\nplt.suptitle('Convolution Kernels & Their Effects', fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Input size: {image.shape}\")\nprint(f\"Kernel size: 3x3\")\nprint(f\"Output size (no padding, stride=1): {conv2d_manual(image, kernels['Identity']).shape}\")\nprint(f\"Output size (padding=1, stride=1): {conv2d_manual(image, kernels['Identity'], padding=1).shape}\")\nprint(f\"Output size (padding=0, stride=2): {conv2d_manual(image, kernels['Identity'], stride=2).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n## 8. Feature Maps & Receptive Fields\n\n### 8.1 Receptive Field\n\nThe **receptive field** is the region of the input image that affects a particular feature in a deeper layer.\n\n**Formula for receptive field size:**\n$$r_l = r_{l-1} + (k_l - 1) \\times \\prod_{i=1}^{l-1} s_i$$\n\nWhere $r_l$ = receptive field at layer $l$, $k_l$ = kernel size, $s_i$ = stride at layer $i$.\n\n### 8.2 Why Receptive Fields Matter\n\n| Layer Depth | Receptive Field | What It \"Sees\" |\n|-------------|----------------|----------------|\n| Early (1-2) | Small (3-7px) | Edges, textures |\n| Middle (3-5) | Medium (20-50px) | Parts, patterns |\n| Deep (6+) | Large (100px+) | Objects, scenes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_receptive_field(layers):\n    \"\"\"Calculate receptive field for a sequence of conv/pool layers.\n    Each layer: (kernel_size, stride, padding)\n    \"\"\"\n    r = 1  # receptive field\n    j = 1  # jump (product of strides)\n\n    print(f\"{'Layer':<20} {'Kernel':<8} {'Stride':<8} {'RF Size':<10} {'Jump':<8}\")\n    print(\"-\" * 60)\n    print(f\"{'Input':<20} {'-':<8} {'-':<8} {r:<10} {j:<8}\")\n\n    for name, (k, s, p) in layers:\n        r = r + (k - 1) * j\n        j = j * s\n        print(f\"{name:<20} {k:<8} {s:<8} {r:<10} {j:<8}\")\n\n    return r\n\n# VGG-16 receptive field\nprint(\"=== VGG-16 Receptive Field ===\")\nvgg_layers = [\n    ('Conv3x3_1', (3, 1, 1)), ('Conv3x3_2', (3, 1, 1)),\n    ('MaxPool', (2, 2, 0)),\n    ('Conv3x3_3', (3, 1, 1)), ('Conv3x3_4', (3, 1, 1)),\n    ('MaxPool', (2, 2, 0)),\n    ('Conv3x3_5', (3, 1, 1)), ('Conv3x3_6', (3, 1, 1)), ('Conv3x3_7', (3, 1, 1)),\n    ('MaxPool', (2, 2, 0)),\n    ('Conv3x3_8', (3, 1, 1)), ('Conv3x3_9', (3, 1, 1)), ('Conv3x3_10', (3, 1, 1)),\n    ('MaxPool', (2, 2, 0)),\n    ('Conv3x3_11', (3, 1, 1)), ('Conv3x3_12', (3, 1, 1)), ('Conv3x3_13', (3, 1, 1)),\n    ('MaxPool', (2, 2, 0)),\n]\nrf = calculate_receptive_field(vgg_layers)\nprint(f\"\\nFinal receptive field of VGG-16: {rf}x{rf} pixels\")\n\nprint(\"\\n=== ResNet-50 (first block) Receptive Field ===\")\nresnet_layers = [\n    ('Conv7x7/2', (7, 2, 3)),\n    ('MaxPool3x3/2', (3, 2, 1)),\n    ('Conv1x1', (1, 1, 0)), ('Conv3x3', (3, 1, 1)), ('Conv1x1', (1, 1, 0)),\n]\nrf = calculate_receptive_field(resnet_layers)\nprint(f\"\\nReceptive field after first ResNet block: {rf}x{rf} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"9\"></a>\n## 9. Weight Initialization Strategies\n\n### Why Initialization Matters\n\nBad initialization \u2192 vanishing or exploding activations \u2192 poor training.\n\n| Method | Formula | Best With |\n|--------|---------|-----------|\n| **Xavier/Glorot** | $W \\sim \\mathcal{N}(0, \\frac{2}{n_{in}+n_{out}})$ | Sigmoid, Tanh |\n| **He** | $W \\sim \\mathcal{N}(0, \\frac{2}{n_{in}})$ | ReLU family |\n| **LeCun** | $W \\sim \\mathcal{N}(0, \\frac{1}{n_{in}})$ | SELU |\n\n### The Key Principle\n\nKeep the **variance of activations** roughly constant across layers. If variance grows \u2192 exploding; if variance shrinks \u2192 vanishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Effect of initialization on activation distributions\nnp.random.seed(42)\n\nlayer_sizes = [784] + [256] * 10  # 10 hidden layers of size 256\n\ninit_methods = {\n    'Random (std=1.0)': lambda fan_in, fan_out: np.random.randn(fan_in, fan_out) * 1.0,\n    'Random (std=0.01)': lambda fan_in, fan_out: np.random.randn(fan_in, fan_out) * 0.01,\n    'Xavier': lambda fan_in, fan_out: np.random.randn(fan_in, fan_out) * np.sqrt(2.0 / (fan_in + fan_out)),\n    'He': lambda fan_in, fan_out: np.random.randn(fan_in, fan_out) * np.sqrt(2.0 / fan_in),\n}\n\nfig, axes = plt.subplots(len(init_methods), len(layer_sizes)-1, figsize=(20, 10))\n\nfor row, (name, init_fn) in enumerate(init_methods.items()):\n    x = np.random.randn(200, 784)  # batch of 200\n\n    for col in range(len(layer_sizes) - 1):\n        W = init_fn(layer_sizes[col], layer_sizes[col + 1])\n        x = np.maximum(0, x @ W)  # ReLU\n\n        axes[row, col].hist(x.flatten(), bins=50, density=True, alpha=0.7, color='steelblue')\n        axes[row, col].set_xlim(-1, 5)\n\n        if col == 0:\n            axes[row, col].set_ylabel(name, fontsize=10, fontweight='bold')\n        if row == 0:\n            axes[row, col].set_title(f'Layer {col+1}', fontsize=10)\n\n        mean_val = x.mean()\n        std_val = x.std()\n        axes[row, col].text(0.7, 0.9, f'\u03bc={mean_val:.2f}\\n\u03c3={std_val:.2f}',\n                           transform=axes[row, col].transAxes, fontsize=7,\n                           verticalalignment='top')\n\nplt.suptitle('Activation Distributions Across Layers (ReLU)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"10\"></a>\n## 10. The Mathematics of Attention\n\n### 10.1 Self-Attention Mechanism\n\nAttention allows the model to focus on different parts of the input for each output element.\n\n**Scaled Dot-Product Attention:**\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nWhere:\n- $Q$ (Query): \"What am I looking for?\"\n- $K$ (Key): \"What do I contain?\"\n- $V$ (Value): \"What information do I provide?\"\n- $d_k$: dimension of keys (scaling factor prevents large dot products)\n\n### 10.2 Multi-Head Attention\n\n$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n$$\\text{where head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$\n\nThis allows the model to attend to different representation subspaces simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    Scaled Dot-Product Attention.\n\n    Args:\n        Q: Queries [batch, heads, seq_len, d_k]\n        K: Keys    [batch, heads, seq_len, d_k]\n        V: Values  [batch, heads, seq_len, d_v]\n    \"\"\"\n    d_k = Q.shape[-1]\n\n    # Compute attention scores\n    scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(d_k)\n\n    if mask is not None:\n        scores = np.where(mask, scores, -1e9)\n\n    # Softmax\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = exp_scores / exp_scores.sum(axis=-1, keepdims=True)\n\n    # Weighted sum of values\n    output = attention_weights @ V\n\n    return output, attention_weights\n\nclass MultiHeadAttention:\n    def __init__(self, d_model, num_heads):\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        # Initialize projection matrices\n        self.W_Q = np.random.randn(d_model, d_model) * 0.02\n        self.W_K = np.random.randn(d_model, d_model) * 0.02\n        self.W_V = np.random.randn(d_model, d_model) * 0.02\n        self.W_O = np.random.randn(d_model, d_model) * 0.02\n\n    def split_heads(self, x):\n        batch, seq_len, d_model = x.shape\n        return x.reshape(batch, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n\n    def forward(self, x):\n        batch, seq_len, d_model = x.shape\n\n        Q = self.split_heads(x @ self.W_Q)\n        K = self.split_heads(x @ self.W_K)\n        V = self.split_heads(x @ self.W_V)\n\n        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V)\n\n        # Concatenate heads\n        attn_output = attn_output.transpose(0, 2, 1, 3).reshape(batch, seq_len, d_model)\n        output = attn_output @ self.W_O\n\n        return output, attn_weights\n\n# Demo\nnp.random.seed(42)\nseq_len, d_model, num_heads = 6, 64, 8\n\n# Simulate a sequence of 6 tokens with 64-dim embeddings\nx = np.random.randn(1, seq_len, d_model)\nmha = MultiHeadAttention(d_model, num_heads)\noutput, attn_weights = mha.forward(x)\n\nprint(f\"Input shape:  {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Attention weights shape: {attn_weights.shape}\")\n\n# Visualize attention weights for each head\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\ntokens = ['Token_0', 'Token_1', 'Token_2', 'Token_3', 'Token_4', 'Token_5']\n\nfor i, ax in enumerate(axes.flat):\n    im = ax.imshow(attn_weights[0, i], cmap='Blues', vmin=0, vmax=0.5)\n    ax.set_title(f'Head {i+1}', fontsize=12, fontweight='bold')\n    ax.set_xticks(range(seq_len))\n    ax.set_yticks(range(seq_len))\n    ax.set_xticklabels(tokens, rotation=45, fontsize=8)\n    ax.set_yticklabels(tokens, fontsize=8)\n\nplt.suptitle('Multi-Head Attention Weights (8 Heads)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"11\"></a>\n## 11. Practical Exercises\n\n### Exercise 1: XOR Problem (Beginner)\nImplement a 2-layer neural network from scratch that solves the XOR problem. XOR is not linearly separable, proving the need for hidden layers.\n\n### Exercise 2: Gradient Checking (Intermediate)\nImplement numerical gradient checking to verify your backpropagation:\n$$\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta + \\epsilon) - L(\\theta - \\epsilon)}{2\\epsilon}$$\n\n### Exercise 3: Custom Optimizer (Advanced)\nImplement the AdamW optimizer from scratch and compare it with Adam on a training task.\n\n### Exercise 4: Architecture Design (Architect)\nDesign a neural network architecture for a specific constraint:\n- Input: 28x28 grayscale images\n- Maximum 50K parameters\n- Target: >95% accuracy on MNIST\n- Must include: Conv layers, BatchNorm, residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: XOR Problem from Scratch\nprint(\"=\" * 50)\nprint(\"Exercise 1: Solving XOR with a Neural Network\")\nprint(\"=\" * 50)\n\n# XOR data\nX_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T  # (2, 4)\nY_xor = np.array([[0, 1, 1, 0]])  # (1, 4)\n\n# Small network: 2 -> 4 -> 1\nnp.random.seed(42)\nW1 = np.random.randn(4, 2) * 0.5\nb1 = np.zeros((4, 1))\nW2 = np.random.randn(1, 4) * 0.5\nb2 = np.zeros((1, 1))\n\nlr = 1.0\nlosses = []\n\nfor epoch in range(10000):\n    # Forward\n    Z1 = W1 @ X_xor + b1\n    A1 = np.maximum(0, Z1)  # ReLU\n    Z2 = W2 @ A1 + b2\n    A2 = 1 / (1 + np.exp(-Z2))  # Sigmoid\n\n    # Loss\n    loss = -np.mean(Y_xor * np.log(A2 + 1e-8) + (1 - Y_xor) * np.log(1 - A2 + 1e-8))\n    losses.append(loss)\n\n    # Backward\n    dZ2 = A2 - Y_xor\n    dW2 = (1/4) * dZ2 @ A1.T\n    db2 = (1/4) * np.sum(dZ2, axis=1, keepdims=True)\n    dA1 = W2.T @ dZ2\n    dZ1 = dA1 * (Z1 > 0)\n    dW1 = (1/4) * dZ1 @ X_xor.T\n    db1 = (1/4) * np.sum(dZ1, axis=1, keepdims=True)\n\n    # Update\n    W1 -= lr * dW1; b1 -= lr * db1\n    W2 -= lr * dW2; b2 -= lr * db2\n\n# Results\npredictions = (A2 > 0.5).astype(int)\nprint(f\"\\nInput -> Expected -> Predicted\")\nfor i in range(4):\n    print(f\"  {X_xor[:, i]} ->    {Y_xor[0, i]}     ->    {predictions[0, i]}  (prob: {A2[0, i]:.4f})\")\n\nprint(f\"\\nFinal loss: {losses[-1]:.6f}\")\nprint(f\"All correct: {np.all(predictions == Y_xor)}\")\n\n# Plot loss\nplt.figure(figsize=(10, 4))\nplt.plot(losses, linewidth=2, color='blue')\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('XOR Training Loss (From Scratch)', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\n| Concept | Key Takeaway |\n|---------|-------------|\n| **Neuron** | $y = f(\\mathbf{w}^T\\mathbf{x} + b)$ \u2014 weighted sum + non-linearity |\n| **Backpropagation** | Chain rule applied through computational graph |\n| **Loss Functions** | Cross-entropy for classification, MSE for regression |\n| **Optimizers** | Adam/AdamW are the modern defaults |\n| **Regularization** | Dropout + BatchNorm + weight decay = standard recipe |\n| **Convolutions** | Local feature extraction with weight sharing |\n| **Initialization** | He init for ReLU, Xavier for sigmoid/tanh |\n| **Attention** | $\\text{softmax}(QK^T/\\sqrt{d_k})V$ \u2014 the foundation of Transformers |\n\n**Next Notebook:** [Deep Learning Pet Classifier Project](DeepLearningProject_Assignment.ipynb) \u2014 Apply these concepts to build real models!"
   ]
  }
 ]
}