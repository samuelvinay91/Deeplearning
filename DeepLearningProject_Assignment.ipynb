{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Pet Classifier: A Comprehensive Guide\n",
    "## From First Principles to Production-Grade CNNs\n",
    "\n",
    "---\n",
    "\n",
    "**Original notebook:** Basic TF 1.x CNN with 2 convolutional layers for cats vs. dogs (40 training images).  \n",
    "**This version:** A complete, modern TensorFlow 2 / Keras rewrite covering theory, three model tiers, interpretability, and production deployment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction & Learning Objectives\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "This notebook is a self-contained deep-learning course built around a single, concrete task: **classifying images of cats and dogs**. By the time you reach the final cell you will have:\n",
    "\n",
    "1. **Understood** how Convolutional Neural Networks (CNNs) see images -- from raw pixels, through learned filters, to high-level features.\n",
    "2. **Built** three progressively more powerful classifiers and compared them head-to-head.\n",
    "3. **Interpreted** model decisions with Grad-CAM heat-maps and layer-wise feature-map visualisations.\n",
    "4. **Prepared** a model for production: SavedModel export, TensorFlow Lite conversion, and quantisation.\n",
    "\n",
    "### Skill-Level Road Map\n",
    "\n",
    "| Level | Sections | What You Will Do |\n",
    "|-------|----------|------------------|\n",
    "| **Beginner** | 1 -- 6 | Set up the environment, explore the data, build and train a simple CNN. |\n",
    "| **Intermediate** | 7 | Add Batch Normalisation, learning-rate schedules, callbacks, and deeper architectures. |\n",
    "| **Advanced** | 8 | Apply transfer learning with MobileNetV2 (feature extraction + fine-tuning). |\n",
    "| **Architect** | 9 -- 12 | Evaluate models with Grad-CAM, export for mobile, discuss serving, and tackle design challenges. |\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.8+\n",
    "- Basic familiarity with NumPy and Matplotlib\n",
    "- Conceptual understanding of what a neural network is (activation functions, backpropagation)\n",
    "- A GPU runtime is **strongly recommended** (Google Colab provides one for free)\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "Run cells **in order**. Each section builds on the previous one. Markdown cells provide the theory; code cells let you experiment. Exercises at the end encourage you to go further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Environment Setup\n",
    "\n",
    "We use **TensorFlow 2.x** with the integrated `tf.keras` API throughout this notebook. No legacy `tf.compat.v1` code is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2-A  Core imports\n",
    "# ============================================================\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Version check\n",
    "print(f\"TensorFlow Version : {tf.__version__}\")\n",
    "print(f\"NumPy Version      : {np.__version__}\")\n",
    "print(f\"GPU available      : {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Mixed-precision (optional speed boost on modern GPUs)\n",
    "# Uncomment the next line if you have a GPU with compute capability >= 7.0\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2-B  Install tensorflow_datasets if not already available\n",
    "# ============================================================\n",
    "try:\n",
    "    import tensorflow_datasets as tfds\n",
    "    print(f\"tensorflow_datasets version: {tfds.__version__}\")\n",
    "except ImportError:\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"tensorflow_datasets\"])\n",
    "    import tensorflow_datasets as tfds\n",
    "    print(f\"tensorflow_datasets version: {tfds.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2-C  Global constants used across the notebook\n",
    "# ============================================================\n",
    "IMG_SIZE_CUSTOM = 128          # For custom CNN models (Sections 6 & 7)\n",
    "IMG_SIZE_TRANSFER = 224        # For transfer-learning models (Section 8)\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "CLASS_NAMES = [\"cat\", \"dog\"]\n",
    "NUM_CLASSES = len(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Understanding CNNs -- Visual Theory\n",
    "\n",
    "Before writing any model code, let us build intuition for the building blocks of a Convolutional Neural Network.\n",
    "\n",
    "### 3.1 What Is a Convolution?\n",
    "\n",
    "A **convolution** slides a small matrix (called a **kernel** or **filter**) across the input image. At every position it computes the element-wise product and sums the results to produce one value in the output **feature map**.\n",
    "\n",
    "```\n",
    "Input image (5x5)          Kernel (3x3)         Output feature map (3x3)\n",
    "+---+---+---+---+---+      +---+---+---+        +----+----+----+\n",
    "| 1 | 0 | 1 | 0 | 1 |      | 1 | 0 | 1 |        | 4  | 3  | 4  |\n",
    "+---+---+---+---+---+      +---+---+---+        +----+----+----+\n",
    "| 0 | 1 | 0 | 1 | 0 |      | 0 | 1 | 0 |        | 2  | 4  | 3  |\n",
    "+---+---+---+---+---+      +---+---+---+        +----+----+----+\n",
    "| 1 | 0 | 1 | 0 | 1 |      | 1 | 0 | 1 |        | 4  | 3  | 4  |\n",
    "+---+---+---+---+---+                            +----+----+----+\n",
    "| 0 | 1 | 0 | 1 | 0 |\n",
    "+---+---+---+---+---+\n",
    "| 1 | 0 | 1 | 0 | 1 |\n",
    "+---+---+---+---+---+\n",
    "```\n",
    "\n",
    "With **valid** padding the output shrinks; with **same** padding we zero-pad the border so the output retains the input dimensions.\n",
    "\n",
    "### 3.2 Stride\n",
    "\n",
    "The **stride** controls how far the kernel moves at each step.\n",
    "\n",
    "- Stride 1: move one pixel at a time (output size ~ input size).\n",
    "- Stride 2: move two pixels at a time (output size ~ input size / 2).\n",
    "\n",
    "```\n",
    "Stride = 1                          Stride = 2\n",
    "[X][X][X][ ][ ][ ]  -> step 1      [X][X][X][ ][ ][ ]  -> step 1\n",
    "[ ][X][X][X][ ][ ]  -> step 2      [ ][ ][X][X][X][ ]  -> step 2\n",
    "[ ][ ][X][X][X][ ]  -> step 3      [ ][ ][ ][ ][X][X]  -> step 3  (only 3 steps vs 4)\n",
    "[ ][ ][ ][X][X][X]  -> step 4\n",
    "```\n",
    "\n",
    "### 3.3 Pooling\n",
    "\n",
    "**Max pooling** (most common) picks the largest value in each local patch, reducing spatial dimensions while preserving the most activated features.\n",
    "\n",
    "```\n",
    "Input (4x4)            MaxPool 2x2, stride 2         Output (2x2)\n",
    "+---+---+---+---+                                    +---+---+\n",
    "| 1 | 3 | 2 | 1 |      max(1,3,0,2)=3               | 3 | 2 |\n",
    "+---+---+---+---+      max(2,1,1,2)=2               +---+---+\n",
    "| 0 | 2 | 1 | 2 |      max(3,1,0,2)=3               | 3 | 4 |\n",
    "+---+---+---+---+      max(0,4,1,2)=4               +---+---+\n",
    "| 3 | 1 | 0 | 4 |\n",
    "+---+---+---+---+\n",
    "| 0 | 2 | 1 | 2 |\n",
    "+---+---+---+---+\n",
    "```\n",
    "\n",
    "### 3.4 Receptive Field\n",
    "\n",
    "The **receptive field** of a neuron is the region of the original input that influences its value. Deeper layers have larger receptive fields, allowing them to capture more global patterns:\n",
    "\n",
    "```\n",
    "Layer 1 neuron  ->  sees a 3x3 patch of the input (edges, textures)\n",
    "Layer 2 neuron  ->  sees a 5x5 patch of the input (parts of objects)\n",
    "Layer 3 neuron  ->  sees a 7x7+ patch of the input (whole objects)\n",
    "```\n",
    "\n",
    "This hierarchical feature extraction is the key power of CNNs:\n",
    "\n",
    "```\n",
    "Input Image  -->  Edges/Textures  -->  Eyes/Ears/Fur  -->  Cat or Dog\n",
    "  (pixels)        (low-level)          (mid-level)         (high-level)\n",
    "```\n",
    "\n",
    "### 3.5 Typical CNN Architecture\n",
    "\n",
    "```\n",
    "INPUT (224x224x3)\n",
    "  |\n",
    "  v\n",
    "[CONV 3x3, 32 filters] -> [BatchNorm] -> [ReLU] -> [MaxPool 2x2]\n",
    "  |  (112x112x32)\n",
    "  v\n",
    "[CONV 3x3, 64 filters] -> [BatchNorm] -> [ReLU] -> [MaxPool 2x2]\n",
    "  |  (56x56x64)\n",
    "  v\n",
    "[CONV 3x3, 128 filters] -> [BatchNorm] -> [ReLU] -> [MaxPool 2x2]\n",
    "  |  (28x28x128)\n",
    "  v\n",
    "[GlobalAveragePooling2D]  -> (128,)\n",
    "  |\n",
    "  v\n",
    "[Dense 128] -> [Dropout 0.5] -> [Dense 1, sigmoid]\n",
    "  |\n",
    "  v\n",
    "OUTPUT: probability(dog)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Dataset -- Cats vs. Dogs\n",
    "\n",
    "We use the **cats_vs_dogs** dataset from `tensorflow_datasets`. It contains roughly **23,262 labelled JPEG images** -- orders of magnitude larger than the 40-image toy set in the original notebook. A realistic dataset size is essential for meaningful training and evaluation.\n",
    "\n",
    "### Split strategy\n",
    "\n",
    "| Split | Percentage | Purpose |\n",
    "|-------|------------|---------|\n",
    "| Train | 70 % | Model weight updates |\n",
    "| Validation | 15 % | Hyperparameter tuning and early stopping |\n",
    "| Test | 15 % | Final, unbiased evaluation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4-A  Download and split the dataset\n",
    "# ============================================================\n",
    "# tensorflow_datasets provides named splits.\n",
    "# cats_vs_dogs only ships a single 'train' split, so we\n",
    "# carve out our own train / val / test partitions.\n",
    "\n",
    "(raw_train, raw_val, raw_test), ds_info = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split=['train[:70%]', 'train[70%:85%]', 'train[85%:]'],\n",
    "    as_supervised=True,   # returns (image, label) tuples\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "print(ds_info.description)\n",
    "print(f\"\\nTotal examples : {ds_info.splits['train'].num_examples}\")\n",
    "print(f\"Label names    : {ds_info.features['label'].names}\")\n",
    "print(f\"Train size     : {raw_train.cardinality().numpy()}\")\n",
    "print(f\"Val size       : {raw_val.cardinality().numpy()}\")\n",
    "print(f\"Test size      : {raw_test.cardinality().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4-B  Explore sample images\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
    "fig.suptitle(\"Sample Images from the Training Set\", fontsize=16, y=1.02)\n",
    "\n",
    "for i, (image, label) in enumerate(raw_train.take(18)):\n",
    "    ax = axes[i // 6, i % 6]\n",
    "    ax.imshow(image.numpy())\n",
    "    ax.set_title(CLASS_NAMES[label.numpy()], fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4-C  Class distribution analysis\n",
    "# ============================================================\n",
    "def count_classes(dataset):\n",
    "    \"\"\"Count number of samples per class in a tf.data.Dataset.\"\"\"\n",
    "    cats, dogs = 0, 0\n",
    "    for _, label in dataset:\n",
    "        if label.numpy() == 0:\n",
    "            cats += 1\n",
    "        else:\n",
    "            dogs += 1\n",
    "    return cats, dogs\n",
    "\n",
    "train_cats, train_dogs = count_classes(raw_train)\n",
    "val_cats, val_dogs = count_classes(raw_val)\n",
    "test_cats, test_dogs = count_classes(raw_test)\n",
    "\n",
    "print(f\"Train -- cats: {train_cats}, dogs: {train_dogs}\")\n",
    "print(f\"Val   -- cats: {val_cats},   dogs: {val_dogs}\")\n",
    "print(f\"Test  -- cats: {test_cats},  dogs: {test_dogs}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "for ax, (title, counts) in zip(axes, [\n",
    "    (\"Train\", [train_cats, train_dogs]),\n",
    "    (\"Validation\", [val_cats, val_dogs]),\n",
    "    (\"Test\", [test_cats, test_dogs]),\n",
    "]):\n",
    "    bars = ax.bar(CLASS_NAMES, counts, color=['#4C72B0', '#DD8452'])\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    for bar, c in zip(bars, counts):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 20,\n",
    "                str(c), ha='center', fontsize=11)\n",
    "\n",
    "plt.suptitle(\"Class Distribution Across Splits\", fontsize=15, y=1.04)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4-D  Image dimension analysis\n",
    "# ============================================================\n",
    "heights, widths = [], []\n",
    "for image, _ in raw_train.take(500):\n",
    "    h, w, _ = image.shape\n",
    "    heights.append(h)\n",
    "    widths.append(w)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].hist(heights, bins=40, color='steelblue', edgecolor='white')\n",
    "axes[0].set_title('Height Distribution (first 500 images)')\n",
    "axes[0].set_xlabel('Pixels')\n",
    "axes[1].hist(widths, bins=40, color='coral', edgecolor='white')\n",
    "axes[1].set_title('Width Distribution (first 500 images)')\n",
    "axes[1].set_xlabel('Pixels')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Height -- min: {min(heights)}, max: {max(heights)}, median: {np.median(heights):.0f}\")\n",
    "print(f\"Width  -- min: {min(widths)},  max: {max(widths)},  median: {np.median(widths):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Data Pipeline & Augmentation\n",
    "\n",
    "### Why does this matter?\n",
    "\n",
    "A well-built `tf.data` pipeline keeps the GPU fed with data and avoids I/O bottlenecks. **Data augmentation** artificially expands the effective training set, reducing overfitting.\n",
    "\n",
    "### Pipeline recipe\n",
    "\n",
    "```\n",
    "raw images\n",
    "  -> resize to fixed shape\n",
    "  -> normalise pixel values to [0, 1]\n",
    "  -> (training only) random augmentation\n",
    "  -> batch\n",
    "  -> prefetch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5-A  Preprocessing functions\n",
    "# ============================================================\n",
    "\n",
    "def preprocess(image, label, img_size):\n",
    "    \"\"\"Resize and normalise an image.\"\"\"\n",
    "    image = tf.image.resize(image, [img_size, img_size])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def make_dataset(raw_ds, img_size, batch_size, augment=False, shuffle=False, cache=True):\n",
    "    \"\"\"Build a performant tf.data pipeline.\"\"\"\n",
    "    ds = raw_ds.map(lambda img, lbl: preprocess(img, lbl, img_size),\n",
    "                    num_parallel_calls=AUTOTUNE)\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size)\n",
    "    if augment:\n",
    "        # Augmentation is applied *after* batching for efficiency\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "            tf.keras.layers.RandomRotation(0.15),\n",
    "            tf.keras.layers.RandomZoom(0.15),\n",
    "            tf.keras.layers.RandomContrast(0.2),\n",
    "        ])\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                    num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.prefetch(AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5-B  Build pipelines for custom CNN models (128x128)\n",
    "# ============================================================\n",
    "train_ds_128 = make_dataset(raw_train, IMG_SIZE_CUSTOM, BATCH_SIZE,\n",
    "                            augment=True, shuffle=True)\n",
    "val_ds_128   = make_dataset(raw_val,   IMG_SIZE_CUSTOM, BATCH_SIZE)\n",
    "test_ds_128  = make_dataset(raw_test,  IMG_SIZE_CUSTOM, BATCH_SIZE)\n",
    "\n",
    "# Quick sanity check\n",
    "for images, labels in train_ds_128.take(1):\n",
    "    print(f\"Batch shape : {images.shape}\")\n",
    "    print(f\"Label batch : {labels.numpy()[:10]}\")\n",
    "    print(f\"Pixel range : [{images.numpy().min():.3f}, {images.numpy().max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5-C  Build pipelines for transfer learning models (224x224)\n",
    "# ============================================================\n",
    "train_ds_224 = make_dataset(raw_train, IMG_SIZE_TRANSFER, BATCH_SIZE,\n",
    "                            augment=True, shuffle=True)\n",
    "val_ds_224   = make_dataset(raw_val,   IMG_SIZE_TRANSFER, BATCH_SIZE)\n",
    "test_ds_224  = make_dataset(raw_test,  IMG_SIZE_TRANSFER, BATCH_SIZE)\n",
    "\n",
    "for images, labels in train_ds_224.take(1):\n",
    "    print(f\"Batch shape : {images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5-D  Visualise augmented samples\n",
    "# ============================================================\n",
    "# Take one original image and show it alongside several augmented versions.\n",
    "\n",
    "augmenter = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.15),\n",
    "    tf.keras.layers.RandomZoom(0.15),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "# Grab a single image\n",
    "for images, labels in train_ds_128.take(1):\n",
    "    original = images[0]  # shape (128, 128, 3)\n",
    "    break\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(16, 7))\n",
    "axes[0, 0].imshow(original.numpy())\n",
    "axes[0, 0].set_title(\"Original\", fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "for i in range(1, 10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    augmented = augmenter(tf.expand_dims(original, 0), training=True)\n",
    "    ax.imshow(tf.squeeze(augmented).numpy())\n",
    "    ax.set_title(f\"Aug {i}\", fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Data Augmentation Examples\", fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Model 1 -- Basic CNN (Beginner Level)\n",
    "\n",
    "Our first model is intentionally simple: three convolutional blocks followed by a dense classifier. This mirrors the architecture style from the original notebook but uses the modern `tf.keras.Sequential` API.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Conv2D(32, 3x3) -> ReLU -> MaxPool(2x2)\n",
    "Conv2D(64, 3x3) -> ReLU -> MaxPool(2x2)\n",
    "Conv2D(128, 3x3) -> ReLU -> MaxPool(2x2)\n",
    "Flatten -> Dense(128) -> ReLU -> Dropout(0.5) -> Dense(1, sigmoid)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6-A  Define Model 1\n",
    "# ============================================================\n",
    "\n",
    "def build_basic_cnn(input_shape=(IMG_SIZE_CUSTOM, IMG_SIZE_CUSTOM, 3)):\n",
    "    \"\"\"A straightforward 3-block CNN for binary classification.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        # Block 1\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                               padding='same', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Block 2\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Block 3\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Classifier head\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ], name='basic_cnn')\n",
    "    return model\n",
    "\n",
    "model1 = build_basic_cnn()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6-B  Compile Model 1\n",
    "# ============================================================\n",
    "model1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6-C  Train Model 1\n",
    "# ============================================================\n",
    "EPOCHS_M1 = 15\n",
    "\n",
    "history1 = model1.fit(\n",
    "    train_ds_128,\n",
    "    validation_data=val_ds_128,\n",
    "    epochs=EPOCHS_M1,\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal train accuracy : {history1.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final val accuracy   : {history1.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6-D  Plot training curves -- reusable helper\n",
    "# ============================================================\n",
    "\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    \"\"\"Plot accuracy and loss curves for training and validation.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "    axes[0].set_title('Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Loss\n",
    "    axes[1].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "    axes[1].set_title('Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    fig.suptitle(title, fontsize=15, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_training_history(history1, title=\"Model 1 -- Basic CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations (Model 1)\n",
    "\n",
    "With only three convolutional blocks and no normalisation, this model is likely to overfit within a handful of epochs: training accuracy climbs sharply while validation accuracy plateaus or drops. This motivates the enhancements in Section 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Model 2 -- Enhanced CNN (Intermediate Level)\n",
    "\n",
    "We improve on Model 1 by adding:\n",
    "\n",
    "| Technique | Benefit |\n",
    "|-----------|---------|\n",
    "| **Batch Normalisation** | Stabilises and accelerates training by normalising layer inputs |\n",
    "| **More convolutional blocks** | Increases model capacity and receptive field |\n",
    "| **GlobalAveragePooling2D** | Replaces Flatten -- dramatically reduces parameters and overfitting |\n",
    "| **Learning-rate schedule** | Reduces LR when validation loss plateaus |\n",
    "| **EarlyStopping** | Halts training automatically when the model stops improving |\n",
    "| **ModelCheckpoint** | Saves the best weights to disk |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7-A  Define Model 2\n",
    "# ============================================================\n",
    "\n",
    "def build_enhanced_cnn(input_shape=(IMG_SIZE_CUSTOM, IMG_SIZE_CUSTOM, 3)):\n",
    "    \"\"\"A deeper CNN with BatchNorm, GlobalAveragePooling, and Dropout.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        # Block 1\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=input_shape),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        # Block 2\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        # Block 3\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        # Block 4\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "\n",
    "        # Classifier head\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ], name='enhanced_cnn')\n",
    "    return model\n",
    "\n",
    "model2 = build_enhanced_cnn()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7-B  Compile Model 2 with callbacks\n",
    "# ============================================================\n",
    "model2.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "callbacks_m2 = [\n",
    "    # Reduce LR when val_loss stops improving\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
    "\n",
    "    # Stop training when no improvement for 7 epochs\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),\n",
    "\n",
    "    # Save best model weights\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model2.keras', monitor='val_loss',\n",
    "        save_best_only=True, verbose=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7-C  Train Model 2\n",
    "# ============================================================\n",
    "EPOCHS_M2 = 30\n",
    "\n",
    "history2 = model2.fit(\n",
    "    train_ds_128,\n",
    "    validation_data=val_ds_128,\n",
    "    epochs=EPOCHS_M2,\n",
    "    callbacks=callbacks_m2,\n",
    ")\n",
    "\n",
    "print(f\"\\nBest val accuracy achieved: {max(history2.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7-D  Plot training curves for Model 2\n",
    "# ============================================================\n",
    "plot_training_history(history2, title=\"Model 2 -- Enhanced CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7-E  Compare Model 1 vs Model 2 on the validation set\n",
    "# ============================================================\n",
    "m1_val_loss, m1_val_acc = model1.evaluate(val_ds_128, verbose=0)\n",
    "m2_val_loss, m2_val_acc = model2.evaluate(val_ds_128, verbose=0)\n",
    "\n",
    "print(f\"Model 1 (Basic CNN)    -- Val Loss: {m1_val_loss:.4f}  Val Acc: {m1_val_acc:.4f}\")\n",
    "print(f\"Model 2 (Enhanced CNN) -- Val Loss: {m2_val_loss:.4f}  Val Acc: {m2_val_acc:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "bars = ax.bar(['Model 1\\n(Basic CNN)', 'Model 2\\n(Enhanced CNN)'],\n",
    "              [m1_val_acc, m2_val_acc],\n",
    "              color=['#4C72B0', '#55A868'], edgecolor='white', width=0.5)\n",
    "for bar, acc in zip(bars, [m1_val_acc, m2_val_acc]):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,\n",
    "            f\"{acc:.3f}\", ha='center', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_ylabel('Validation Accuracy')\n",
    "ax.set_title('Model Comparison (so far)', fontsize=14)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Model 3 -- Transfer Learning with MobileNetV2 (Advanced Level)\n",
    "\n",
    "Training a CNN from scratch requires a large dataset and significant compute. **Transfer learning** lets us leverage a model that has already been trained on millions of images (ImageNet) and re-use its learned feature representations.\n",
    "\n",
    "### Strategy\n",
    "\n",
    "1. **Feature extraction** -- Freeze the entire MobileNetV2 base and only train the new classification head.\n",
    "2. **Fine-tuning** -- Unfreeze the top layers of MobileNetV2 and train them with a very small learning rate.\n",
    "\n",
    "We use **MobileNetV2** because it is:\n",
    "- Lightweight (3.4 M parameters vs 138 M for VGG-16)\n",
    "- Fast inference (designed for mobile devices)\n",
    "- Still very accurate on ImageNet\n",
    "- The natural choice if we later want to deploy on mobile (Section 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8-A  Load MobileNetV2 as the base model\n",
    "# ============================================================\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_SIZE_TRANSFER, IMG_SIZE_TRANSFER, 3),\n",
    "    include_top=False,          # discard the ImageNet classifier head\n",
    "    weights='imagenet',\n",
    ")\n",
    "\n",
    "# Freeze all base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "print(f\"Base model layers      : {len(base_model.layers)}\")\n",
    "print(f\"Trainable variables    : {len(base_model.trainable_variables)}\")\n",
    "print(f\"Non-trainable variables: {len(base_model.non_trainable_variables)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8-B  Build Model 3 (feature extraction head)\n",
    "# ============================================================\n",
    "\n",
    "def build_transfer_model(base):\n",
    "    \"\"\"Add a classification head on top of a frozen base model.\"\"\"\n",
    "    # MobileNetV2 expects pixels in [-1, 1]\n",
    "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(IMG_SIZE_TRANSFER, IMG_SIZE_TRANSFER, 3))\n",
    "    x = preprocess_input(inputs)        # rescale from [0,1] to [-1,1]\n",
    "    x = base(x, training=False)         # keep BN layers in inference mode\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='mobilenetv2_transfer')\n",
    "    return model\n",
    "\n",
    "model3 = build_transfer_model(base_model)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8-C  Phase 1: Feature extraction (frozen base)\n",
    "# ============================================================\n",
    "model3.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "EPOCHS_FE = 5  # Feature extraction only needs a few epochs\n",
    "\n",
    "history3_fe = model3.fit(\n",
    "    train_ds_224,\n",
    "    validation_data=val_ds_224,\n",
    "    epochs=EPOCHS_FE,\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter feature extraction -- Val Acc: {history3_fe.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8-D  Phase 2: Fine-tuning (unfreeze top layers)\n",
    "# ============================================================\n",
    "# Unfreeze the top portion of MobileNetV2.\n",
    "# The model has 154 layers; we unfreeze from layer 120 onward.\n",
    "\n",
    "base_model.trainable = True\n",
    "\n",
    "FINE_TUNE_AT = 120\n",
    "for layer in base_model.layers[:FINE_TUNE_AT]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_count = sum(1 for l in base_model.layers if l.trainable)\n",
    "print(f\"Fine-tuning {trainable_count} layers (out of {len(base_model.layers)})\")\n",
    "\n",
    "# Re-compile with a much lower learning rate to avoid catastrophic forgetting\n",
    "model3.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "callbacks_m3 = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model3.keras', monitor='val_loss',\n",
    "        save_best_only=True, verbose=1),\n",
    "]\n",
    "\n",
    "EPOCHS_FT = 15\n",
    "total_epochs = EPOCHS_FE + EPOCHS_FT\n",
    "\n",
    "history3_ft = model3.fit(\n",
    "    train_ds_224,\n",
    "    validation_data=val_ds_224,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=EPOCHS_FE,  # continue from where feature extraction left off\n",
    "    callbacks=callbacks_m3,\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter fine-tuning -- Val Acc: {max(history3_ft.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8-E  Plot combined training history for Model 3\n",
    "# ============================================================\n",
    "# Merge the two phases into a single history for plotting.\n",
    "\n",
    "def merge_histories(h1, h2):\n",
    "    \"\"\"Concatenate two Keras history objects.\"\"\"\n",
    "    merged = {}\n",
    "    for key in h1.history:\n",
    "        merged[key] = h1.history[key] + h2.history[key]\n",
    "    class MergedHistory:\n",
    "        pass\n",
    "    mh = MergedHistory()\n",
    "    mh.history = merged\n",
    "    return mh\n",
    "\n",
    "history3_full = merge_histories(history3_fe, history3_ft)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, metric, title in zip(axes, ['accuracy', 'loss'], ['Accuracy', 'Loss']):\n",
    "    ax.plot(history3_full.history[metric], label='Train', linewidth=2)\n",
    "    ax.plot(history3_full.history[f'val_{metric}'], label='Validation', linewidth=2)\n",
    "    ax.axvline(x=EPOCHS_FE - 1, color='gray', linestyle='--', label='Fine-tune start')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Model 3 -- MobileNetV2 Transfer Learning\", fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Model Evaluation & Interpretation (Architect Level)\n",
    "\n",
    "Accuracy alone rarely tells the full story. In this section we:\n",
    "\n",
    "1. Evaluate all three models on the **held-out test set**.\n",
    "2. Visualise the **confusion matrix**.\n",
    "3. Generate a **classification report** (precision, recall, F1).\n",
    "4. Implement **Grad-CAM** from scratch to see *where* the model is looking.\n",
    "5. Visualise intermediate **feature maps**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9-A  Evaluate all models on the test set\n",
    "# ============================================================\n",
    "results = {}\n",
    "\n",
    "for name, model, ds in [\n",
    "    ('Model 1 (Basic CNN)',      model1, test_ds_128),\n",
    "    ('Model 2 (Enhanced CNN)',   model2, test_ds_128),\n",
    "    ('Model 3 (MobileNetV2)',    model3, test_ds_224),\n",
    "]:\n",
    "    loss, acc = model.evaluate(ds, verbose=0)\n",
    "    results[name] = {'loss': loss, 'accuracy': acc}\n",
    "    print(f\"{name:<30s}  Test Loss: {loss:.4f}  Test Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9-B  Model comparison bar chart\n",
    "# ============================================================\n",
    "names = list(results.keys())\n",
    "accs  = [results[n]['accuracy'] for n in names]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "colors = ['#4C72B0', '#55A868', '#C44E52']\n",
    "bars = ax.barh(names, accs, color=colors, edgecolor='white', height=0.5)\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{acc:.3f}\", va='center', fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(0, 1.08)\n",
    "ax.set_xlabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('All Models -- Test Set Comparison', fontsize=14)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9-C  Confusion matrix & classification report\n",
    "# ============================================================\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "def evaluate_model_detailed(model, dataset, model_name, class_names=CLASS_NAMES):\n",
    "    \"\"\"Generate predictions and display confusion matrix + classification report.\"\"\"\n",
    "    y_true, y_pred_prob = [], []\n",
    "    for images, labels in dataset:\n",
    "        preds = model.predict(images, verbose=0)\n",
    "        y_pred_prob.extend(preds.flatten())\n",
    "        y_true.extend(labels.numpy().flatten())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = (np.array(y_pred_prob) >= 0.5).astype(int)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    ax.set_title(f'Confusion Matrix -- {model_name}', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"\\nClassification Report -- {model_name}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    return y_true, y_pred, y_pred_prob\n",
    "\n",
    "\n",
    "# We focus the detailed evaluation on Model 3 (best expected performance)\n",
    "y_true_m3, y_pred_m3, y_prob_m3 = evaluate_model_detailed(\n",
    "    model3, test_ds_224, 'Model 3 (MobileNetV2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9-D  Confusion matrices for Models 1 & 2 as well\n",
    "# ============================================================\n",
    "_ = evaluate_model_detailed(model1, test_ds_128, 'Model 1 (Basic CNN)')\n",
    "_ = evaluate_model_detailed(model2, test_ds_128, 'Model 2 (Enhanced CNN)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Grad-CAM: Where Is the Model Looking?\n",
    "\n",
    "**Gradient-weighted Class Activation Mapping (Grad-CAM)** produces a heat-map highlighting the image regions that most influenced the model's prediction. The algorithm:\n",
    "\n",
    "1. Forward-pass the image to get the prediction.\n",
    "2. Compute the gradient of the predicted class score with respect to the feature maps of the last convolutional layer.\n",
    "3. Global-average-pool these gradients to obtain per-channel importance weights.\n",
    "4. Compute a weighted combination of the feature maps and apply ReLU.\n",
    "5. Upscale and overlay the heat-map on the original image.\n",
    "\n",
    "Reference: Selvaraju et al., \"Grad-CAM: Visual Explanations from Deep Networks\" (ICCV 2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9-E  Grad-CAM implementation\n",
    "# ============================================================\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    \"\"\"\n",
    "    Generate a Grad-CAM heatmap for a given image and model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_array : tf.Tensor\n",
    "        Preprocessed image tensor of shape (1, H, W, 3).\n",
    "    model : tf.keras.Model\n",
    "        The trained Keras model.\n",
    "    last_conv_layer_name : str\n",
    "        Name of the last convolutional layer to compute Grad-CAM for.\n",
    "    pred_index : int or None\n",
    "        Class index.  For binary (sigmoid) output, use 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    heatmap : np.ndarray\n",
    "        2-D array of values in [0, 1].\n",
    "    \"\"\"\n",
    "    # Build a sub-model that outputs both the conv layer output and the final prediction\n",
    "    grad_model = tf.keras.Model(\n",
    "        inputs=model.inputs,\n",
    "        outputs=[model.get_layer(last_conv_layer_name).output, model.output],\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = 0  # binary classification\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    # Gradient of the predicted class w.r.t. the conv layer output\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "\n",
    "    # Global average pooling of gradients -> channel importance weights\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # Weighted combination of feature maps\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # Normalise to [0, 1]\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-8)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "\n",
    "def display_gradcam(img, heatmap, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Overlay the Grad-CAM heatmap on the original image.\n",
    "    \"\"\"\n",
    "    # Resize heatmap to match image\n",
    "    heatmap_resized = tf.image.resize(\n",
    "        heatmap[..., tf.newaxis], (img.shape[0], img.shape[1])\n",
    "    ).numpy().squeeze()\n",
    "\n",
    "    # Apply a colourmap\n",
    "    heatmap_colour = cm.jet(heatmap_resized)[:, :, :3]  # drop alpha channel\n",
    "\n",
    "    # Superimpose\n",
    "    superimposed = heatmap_colour * alpha + img * (1 - alpha)\n",
    "    superimposed = np.clip(superimposed, 0, 1)\n",
    "    return superimposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9-F  Apply Grad-CAM to Model 3 (MobileNetV2)\n",
    "# ============================================================\n",
    "\n",
    "# Find the last convolutional layer in the base model\n",
    "last_conv_layer = None\n",
    "for layer in reversed(base_model.layers):\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        last_conv_layer = layer.name\n",
    "        break\n",
    "print(f\"Last conv layer for Grad-CAM: {last_conv_layer}\")\n",
    "\n",
    "# We need a model that can route gradients through the base model.\n",
    "# Since model3 wraps base_model as a single layer, we build a\n",
    "# \"flat\" version for Grad-CAM purposes.\n",
    "\n",
    "gradcam_model = tf.keras.Model(\n",
    "    inputs=model3.inputs,\n",
    "    outputs=model3.output,\n",
    ")\n",
    "\n",
    "# Grab a batch from the test set\n",
    "for test_images, test_labels in test_ds_224.take(1):\n",
    "    break\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle(\"Grad-CAM Visualisations (Model 3 -- MobileNetV2)\", fontsize=16, y=1.02)\n",
    "\n",
    "for i in range(min(6, test_images.shape[0])):\n",
    "    img = test_images[i]\n",
    "    label = test_labels[i].numpy()\n",
    "    img_batch = tf.expand_dims(img, 0)\n",
    "\n",
    "    # Build a temporary model exposing the inner conv layer\n",
    "    # We access through the nested base_model layer\n",
    "    inner_model = model3.get_layer('mobilenetv2_1.00_224')\n",
    "    grad_sub = tf.keras.Model(\n",
    "        inputs=model3.inputs,\n",
    "        outputs=[inner_model.get_layer(last_conv_layer).output, model3.output],\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_out, pred = grad_sub(img_batch)\n",
    "        class_channel = pred[:, 0]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_out)\n",
    "    pooled = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    heatmap = tf.squeeze(conv_out[0] @ pooled[..., tf.newaxis])\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-8)\n",
    "    heatmap = heatmap.numpy()\n",
    "\n",
    "    superimposed = display_gradcam(img.numpy(), heatmap)\n",
    "\n",
    "    pred_val = pred.numpy()[0, 0]\n",
    "    pred_label = CLASS_NAMES[int(pred_val >= 0.5)]\n",
    "    true_label = CLASS_NAMES[label]\n",
    "\n",
    "    row = i // 2\n",
    "    col = (i % 2) * 2\n",
    "\n",
    "    axes[row, col].imshow(img.numpy())\n",
    "    axes[row, col].set_title(f\"True: {true_label}\", fontsize=11)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "    axes[row, col + 1].imshow(superimposed)\n",
    "    axes[row, col + 1].set_title(f\"Pred: {pred_label} ({pred_val:.2f})\", fontsize=11)\n",
    "    axes[row, col + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9-G  Feature map visualisation (Model 1 -- Basic CNN)\n",
    "# ============================================================\n",
    "# Show what the first few convolutional layers \"see\".\n",
    "\n",
    "# Pick a sample image\n",
    "for sample_images, sample_labels in test_ds_128.take(1):\n",
    "    sample_img = sample_images[0:1]  # keep batch dim\n",
    "    sample_label = sample_labels[0].numpy()\n",
    "    break\n",
    "\n",
    "# Build sub-models that output each conv layer's activations\n",
    "conv_layer_names = [l.name for l in model1.layers if 'conv2d' in l.name]\n",
    "print(f\"Conv layers in Model 1: {conv_layer_names}\")\n",
    "\n",
    "fig_cols = 8\n",
    "for layer_name in conv_layer_names:\n",
    "    sub_model = tf.keras.Model(inputs=model1.inputs,\n",
    "                                outputs=model1.get_layer(layer_name).output)\n",
    "    feature_maps = sub_model.predict(sample_img, verbose=0)\n",
    "    n_filters = feature_maps.shape[-1]\n",
    "    n_show = min(n_filters, fig_cols * 2)  # show up to 16 filters\n",
    "    fig_rows = (n_show + fig_cols - 1) // fig_cols\n",
    "\n",
    "    fig, axes = plt.subplots(fig_rows, fig_cols, figsize=(16, 2.5 * fig_rows))\n",
    "    fig.suptitle(f\"Feature Maps -- {layer_name}  (shape: {feature_maps.shape[1:]})\",\n",
    "                 fontsize=13, y=1.02)\n",
    "    axes = axes.flatten() if fig_rows > 1 else [axes] if fig_rows == 1 and fig_cols == 1 else axes\n",
    "\n",
    "    for j in range(n_show):\n",
    "        axes[j].imshow(feature_maps[0, :, :, j], cmap='viridis')\n",
    "        axes[j].axis('off')\n",
    "        axes[j].set_title(f'Filter {j}', fontsize=9)\n",
    "    # Hide unused axes\n",
    "    for j in range(n_show, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9-H  Model comparison summary table\n",
    "# ============================================================\n",
    "\n",
    "comparison_data = []\n",
    "for name, model, ds in [\n",
    "    ('Model 1 (Basic CNN)',      model1, test_ds_128),\n",
    "    ('Model 2 (Enhanced CNN)',   model2, test_ds_128),\n",
    "    ('Model 3 (MobileNetV2)',    model3, test_ds_224),\n",
    "]:\n",
    "    loss, acc = model.evaluate(ds, verbose=0)\n",
    "    n_params = model.count_params()\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Parameters': f\"{n_params:,}\",\n",
    "        'Test Accuracy': f\"{acc:.4f}\",\n",
    "        'Test Loss': f\"{loss:.4f}\",\n",
    "    })\n",
    "\n",
    "# Print as a formatted table\n",
    "header = f\"{'Model':<30s} {'Parameters':>15s} {'Test Accuracy':>15s} {'Test Loss':>12s}\"\n",
    "print(header)\n",
    "print('-' * len(header))\n",
    "for row in comparison_data:\n",
    "    print(f\"{row['Model']:<30s} {row['Parameters']:>15s} {row['Test Accuracy']:>15s} {row['Test Loss']:>12s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters are the knobs you turn *before* training starts. Finding good settings can make the difference between a mediocre model and a great one.\n",
    "\n",
    "### 10.1 Learning Rate -- The Single Most Important Hyperparameter\n",
    "\n",
    "A **learning rate finder** trains for a few hundred steps while gradually increasing the learning rate from a very small value to a very large one, recording the loss at each step. The ideal learning rate sits in the steepest-descent region of the resulting curve -- typically one order of magnitude below the point where the loss starts exploding.\n",
    "\n",
    "```\n",
    "Loss\n",
    " |\n",
    " |\\                          <- too high: loss explodes\n",
    " | \\        .--.             <- unstable\n",
    " |  \\      /    \\\n",
    " |   \\____/      \\          <- sweet spot is around the bottom\n",
    " |                \\\n",
    " +----+----+----+--+---->\n",
    "   1e-7  1e-5  1e-3  1e-1   Learning Rate (log scale)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10-A  Learning rate finder\n",
    "# ============================================================\n",
    "\n",
    "def lr_finder(model_fn, train_dataset, start_lr=1e-7, end_lr=1.0, num_steps=200):\n",
    "    \"\"\"\n",
    "    Train a fresh model for `num_steps` steps while exponentially\n",
    "    increasing the learning rate from `start_lr` to `end_lr`.\n",
    "    Returns the learning rates and corresponding losses.\n",
    "    \"\"\"\n",
    "    model = model_fn()\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=start_lr),\n",
    "        loss='binary_crossentropy',\n",
    "    )\n",
    "\n",
    "    # Compute the multiplicative factor per step\n",
    "    factor = (end_lr / start_lr) ** (1.0 / num_steps)\n",
    "\n",
    "    lrs, losses = [], []\n",
    "    step = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for images, labels in train_dataset.repeat():\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(images, training=True)\n",
    "            loss = tf.keras.losses.binary_crossentropy(labels, tf.squeeze(preds))\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        current_lr = start_lr * (factor ** step)\n",
    "        model.optimizer.learning_rate.assign(current_lr)\n",
    "\n",
    "        lrs.append(current_lr)\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        # Stop if loss explodes (> 4x the best)\n",
    "        if loss.numpy() < best_loss:\n",
    "            best_loss = loss.numpy()\n",
    "        if loss.numpy() > best_loss * 4:\n",
    "            break\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return lrs, losses\n",
    "\n",
    "\n",
    "# Run the finder on a fresh Basic CNN\n",
    "lrs, losses = lr_finder(build_basic_cnn, train_ds_128, num_steps=300)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(lrs, losses, linewidth=2)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Learning Rate (log scale)', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Learning Rate Finder', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Tip: Pick a learning rate from the steepest downward slope, \"\n",
    "      \"typically 1/10 of the LR where loss begins rising.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Batch Size Impact\n",
    "\n",
    "| Batch Size | Pros | Cons |\n",
    "|------------|------|------|\n",
    "| Small (8--16) | Better generalisation, acts as regulariser | Slower training, noisier gradients |\n",
    "| Medium (32--64) | Good balance of speed and generalisation | -- |\n",
    "| Large (128--512) | Faster per-epoch time on GPUs | May generalise worse, requires LR scaling |\n",
    "\n",
    "**Rule of thumb:** When you double the batch size, double the learning rate (\"linear scaling rule\").\n",
    "\n",
    "### 10.3 Architecture Search (Discussion)\n",
    "\n",
    "Beyond manual tuning, systematic approaches include:\n",
    "\n",
    "- **Grid search**: exhaustive but expensive.\n",
    "- **Random search**: surprisingly effective (Bergstra & Bengio, 2012).\n",
    "- **Bayesian optimisation**: models the objective function to pick the next trial intelligently (e.g., Keras Tuner with `BayesianOptimization`).\n",
    "- **Neural Architecture Search (NAS)**: uses RL or evolutionary methods to design architectures automatically (e.g., EfficientNet was discovered via NAS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: Production Considerations (Architect Level)\n",
    "\n",
    "Training a model is only half the battle. Getting it into users' hands reliably, efficiently, and at scale requires additional engineering.\n",
    "\n",
    "### 11.1 Model Export Formats\n",
    "\n",
    "| Format | Use Case |\n",
    "|--------|----------|\n",
    "| **SavedModel** | TensorFlow Serving, cloud deployment |\n",
    "| **TF Lite (.tflite)** | Mobile and edge devices |\n",
    "| **TF.js** | In-browser inference |\n",
    "| **ONNX** | Cross-framework interoperability |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11-A  Export as SavedModel\n",
    "# ============================================================\n",
    "SAVED_MODEL_DIR = 'pet_classifier_savedmodel'\n",
    "\n",
    "model3.save(SAVED_MODEL_DIR)\n",
    "print(f\"SavedModel exported to: {SAVED_MODEL_DIR}/\")\n",
    "\n",
    "# Verify we can reload it\n",
    "reloaded = tf.keras.models.load_model(SAVED_MODEL_DIR)\n",
    "reloaded_loss, reloaded_acc = reloaded.evaluate(test_ds_224, verbose=0)\n",
    "print(f\"Reloaded model -- Test Acc: {reloaded_acc:.4f} (should match original)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11-B  Convert to TensorFlow Lite\n",
    "# ============================================================\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = 'pet_classifier.tflite'\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "tflite_size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "print(f\"TFLite model saved to: {tflite_path}\")\n",
    "print(f\"TFLite model size    : {tflite_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11-C  Post-training quantisation (INT8)\n",
    "# ============================================================\n",
    "# Dynamic range quantisation -- simplest form, no calibration data needed.\n",
    "\n",
    "converter_quant = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\n",
    "converter_quant.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant = converter_quant.convert()\n",
    "\n",
    "quant_path = 'pet_classifier_quant.tflite'\n",
    "with open(quant_path, 'wb') as f:\n",
    "    f.write(tflite_quant)\n",
    "\n",
    "quant_size_mb = os.path.getsize(quant_path) / (1024 * 1024)\n",
    "print(f\"Quantised model saved to: {quant_path}\")\n",
    "print(f\"Quantised model size    : {quant_size_mb:.2f} MB\")\n",
    "print(f\"Size reduction          : {(1 - quant_size_mb / tflite_size_mb) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11-D  TFLite inference benchmark\n",
    "# ============================================================\n",
    "import time\n",
    "\n",
    "def tflite_predict(tflite_path, images):\n",
    "    \"\"\"Run inference with a TFLite model on a batch of images.\"\"\"\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    predictions = []\n",
    "    for img in images:\n",
    "        img_input = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_details[0]['index'], img_input)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions.append(output[0])\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "# Take a small batch for benchmarking\n",
    "for bench_images, bench_labels in test_ds_224.take(1):\n",
    "    break\n",
    "\n",
    "# Time the full-precision TFLite model\n",
    "start = time.time()\n",
    "preds_fp = tflite_predict(tflite_path, bench_images.numpy())\n",
    "time_fp = time.time() - start\n",
    "\n",
    "# Time the quantised TFLite model\n",
    "start = time.time()\n",
    "preds_q = tflite_predict(quant_path, bench_images.numpy())\n",
    "time_q = time.time() - start\n",
    "\n",
    "print(f\"Full-precision TFLite -- {bench_images.shape[0]} images in {time_fp:.3f}s  \"\n",
    "      f\"({time_fp / bench_images.shape[0] * 1000:.1f} ms/image)\")\n",
    "print(f\"Quantised TFLite      -- {bench_images.shape[0]} images in {time_q:.3f}s  \"\n",
    "      f\"({time_q / bench_images.shape[0] * 1000:.1f} ms/image)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Full-Integer Quantisation (with representative dataset)\n",
    "\n",
    "For maximum speed on edge TPUs or integer-only hardware, you can perform **full integer quantisation** by providing a representative calibration dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11-E  Full integer quantisation\n",
    "# ============================================================\n",
    "\n",
    "def representative_dataset():\n",
    "    \"\"\"Yield representative samples for calibration.\"\"\"\n",
    "    for images, _ in test_ds_224.take(50):\n",
    "        for img in images:\n",
    "            yield [tf.expand_dims(img, 0)]\n",
    "\n",
    "converter_full = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\n",
    "converter_full.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter_full.representative_dataset = representative_dataset\n",
    "# Ensure input/output remain float for ease of use\n",
    "converter_full.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n",
    "]\n",
    "converter_full.inference_input_type = tf.uint8\n",
    "converter_full.inference_output_type = tf.uint8\n",
    "\n",
    "try:\n",
    "    tflite_full_int = converter_full.convert()\n",
    "    full_int_path = 'pet_classifier_full_int8.tflite'\n",
    "    with open(full_int_path, 'wb') as f:\n",
    "        f.write(tflite_full_int)\n",
    "    print(f\"Full INT8 model saved: {full_int_path}  \"\n",
    "          f\"({os.path.getsize(full_int_path) / 1024 / 1024:.2f} MB)\")\n",
    "except Exception as e:\n",
    "    print(f\"Full INT8 conversion failed (this is expected on some ops): {e}\")\n",
    "    print(\"Dynamic-range quantisation (11-C) is still available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Serving Architecture (Discussion)\n",
    "\n",
    "A production serving stack for an image classification model typically looks like this:\n",
    "\n",
    "```\n",
    "                       +------------------+\n",
    "  Client (mobile/web)  |   API Gateway    |   (rate limiting, auth)\n",
    "         |             +--------+---------+\n",
    "         |                      |\n",
    "         v                      v\n",
    "  +------+------+      +-------+--------+\n",
    "  | TFLite model |      | TF Serving     |   (SavedModel, gRPC/REST)\n",
    "  | (on device)  |      | (in container) |\n",
    "  +--------------+      +-------+--------+\n",
    "                                |\n",
    "                        +-------+--------+\n",
    "                        | Model Registry  |   (versioning, A/B tests)\n",
    "                        +----------------+\n",
    "```\n",
    "\n",
    "**Key considerations:**\n",
    "\n",
    "- **Latency budget:** On-device TFLite eliminates network round-trips; server-side allows larger models.\n",
    "- **Batching:** TF Serving can batch multiple requests together for higher GPU utilisation.\n",
    "- **Model versioning:** Roll out new models gradually; roll back instantly if metrics degrade.\n",
    "- **Monitoring:** Track prediction distributions, latency percentiles, and data drift in production.\n",
    "- **Preprocessing consistency:** The same resize/normalise pipeline must run in both training and serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Exercises & Challenges\n",
    "\n",
    "### Beginner Exercises\n",
    "\n",
    "1. **Experiment with Model 1:** Change the number of filters in each convolutional layer (e.g., 16-32-64 instead of 32-64-128). How does this affect accuracy and training speed?\n",
    "\n",
    "2. **Augmentation ablation:** Remove one augmentation at a time from the pipeline (e.g., disable RandomRotation). Re-train Model 1 and observe the effect on validation accuracy.\n",
    "\n",
    "3. **Learning rate experiment:** Train Model 1 with three different learning rates: 1e-2, 1e-3, and 1e-4. Plot all three training curves on the same graph. What do you observe?\n",
    "\n",
    "### Intermediate Exercises\n",
    "\n",
    "4. **Kernel size comparison:** In Model 2, replace all 3x3 kernels with 5x5 kernels. Compare the parameter count and accuracy. Why do modern architectures prefer 3x3 kernels?\n",
    "\n",
    "5. **Regularisation showdown:** Add L2 weight regularisation (`kernel_regularizer=tf.keras.regularizers.l2(1e-4)`) to Model 2's convolutional layers. Does it improve generalisation beyond what BatchNorm and Dropout already provide?\n",
    "\n",
    "6. **Custom learning rate schedule:** Implement a **cosine annealing** schedule using `tf.keras.optimizers.schedules.CosineDecay`. Compare it with ReduceLROnPlateau.\n",
    "\n",
    "### Advanced Exercises\n",
    "\n",
    "7. **Different backbones:** Replace MobileNetV2 with **EfficientNetB0** (`tf.keras.applications.EfficientNetB0`). Compare accuracy, parameter count, and inference speed.\n",
    "\n",
    "8. **Multi-class extension:** Extend the pipeline to classify **cats, dogs, and horses** using the Stanford Dogs dataset or a custom dataset. Change the output layer to softmax with 3 classes.\n",
    "\n",
    "9. **Grad-CAM for all models:** Apply the Grad-CAM implementation to Models 1 and 2. How do the attention maps differ between a shallow CNN and a deep pre-trained model?\n",
    "\n",
    "### Architect Challenges\n",
    "\n",
    "10. **Serving pipeline:** Deploy Model 3 using TensorFlow Serving in a Docker container. Write a Python client that sends an image and receives a prediction.\n",
    "\n",
    "11. **Quantisation-aware training (QAT):** Instead of post-training quantisation, use `tf.quantization` APIs to insert fake-quantisation nodes during training. Compare accuracy with post-training quantisation.\n",
    "\n",
    "12. **Design a real-world system:** You are asked to build a pet breed classifier (120 breeds) that runs on both a mobile app and a web dashboard. Write a one-page architecture document covering:\n",
    "    - Data collection and labelling strategy\n",
    "    - Model selection and training infrastructure\n",
    "    - On-device vs. server-side inference trade-offs\n",
    "    - Monitoring, retraining triggers, and A/B testing\n",
    "    - Privacy considerations (user-uploaded pet photos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Section | Key Takeaway |\n",
    "|---------|-------------|\n",
    "| 3. CNN Theory | Convolutions extract hierarchical features: edges -> parts -> objects |\n",
    "| 4. Dataset | Real-world datasets are large and messy; proper splits prevent data leakage |\n",
    "| 5. Pipeline | `tf.data` with caching, prefetching, and augmentation keeps GPUs busy |\n",
    "| 6. Basic CNN | A simple model establishes a baseline but overfits quickly |\n",
    "| 7. Enhanced CNN | BatchNorm, deeper architecture, and callbacks significantly improve results |\n",
    "| 8. Transfer Learning | Pre-trained models deliver the best accuracy with minimal training |\n",
    "| 9. Evaluation | Confusion matrices, classification reports, and Grad-CAM reveal *why* a model succeeds or fails |\n",
    "| 10. Tuning | The learning rate finder is the single most impactful tuning technique |\n",
    "| 11. Production | SavedModel for serving, TFLite for mobile, quantisation for speed |\n",
    "| 12. Exercises | Progressive challenges reinforce concepts and prepare you for real-world projects |\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You have progressed from a basic 40-image classifier to a production-ready, interpretable deep learning system. The skills you have practised -- data engineering, model design, transfer learning, interpretability, and deployment -- form the core toolkit of a modern ML engineer."
   ]
  }
 ]
}
